# 핵심 프레임워크 (이미지에 포함되어 있으나 버전 고정을 위해 명시)
torch>=2.4.0
torchvision
torchaudio

# Hugging Face 생태계 (Llama-3 학습 필수)
transformers>=4.40.0
datasets>=2.19.0
accelerate>=0.30.0
evaluate
tokenizers>=0.19.0

# 분산 학습 및 최적화
deepspeed>=0.14.0
mpi4py                      # Multi-node 통신 보조

# 데이터 처리 및 유틸리티
sentencepiece               # Llama 토크나이저 대응
protobuf
numpy<2.0.0                 # PyTorch 호환성을 위해 1.x 유지 권장
pandas
tqdm
pyyaml

# AWS 및 클라우드 환경
boto3                       # S3/FSx 연동용
fsspec>=2024.3.1
huggingface_hub             # Llama-3 게이트 모델 인증용

# 성능 모니터링 (필요 시 선택)
psutil
py-cpuinfo

# Flash Attention 2: 만약 A100, H100, B200 또는 G6e(L40S) 를 사용 중이면, 학습 속도를 2배 이상 높이기 위해 flash-attn을 추가 설치
# pip install flash-attn을 실행하면 런타임 GPU 사양에 맞춰 어텐션 코드를 기계어로 번역(컴파일)
# 컴파일 타임이 20-30분 정도 소요되므로, 별도의 컨테이너 이미지를 사전에 빌드하는게 좋다..
# 본 워크샵에서는 실행시 매번 컴파일 하는 것으로 하니, NCCL 통신 동기화에서 파드간의 준비 시점 불일치로 훈련 작업을 시작하지 못하고 멈춘다...
# 필요한 모든 라이브러리를 설치해논 커스텀 컨테이너 이미지를 빌드해야 한다. 특히 플래시 어탠션 처럼 설치에 시간이 많이 걸리는 라이브러리는 문제를 발생시킨다. 
flash-attn

