
## Llama 8B 훈련 메모리 계산 ##

### 1. 총 메모리 공식 ###
총 VRAM = 모델 가중치 + 그래디언트 + 옵티마이저 상태 + 활성화 값

### 2. 8B 모델 상세 계산 (FP16 기준) ###

* 모델 가중치: 
  * 학습을 위해 모델을 메모리에 올리는 단계입니다.
  * 80억 파라미터 x 2바이트 = 16GB

* 그래디언트: 
  * 역전파 시 각 파라미터의 기울기를 저장하고, 모델 가중치와 동일한 크기가 필요하다.
  * 80억 파라미터 x 2바이트 = 16GB 

* 옵티마이저 상태 (Adam 기준):
  * 가장 많은 메모리를 차지하는 부분으로, Adam 옵티마이저는 가중치의 복사본(FP32), Momentum, Variance를 저장해야 하므로 파라미터당 총 12~16바이트가 필요합니다
  * 80억 파라미터 x 12바이트 = 96GB

* 활성화 값 및 여유 공간: 약 12GB 이상 (배치 사이즈 및 시퀀스 길이에 따라 증가)
  * 1. 활성화 값 (Activations)이란?
  순전파(Forward Pass) 과정에서 각 층(Layer)의 연산 결과물들을 임시로 저장해두는 메모리이다. 역전파(Backward Pass) 때 그래디언트(기울기)를 계산하려면 이 값들이 반드시 필요하기 때문에 버리지 못하고 메모리에 들고 있어야 한다. 
    * 영향을 주는 요소:
       * 배치 사이즈(Batch Size): 한 번에 학습시키는 문장 개수가 많을수록 선형적으로 증가한다.
       * 시퀀스 길이(Sequence Length): 문장이 길어질수록 제곱(Square)에 비례하여 증가한다. (Self-Attention 연산 때문).
       * 모델 구조: Hidden Size(모델의 폭)와 레이어 수가 많을수록 커진다.
       * 8B 모델 예시:
          * 시퀀스 길이 2048, 배치 사이즈 1로 설정 시 약 4~6GB 정도를 차지합니다.
          * 하지만 시퀀스 길이를 8192로 늘리면 이 값만 20GB 이상으로 치솟을 수 있습니다.

#### 최소 필요량: 약 140GB ###
#### 권장 사양: 160GB 이상의 VRAM (A100 80GB 또는 H100 80GB 2개 이상 필요) ####










