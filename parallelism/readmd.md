모델이 커지면 학습 과정에서 데이터를 저장하는 활성화 메모리(Activation Memory)가 그래픽 카드(GPU) 용량을 초과하는 병목 현상이 발생한다. 이를 해결하기 위해 텐서 병렬화(Tensor Parallel, TP)를 사용하지만, TP만으로는 부족한 부분을 시퀀스 병렬화(Sequence Parallel, SP)가 채워주게 된다. 

### 1. 왜 활성화 메모리가 병목인가요? ###
학습할 때 역전파(Backpropagation)를 위해 forward 단계의 중간값(활성화 값)을 메모리에 계속 들고 있어야 한다. 모델이 커지고 문장(Sequence)이 길어질수록 이 데이터 양이 기하급수적으로 늘어나 GPU 메모리를 다 잡아먹게 된다. 

### 2. 텐서 병렬화(TP)의 한계 ###
TP는 연산량이 많은 Attention이나 MLP 층의 행렬을 쪼개서 여러 GPU가 나눠 갖게 한다. 하지만 LayerNorm이나 RMSNorm 같은 층은 연산량은 적으면서도 전체 시퀀스 데이터를 그대로 복사해서 들고 있어야 하므로, 여기서 메모리 낭비가 발생한다. 

### 3. 시퀀스 병렬화(SP)의 역할 ###
SP는 바로 이 LayerNorm/RMSNorm 층에서 데이터를 시퀀스(문장 길이) 방향으로 쪼개서 각 GPU에 나눠 준다. 
* 기존 TP: 모든 GPU가 똑같은 전체 문장 데이터를 복사해서 들고 있음.
* SP 적용 시: 1번 GPU는 문장의 앞부분, 2번 GPU는 뒷부분만 들고 연산함.
* 결과: 각 GPU가 저장해야 할 활성화 메모리 양이 병렬화 개수만큼 줄어들어, 더 긴 문장이나 더 큰 모델을 학습할 수 있게 된다. 
요약하자면, "연산이 무거운 곳은 TP로, 연산은 가볍지만 메모리를 많이 먹는 Norm 층은 SP로 쪼개서" 전체적인 메모리 효율을 극대화하는 전략이다. 
