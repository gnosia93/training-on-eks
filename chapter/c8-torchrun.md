## [íŠ¸ë ˆì´ë‹ ì˜¤í¼ë ˆì´í„°(V2) ì„¤ì¹˜](https://www.kubeflow.org/docs/components/trainer/operator-guides/installation/) ##

V2 ë²„ì „ì˜ íë¸Œ í”Œë¡œìš° íŠ¸ë ˆì´ë‹ ì˜¤í¼ë ˆì´í„°ì™€ ëŸ°í„°ì„ ì„¤ì¹˜í•œë‹¤. 
```
sudo dnf install git -y

export VERSION=v2.1.0
kubectl apply --server-side -k "https://github.com/kubeflow/trainer.git/manifests/overlays/manager?ref=${VERSION}"
kubectl apply --server-side -k "https://github.com/kubeflow/trainer.git/manifests/overlays/runtimes?ref=${VERSION}"

kubectl get pods -n kubeflow-system
kubectl get clustertrainingruntimes

```
[ê²°ê³¼]
```
NAME                                                   READY   STATUS    RESTARTS   AGE
jobset-controller-manager-58555b47c7-ltrck             1/1     Running   0          2m55s
kubeflow-trainer-controller-manager-5b7b978fbf-r24kr   1/1     Running   0          2m55s

NAME                     AGE
deepspeed-distributed    114s
mlx-distributed          114s
torch-distributed        114s
torchtune-llama3.2-1b    114s
torchtune-llama3.2-3b    114s
torchtune-qwen2.5-1.5b   114s
```

## CRD ì¡°íšŒ ##
```
kubectl explain trainjob.spec
kubectl explain trainjob.spec.podTemplateOverrides.spec
```

## runtme ì¡°íšŒ ##
```
kubectl get clustertrainingruntime torch-distributed -o yaml
```
[ê²°ê³¼]
```
apiVersion: trainer.kubeflow.org/v1alpha1
kind: ClusterTrainingRuntime
metadata:
  creationTimestamp: "2025-12-25T17:37:32Z"
  generation: 1
  labels:
    trainer.kubeflow.org/framework: torch
  name: torch-distributed
  resourceVersion: "1310309"
  uid: 2067ff23-511e-4b9c-b37e-b4d873f43c85
spec:
  mlPolicy:
    numNodes: 1
    torch:
      numProcPerNode: auto
  template:
    spec:
      replicatedJobs:
      - groupName: default
        name: node
        replicas: 1
        template:
          metadata:
            labels:
              trainer.kubeflow.org/trainjob-ancestor-step: trainer
          spec:
            template:
              spec:
                containers:
                - image: pytorch/pytorch:2.7.1-cuda12.8-cudnn9-runtime
                  name: node
```
```
kubectl explain ClusterTrainingRuntime.spec.template.spec.failurePolicy.maxRestarts
```
```
GROUP:      trainer.kubeflow.org
KIND:       ClusterTrainingRuntime
VERSION:    v1alpha1

FIELD: maxRestarts <integer>


DESCRIPTION:
    MaxRestarts defines the limit on the number of JobSet restarts.
    A restart is achieved by recreating all active child jobs.
```


## íŠ¸ë ˆì´ë‹ ì‘ì—… ì‹¤í–‰ ##
TrainJob ì˜¤í¼ë ˆì´í„°ëŠ” backoffLimit ë¼ëŠ” í•„ë“œë¥¼ ì´ìš©í•˜ì—¬ ì‘ì—… ë³µêµ¬ ë§¤ì»¤ë‹ˆì¦˜ì„ ì œê³µí•œë‹¤. ì‘ì—…ì´ ì‹¤íŒ¨ í–ˆì„ë•Œ ë‹¤ì‹œ ì‹œì‘í•˜ëŠ” ê¸°ëŠ¥ìœ¼ë¡œ, ì´ ì˜ˆì œì—ì„œëŠ” 3ë²ˆê¹Œì§€ íŠ¸ë ˆì´ë‹ ì‘ì—…ì„ ì¬ ì‹œì‘ í•˜ë„ë¡ ì„¤ì • í•˜ì˜€ë‹¤.  
```
cat <<EOF > t5-large.yaml
apiVersion: trainer.kubeflow.org/v1alpha1
kind: TrainJob
metadata:
  name: t5-large
spec:
  backoffLimit: 3                             # ì‘ì—… ì‹¤íŒ¨ì‹œ ì¬ì‹œë„ íšŸìˆ˜
  restartPolicy: OnFailure

  podTemplateOverrides:
    - targetJobs:
        - name: trainer
      spec:
        nodeSelector:
          node.kubernetes.io/instance-type: g6e.48xlarge
          topology.kubernetes.io/zone: ap-northeast-2                # íŠ¹ì • ê°€ìš© ì˜ì—­(AZ) ë‚´ ë°°ì¹˜ë¥¼ ê°•ì œí•˜ì—¬ ë…¸ë“œ ê°„ í†µì‹  ì§€ì—°ì„ ìµœì†Œí™” 

  runtimeRef:
    name: torch-distributed                   # torch ë¶„ì‚° ë°±ì—”ë“œ ì‚¬ìš© (ê´€ë ¨ íŒŒì´ì¬ íŒ¨í‚¤ì§€ ë¬¶ìŒ)

  trainer:
    numNodes: 2                            # ë…¸ë“œìˆ˜ ì„¤ì •
    numProcPerNode: auto                   # ë…¸ë“œë³„ í”„ë¡œì„¸ìŠ¤ ê°¯ìˆ˜                                                                               
    image: public.ecr.aws/deep-learning-containers/pytorch-training:2.8.0-gpu-py312-cu129-ubuntu22.04-ec2-v1.0
    command: 
      - git clone https://github.com/gnosia93/training-on-eks /workspace/code
      - cd /workspace/code/samples/fsdp
      - echo "working directory: "$(pwd)
      - pip install -r requirements.txt
      - torchrun --nproc_per_node 8 --rdzv_id=elastic-job --rdzv_backend=c10d t5-fsdp.py
    resourcesPerNode:
      limits:
        nvidia.com: "8"
      requests:
        nvidia.com: "8"
EOF
```
íŠ¸ë ˆì´ë‹ ì‘ì—…ì„ ì‹œì‘í•˜ê³  ë¡œê·¸ë¥¼ í™•ì¸í•œë‹¤. 
```
kubectl apply -f t5-large.yaml

kubectl logs -f -l trainjob-name=t5-large
```

* Placement Group (ê°€ìš© ì˜ì—­ ì§€ì •):
nodeSelectorì— topology.kubernetes.io/zoneì„ ëª…ì‹œí•˜ë©´, ë¶„ì‚° í•™ìŠµì— ì°¸ì—¬í•˜ëŠ” ë…¸ë“œë“¤ì´ ë™ì¼í•œ ë°ì´í„° ì„¼í„° ë‚´ì— ë°°ì¹˜ë˜ì–´ NCCL í†µì‹  ë ˆì´í„´ì‹œê°€ í¬ê²Œ ì¤„ì–´ë“­ë‹ˆë‹¤.
* Scheduling Policy (Gang Scheduling):
schedulingPolicyë¥¼ ì‚¬ìš©í•˜ë©´ 2ê°œì˜ ë…¸ë“œê°€ ë™ì‹œì— í• ë‹¹ë  ë•Œë§Œ í•™ìŠµì„ ì‹œì‘í•©ë‹ˆë‹¤. ì´ëŠ” í•˜ë‚˜ëŠ” í™•ë³´ë˜ê³  í•˜ë‚˜ëŠ” ëŒ€ê¸° ìƒíƒœì¼ ë•Œ ë°œìƒí•˜ëŠ” ìì› ë‚­ë¹„ì™€ í†µì‹  ë¹„íš¨ìœ¨ì„ ë°©ì§€í•©ë‹ˆë‹¤.

## ì¥ì•  ë°œìƒ ì‹œ ë³µêµ¬ í”„ë¡œì„¸ìŠ¤ ##
ë…¸ë“œ 1ê°œê°€ ì£½ì—ˆì„ ë•Œ, ì¼ë°˜ì ì¸ NCCL í›ˆë ¨ê³¼ ë‹¬ë¦¬ torchrunì€ ë‹¤ìŒê³¼ ê°™ì´ í–‰ë™í•©ë‹ˆë‹¤.

* ì¥ì•  ê°ì§€: íŠ¹ì • Podê°€ ì£½ìœ¼ë©´ NCCL í†µì‹ ì´ ê¹¨ì§‘ë‹ˆë‹¤. ì´ë•Œ ì‚´ì•„ìˆëŠ” ë‚˜ë¨¸ì§€ Podì˜ torchrun í”„ë¡œì„¸ìŠ¤ê°€ ì´ë¥¼ ê°ì§€í•˜ê³  ìì‹ ì˜ ë¡œì»¬ í”„ë¡œì„¸ìŠ¤ë“¤ì„ ëª¨ë‘ ì¢…ë£Œ(Terminate)ì‹œí‚µë‹ˆë‹¤. (ì „ì²´ ì‘ì—…ì€ ì ì‹œ ë©ˆì¶¥ë‹ˆë‹¤.)
* ì¿ ë²„ë„¤í‹°ìŠ¤ ì¬ìŠ¤ì¼€ì¤„ë§: ì¿ ë²„ë„¤í‹°ìŠ¤ì˜ Job ì»¨íŠ¸ë¡¤ëŸ¬ë‚˜ ReplicaSetì´ ì£½ì€ Podë¥¼ ê°ì§€í•˜ê³ , ìƒˆë¡œìš´ Podë¥¼ ìë™ìœ¼ë¡œ ë‹¤ì‹œ ìƒì„±í•©ë‹ˆë‹¤.
* ìƒˆë¡œìš´ ë‘ë°ë¶€: ìƒˆë¡œ ëœ¬ Podì™€ ê¸°ì¡´ì— ì‚´ì•„ìˆë˜ Podë“¤ì´ ë‹¤ì‹œ ë‘ë°ë¶€ ì„œë²„ì— ëª¨ì…ë‹ˆë‹¤.
* World ì¬êµ¬ì„±: ë‘ë°ë¶€ ì„œë²„ëŠ” "ì, ë‹¤ì‹œ 8ëª…ì´ ëª¨ì˜€ìœ¼ë‹ˆ ìƒˆë¡œ ì‹œì‘í•˜ì"ë¼ê³  ì‹ í˜¸ë¥¼ ë³´ëƒ…ë‹ˆë‹¤. ì´ë•Œ ë°”ë€ IP ì •ë³´ ë“±ì„ NCCLì— ë‹¤ì‹œ ì „íŒŒí•˜ì—¬ í†µì‹  ê·¸ë£¹ì„ ì¬í˜•ì„±(Re-init)í•©ë‹ˆë‹¤.
* í•™ìŠµ ì¬ê°œ: ê°œë°œìê°€ ì§  ì½”ë“œ ë‚´ì˜ load_checkpoint ë¡œì§ì— ì˜í•´ ê³µìœ  ìŠ¤í† ë¦¬ì§€ì—ì„œ ë§ˆì§€ë§‰ ìƒíƒœë¥¼ ë¶ˆëŸ¬ì™€ í•™ìŠµì„ ì´ì–´ê°‘ë‹ˆë‹¤.

#### ìš”ì•½: ë§ˆìŠ¤í„° íŒŒë“œ ì¬ì‹œì‘ ì‹œ ì‹œë‚˜ë¦¬ì˜¤ ####
* Operatorê°€ íŒŒë“œ ì¬ì‚´ë¦¼ (IPê°€ ë°”ë€Œì–´ë„ ì„œë¹„ìŠ¤ ì´ë¦„ìœ¼ë¡œ ì—°ê²° ìœ ì§€).
* PyTorch Elasticì´ ë‘ë°ë·° ê´‘ì¥ì„ ìƒˆë¡œ ê°œì„¤.
* ëª¨ë“  ì›Œì»¤ê°€ ë‹¤ì‹œ ëª¨ì—¬ì„œ ê·¸ë£¹ êµ¬ì„± (ì²˜ìŒë¶€í„° ë‹¤ì‹œ ì‹œì‘).
* (ì¤‘ìš”) ì½”ë“œì— ì²´í¬í¬ì¸íŠ¸ ë¡œë“œ ë¡œì§ì´ ìˆë‹¤ë©´ ëŠê¸´ ì§€ì ë¶€í„° í•™ìŠµ ì¬ê°œ, ì—†ë‹¤ë©´ 0 ì—í­ë¶€í„° ë‹¤ì‹œ ì‹œì‘.

## ë‘ë°ë·° í¬ì¸íŠ¸ ##
* c10d (ê¶Œì¥): ì¶”ê°€ ì¸í”„ë¼ê°€ í•„ìš” ì—†ì–´ ê°€ì¥ ê°€ë³ìŠµë‹ˆë‹¤. í¬ë“œê°€ ì¬ì‹œì‘ë˜ì–´ë„ ì¿ ë²„ë„¤í‹°ìŠ¤ ì„œë¹„ìŠ¤ ì´ë¦„ì€ ìœ ì§€ë˜ë¯€ë¡œ torchrunì´ ë‹¤ì‹œ ë‘ë°ë·°í•˜ëŠ” ë° ë¬¸ì œê°€ ì—†ìŠµë‹ˆë‹¤.
* etcd: ìˆ˜ë°± ê°œ ì´ìƒì˜ ë…¸ë“œë¥¼ ì‚¬ìš©í•˜ëŠ” ëŒ€ê·œëª¨ í´ëŸ¬ìŠ¤í„°ì—ì„œ ë‘ë°ë·°ì˜ ì•ˆì •ì„±ì„ ê·¹í•œìœ¼ë¡œ ë†’ì—¬ì•¼ í•  ë•Œ ì‚¬ìš©í•©ë‹ˆë‹¤. ì¼ë°˜ì ì¸ 5~10ê°œ ë…¸ë“œ ê·œëª¨ì—ì„œëŠ” c10dë¡œë„ ì¶©ë¶„í•©ë‹ˆë‹¤.





--------------------

## ì²´í¬í¬ì¸íŠ¸ ##
ëª¨ë“  ë…¸ë“œê°€ ìµœì‹  ì²´í¬í¬ì¸íŠ¸ íŒŒì¼ì— ì ‘ê·¼í•  ìˆ˜ ìˆì–´ì•¼ í•©ë‹ˆë‹¤. ì´ë¥¼ ìœ„í•´ í˜„ì—…ì—ì„œëŠ” í¬ê²Œ ë‘ ê°€ì§€ ë°©ë²•ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
#### 1. ê³µìœ  ìŠ¤í† ë¦¬ì§€ ì‚¬ìš© (ê°€ì¥ ê¶Œì¥ë¨) ####
ëª¨ë“  ë…¸ë“œê°€ NFS, AWS FSx, Google Cloud Filestoreì™€ ê°™ì€ ê³µìœ  ë„¤íŠ¸ì›Œí¬ ìŠ¤í† ë¦¬ì§€ë¥¼ ë™ì¼í•œ ê²½ë¡œì— ë§ˆìš´íŠ¸í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.
* ì¥ì : íŠ¹ì • ë…¸ë“œê°€ ì™„ì „íˆ ì‚¬ë¼ì ¸ë„ ë°ì´í„°ê°€ ì•ˆì „í•˜ë©°, ëª¨ë“  ë…¸ë“œê°€ ê°™ì€ ê²½ë¡œ(/mnt/nfs/checkpoint.pt)ë¥¼ ë°”ë¼ë³´ê¸°ë§Œ í•˜ë©´ ë©ë‹ˆë‹¤.
* ë°©ì‹: 0ë²ˆ ë§ˆìŠ¤í„° ë…¸ë“œê°€ ì²´í¬í¬ì¸íŠ¸ë¥¼ ì €ì¥í•˜ë©´ ë‚˜ë¨¸ì§€ ë…¸ë“œë“¤ì´ ì¬ì‹œì‘ ì‹œ í•´ë‹¹ íŒŒì¼ì„ ì½ì–´ì˜µë‹ˆë‹¤.

#### 2. ë¡œì»¬ ìŠ¤í† ë¦¬ì§€ + ë³µì œ ####
ê° ë…¸ë“œì˜ ë¡œì»¬ ë””ìŠ¤í¬(SSD)ì— ì €ì¥í•˜ëŠ” ë°©ì‹ì…ë‹ˆë‹¤.
* ë‹¨ì : ë…¸ë“œ ìì²´ê°€ ë¬¼ë¦¬ì ìœ¼ë¡œ ê³ ì¥ ë‚˜ë©´ í•´ë‹¹ ë…¸ë“œì— ìˆë˜ ì²´í¬í¬ì¸íŠ¸ëŠ” ìœ ì‹¤ë©ë‹ˆë‹¤.
* ë°©ì‹: ì´ë¥¼ í•´ê²°í•˜ë ¤ë©´ í•™ìŠµ ì¤‘ ì£¼ê¸°ì ìœ¼ë¡œ ì²´í¬í¬ì¸íŠ¸ë¥¼ S3 ê°™ì€ í´ë¼ìš°ë“œ ìŠ¤í† ë¦¬ì§€ë¡œ ì—…ë¡œë“œí•˜ê±°ë‚˜, ëª¨ë“  ë…¸ë“œê°€ ê°ì ìê¸° ë””ìŠ¤í¬ì— ë™ì¼í•œ ë³µì‚¬ë³¸ì„ ì €ì¥í•˜ë„ë¡ ì„¤ê³„í•´ì•¼ í•©ë‹ˆë‹¤.

#### ğŸ’¡ torchrun ì¬ì‹œì‘ ì‹œ ì½”ë“œ êµ¬í˜„ í•µì‹¬ ####
torchrunì€ í”„ë¡œì„¸ìŠ¤ë¥¼ ë‹¤ì‹œ ë„ì›Œì¤„ ë¿, ì²´í¬í¬ì¸íŠ¸ë¥¼ ë¶ˆëŸ¬ì˜¤ëŠ” ì½”ë“œëŠ” ì§ì ‘ ì‘ì„±í•´ì•¼ í•©ë‹ˆë‹¤. ë³´í†µ ë‹¤ìŒê³¼ ê°™ì€ ë¡œì§ì„ ì‚¬ìš©í•©ë‹ˆë‹¤.
```
import torch
import os

def main():
    # 1. ì²´í¬í¬ì¸íŠ¸ ê²½ë¡œ ì„¤ì • (ê³µìœ  ìŠ¤í† ë¦¬ì§€ ê¶Œì¥)
    ckpt_path = "/shared/storage/model_latest.pt"

    # 2. ë§Œì•½ ê¸°ì¡´ ì²´í¬í¬ì¸íŠ¸ê°€ ìˆë‹¤ë©´ ë¡œë“œ
    if os.path.exists(ckpt_path):
        checkpoint = torch.load(ckpt_path, map_location='cpu')
        model.load_state_dict(checkpoint['model'])
        optimizer.load_state_dict(checkpoint['optimizer'])
        start_epoch = checkpoint['epoch']
        print(f"Resuming from epoch {start_epoch}")
    
    # 3. í•™ìŠµ ë£¨í”„ ì¤‘ ì£¼ê¸°ì  ì €ì¥ (Rank 0ë²ˆë§Œ ì €ì¥)
    if dist.get_rank() == 0:
        torch.save({...}, ckpt_path)
```
* ê³µìœ  ì €ì¥ì†Œ(NFS ë“±)ë¥¼ ì“°ëŠ” ê²ƒì´ ê°€ì¥ ì•ˆì „í•˜ê³  í¸ë¦¬í•©ë‹ˆë‹¤. 
* ë…¸ë“œê°€ ì¬ì‹œì‘ë  ë•Œ ìë™ìœ¼ë¡œ ckpt_pathë¥¼ í™•ì¸í•˜ì—¬ load_state_dictë¥¼ ìˆ˜í–‰í•˜ëŠ” ë¡œì§ì´ ì½”ë“œì— í¬í•¨ë˜ì–´ì•¼ í•©ë‹ˆë‹¤.
* ë§Œì•½ ê³µìœ  ì €ì¥ì†Œê°€ ì—†ë‹¤ë©´, ë…¸ë“œ ì¥ì•  ì‹œ í•´ë‹¹ ë…¸ë“œì— ìˆë˜ ë°ì´í„°ëŠ” ëª» ì“°ê²Œ ë˜ë¯€ë¡œ ì™¸ë¶€ í´ë¼ìš°ë“œ ì €ì¥ì†Œ(S3 ë“±)ì— ë°±ì—…í•˜ëŠ” ì ˆì°¨ê°€ í•„ìš”í•©ë‹ˆë‹¤.


## ë ˆí¼ëŸ°ìŠ¤ ##

* https://github.com/kubeflow/trainer
* https://www.kubeflow.org/docs/components/trainer/operator-guides/migration/
* https://blog.kubeflow.org/trainer/intro/
