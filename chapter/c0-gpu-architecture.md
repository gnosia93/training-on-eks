2025년 현재 엔비디아의 데이터 센터용 GPU 라인업은 Ampere(A100), Hopper(H100/H200), 그리고 최신 Blackwell(B100/B200/B300) 아키텍처로 구분됩니다. 각 세대별 주요 특징과 차이점은 다음과 같습니다

### 1. 세대별 주요 모델 비교 ###
![](https://github.com/gnosia93/training-on-eks/blob/main/chapter/images/gpu-gen.png)

### 2. 주요 모델별 상세 특징 ###
* A100: 인공지능 붐을 일으킨 전설적인 모델입니다. 현재는 단종(EOL) 수순이지만, 여전히 중소 규모의 AI 학습 및 추론용으로 널리 쓰입니다.
* H100 / H200: 대규모 언어 모델(LLM) 학습의 표준입니다. 특히 H200은 메모리를 141GB로 늘려 H100보다 더 큰 모델을 다룰 수 있습니다.
* B100 / B200 (Blackwell): 2025년 기준 최강의 GPU입니다. 2개의 칩을 하나처럼 연결한 듀얼 다이(Dual-die) 설계를 통해 2,080억 개의 트랜지스터를 탑재했습니다.
  * B100: 전력 효율을 중시한 모델 (TDP 약 700W).
  * B200: 성능을 극대화한 모델 (TDP 최대 1,000W).
* GB200 / GB300 (Superchip): GPU만 있는 것이 아니라, 엔비디아의 자체 Grace CPU와 Blackwell GPU를 NVLink로 직접 연결한 '슈퍼칩' 형태입니다. CPU와 GPU 사이의 병목 현상을 제거하여 조 단위 파라미터 모델 학습에 최적화되어 있습니다.
* B300 (Blackwell Ultra): 2025년 하반기 출시 예정인 강화 버전으로, 메모리 용량이 288GB까지 늘어나 거대 모델 추론 효율이 더욱 극대화됩니다. 

### 3. 핵심 차이점 요약 ###
* 연산 정밀도: 세대가 올라갈수록 더 낮은 정밀도(FP16 → FP8 → FP4)를 지원하여, 정확도는 유지하면서 연산 속도를 비약적으로 높이고 있습니다.
* 메모리 용량 및 대역폭: AI 모델이 커짐에 따라 한 번에 로드할 수 있는 메모리 용량(80GB → 192GB → 288GB)과 데이터를 주고받는 속도가 급격히 상승했습니다.
* 시스템 단위 확장: 과거에는 개별 GPU 성능이 중요했지만, 이제는 수십 개의 GPU를 하나처럼 묶는 NVLink 기술과 GB200 같은 랙 규모(Rack-scale) 솔루션이 차별점입니다
