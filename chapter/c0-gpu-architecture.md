2025년 현재 엔비디아의 데이터 센터용 GPU 라인업은 Ampere(A100), Hopper(H100/H200), 그리고 최신 Blackwell(B100/B200/B300) 아키텍처로 구분됩니다. 각 세대별 주요 특징과 차이점은 다음과 같습니다

## 세대별 비교 ##

### 1. 세대별 주요 모델 비교 ###
![](https://github.com/gnosia93/training-on-eks/blob/main/chapter/images/gpu-gen.png)

### 2. 주요 모델별 상세 특징 ###
* A100: 인공지능 붐을 일으킨 전설적인 모델입니다. 현재는 단종(EOL) 수순이지만, 여전히 중소 규모의 AI 학습 및 추론용으로 널리 쓰입니다.
* H100 / H200: 대규모 언어 모델(LLM) 학습의 표준입니다. 특히 H200은 메모리를 141GB로 늘려 H100보다 더 큰 모델을 다룰 수 있습니다.
* B100 / B200 (Blackwell): 2025년 기준 최강의 GPU입니다. 2개의 칩을 하나처럼 연결한 듀얼 다이(Dual-die) 설계를 통해 2,080억 개의 트랜지스터를 탑재했습니다.
  * B100: 전력 효율을 중시한 모델 (TDP 약 700W).
  * B200: 성능을 극대화한 모델 (TDP 최대 1,000W).
* GB200 / GB300 (Superchip): GPU만 있는 것이 아니라, 엔비디아의 자체 Grace CPU와 Blackwell GPU를 NVLink로 직접 연결한 '슈퍼칩' 형태입니다. CPU와 GPU 사이의 병목 현상을 제거하여 조 단위 파라미터 모델 학습에 최적화되어 있습니다.
* B300 (Blackwell Ultra): 2025년 하반기 출시 예정인 강화 버전으로, 메모리 용량이 288GB까지 늘어나 거대 모델 추론 효율이 더욱 극대화됩니다. 

### 3. 핵심 차이점 요약 ###
* 연산 정밀도: 세대가 올라갈수록 더 낮은 정밀도(FP16 → FP8 → FP4)를 지원하여, 정확도는 유지하면서 연산 속도를 비약적으로 높이고 있습니다.
* 메모리 용량 및 대역폭: AI 모델이 커짐에 따라 한 번에 로드할 수 있는 메모리 용량(80GB → 192GB → 288GB)과 데이터를 주고받는 속도가 급격히 상승했습니다.
* 시스템 단위 확장: 과거에는 개별 GPU 성능이 중요했지만, 이제는 수십 개의 GPU를 하나처럼 묶는 NVLink 기술과 GB200 같은 랙 규모(Rack-scale) 솔루션이 차별점입니다


## AI 개발 및 딥러닝 (학습/추론) GPU 선정 기준 ##
* 고려 사항: VRAM 용량 (가장 중요), FP16/FP8 연산 성능 (텐서 코어 성능), 멀티 GPU 간 연결(NVLink), 안정성.
* 추천 모델: 엔비디아 A100, H100/H200, B200, GB200 등 데이터 센터 라인업.

## 연산 성능 ##
엔비디아는 H100(Hopper) 세대부터 FP8을 본격 도입했고, 최신 B200(Blackwell)에서는 FP4까지 지원하며 성능을 비약적으로 높였습니다.

### 1. 세대별 연산 성능 비교표 (Tensor Core 활용 시) ###
단위: TFLOPS (Tera Floating Point Operations Per Second)
![](https://github.com/gnosia93/training-on-eks/blob/main/chapter/images/gpu-calc.png)

*Blackwell 세대부터는 전통적인 FP32 성능보다 AI용 저정밀도(FP8, FP4) 연산 최적화에 집중하여 공식 수치를 추론 기반으로 발표합니다.

### 2. 연산 성능 변화 추이 (시각화 데이터) ###
세대별 성능 향상을 직관적으로 보면 다음과 같습니다. (A100의 FP16 성능을 1로 보았을 때의 상대적 배수)
* FP16 성능: A100 (1x) → H100 (3.1x) → B200 (7.2x)
* FP8 성능: A100 (N/A) → H100 (1x) → B200 (2.2x)

### 3. 주요 성능 특징 분석 ###
#### FP32 vs FP16/FP8: ####
그래프를 그리면 FP32는 완만하게 상승하는 반면, FP16과 FP8은 수직에 가깝게 상승합니다. 이는 엔비디아가 AI 모델 학습/추론에 특화된 텐서 코어(Tensor Core)에 연산 자원을 집중하고 있기 때문입니다.

#### 정밀도와 속도의 관계: ####
* FP16은 FP32보다 약 16배 빠릅니다 (A100 기준).
* FP8은 FP16보다 약 2배 더 빠릅니다 (H100 기준).
* B200에서 지원하는 FP4는 FP8보다 다시 2배 더 빠릅니다.

#### 선정 기준 팁: ####
* 모델 학습(Training): FP16 성능이 높은 H100/H200이 표준입니다.
* 모델 추론(Inference): FP8 및 FP4 성능이 압도적인 B200이 가장 효율적입니다.

더 구체적인 벤치마크나 특정 모델 간의 상세 비교가 필요하시면 말씀해 주세요. 관련 기술 문서는 NVIDIA Data Center 솔루션 페이지에서 확인할 수 있습니다.


## 성능 수치 비교 ##
성능 수치는 엔비디아의 공식 발표 및 MLPerf 벤치마크 제출 자료를 기반으로 하며, AI 워크로드에서 텐서 코어(Tensor Core)를 활용한 최대치(Peak performance)입니다. 
### 1. 주요 모델 간 성능 비교표 ###

![](https://github.com/gnosia93/training-on-eks/blob/main/chapter/images/gpu-gen-perf.png)

### 2. 실제 AI 워크로드 벤치마크 (MLPerf 기준) ###
실제 대규모 언어 모델(LLM) 학습 및 추론 작업에서의 상대적인 성능 차이는 다음과 같습니다.

#### 학습 속도 (Training Throughput): ####
* H100은 A100 대비 최대 2.4배 더 빠른 학습 처리량을 제공합니다.
* B200 기반 시스템은 H100 시스템 대비 학습 성능이 최대 2.2배 더 높습니다.

#### 추론 성능 (Inference Performance): ####
* H100은 A100 대비 1.5~2배 빠른 추론 성능을 보입니다.
* B200은 H100 대비 추론 성능이 최대 4배 빠르며, 특정 최적화된 워크로드에서는 최대 15배까지 성능 향상이 보고되었습니다

### 3. 상세 모델별 비교 및 활용 분야 ###
#### A100: ####
특징: 여전히 비용 효율적인 옵션이며 안정성이 검증되었습니다. 다양한 크기의 모델 학습 및 추론에 활용되지만, 최신 LLM 워크로드에서는 H100 대비 효율이 떨어집니다.

#### H100 / H200: ####
특징: 현재 대규모 AI 학습의 표준입니다. H200은 H100보다 메모리 용량(80GB → 141GB)과 대역폭이 증가하여, 메모리 제약이 있는 장문맥(Long-context) LLM 추론에 특히 강점을 보입니다.

#### B200 / GB200: ####
특징: 2025년 기준 최강의 성능을 자랑합니다. FP4 도입과 듀얼 다이 설계로 추론 성능을 극대화했습니다. GB200은 CPU와 GPU를 통합한 슈퍼칩 형태로, 시스템 전체 메모리가 384GB에 달해 조 단위 파라미터 모델을 메모리 병목 없이 처리할 수 있습니다


## 클럭 스피드 ##
2025년 기준 엔비디아의 아키텍처 변화를 살펴보면, 연산 코어의 클럭 속도(Clock Speed)는 세대별로 완만하게 상승해 왔지만, 전체적인 성능 향상은 클럭보다는 코어의 개수 증가와 아키텍처 효율화(병렬 처리)에 더 큰 비중을 두고 있습니다.

### 1. 세대별 대표 모델 클럭 비교 (Boost Clock 기준) ###

![](https://github.com/gnosia93/training-on-eks/blob/main/chapter/images/gpu-clock.png)
*B200의 경우 전력 소모량(TDP 1,000W)이 급증하면서 클럭을 높이기보다는 2개의 다이를 연결해 코어 숫자를 확보하는 방향을 선택했습니다.

### 2. 클럭 변화의 핵심 포인트 ###
* 완만한 상승: 1.4GHz 수준에서 2GHz 초반까지 상승했으나, CPU(5GHz 이상)처럼 극적인 속도 향상은 아닙니다. 이는 GPU가 수천 개의 코어를 동시에 돌리는 병렬 연산 장치이기 때문에, 개별 코어의 속도를 높이기보다는 코어 수를 늘리는 것이 발열 관리와 성능 면에서 유리하기 때문입니다.
* 클럭보다는 '처리량(Throughput)': 엔비디아는 클럭 수치를 높이는 대신, 한 클럭당 처리할 수 있는 데이터 양을 늘리는 데 집중합니다. 예를 들어, H100은 A100과 클럭 차이는 30%지만, AI 연산 성능은 트랜스포머 엔진 덕분에 몇 배나 더 빠릅니다.
* 전력 및 발열 제약: 클럭을 높이면 전력 소모와 발열이 지수적으로 증가합니다. 2025년 최신 GPU인 B200이 수랭식 쿨링을 권장하는 수준까지 온 것을 감안하면, 앞으로도 클럭을 무한정 높이기는 어려운 상황입니다.

### 3. 클럭 외에 성능을 결정짓는 요소 (2025년 기준) ###
* 코어 개수: A100(6,912개) → H100(16,896개) → B200(약 40,000개 추정)으로 기하급수적으로 증가.
* 메모리 대역폭: HBM3e 도입으로 데이터를 공급하는 속도가 클럭 상승폭보다 훨씬 큽니다.
* 저정밀도 연산 최적화: 동일 클럭에서 FP4, FP8 연산 전용 하드웨어를 배치하여 효율을 극대화합니다.

요약하자면 클럭 속도는 세대를 거치며 조금씩 빨라지고 있지만, GPU 성능 폭발의 진짜 비결은 클럭보다는 트랜지스터 개수를 늘려 더 많은 코어를 배치하고, 새로운 데이터 규격(FP8/FP4)을 지원하는 아키텍처 개선에 있습니다. NVIDIA Blackwell 아키텍처 상세에서 더 자세한 하드웨어 설계 원리를 확인할 수 있습니다.
