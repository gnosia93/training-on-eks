### 1. 상시 상태 모니터링 (Health Monitoring Agent) ### 
HyperPod는 모든 GPU 및 Trainium 노드에서 백그라운드로 실행되는 상태 모니터링 에이전트를 사용합니다. 
* 패시브 체크: GPU 응답성, NVLink 오류 카운터, 메모리 고갈(OOM), 커널 데드락 등 하드웨어 및 시스템 수준의 결함을 실시간으로 감시합니다.
* 딥 헬스 체크(Deep Health Checks): 새로운 인스턴스가 클러스터에 추가될 때 EFA(Elastic Fabric Adapter) 네트워크와 GPU의 부하 테스트를 수행하여 불량 노드가 사전에 투입되는 것을 방지합니다. 

### 2. 자동 노드 복구 및 교체 (Self-Healing) ###
장애가 감지되면 시스템이 즉시 개입하여 클러스터의 복원력을 유지합니다.
* 지능적 판단: 문제의 경중에 따라 노드를 단순히 재부팅(Reboot)하거나, 하드웨어 결함인 경우 정상적인 새 인스턴스로 교체(Replace)합니다.
* 자동화: 2025년에 도입된 최신 기능을 통해 사용자는 별도의 수동 구성 없이도 에이전트가 노드를 'Unhealthy'로 표시하고 자동으로 복구 작업을 시작하도록 설정할 수 있습니다. 

### 3. 체크포인트 기반 자동 재개 (Auto-Resume) ###
노드 교체 후 중단된 지점부터 다시 학습을 시작합니다.
* Job Auto-Resume: Slurm 또는 Kubernetes(EKS) 오케스트레이터와 통합되어, 새 노드가 준비되면 마지막으로 저장된 체크포인트(Checkpoint)를 자동으로 로드하여 훈련을 재개합니다.
* 중단 최소화: 수동으로 작업을 다시 제출할 필요가 없으므로 관리자의 운영 부담이 0에 가깝습니다. 

### 4. 2025년 신기술: 체크포인트리스 훈련 (Checkpointless Training) ###
최근에는 저장 장치에 체크포인트를 쓰는 시간을 줄이기 위해 더 진보된 방식이 도입되었습니다.
* P2P 상태 복구: 장애 발생 시 전체 작업을 멈추고 체크포인트를 읽어오는 대신, 정상적인 인접 노드(Peer)로부터 모델 및 옵티마이저 상태를 직접 전송받아 복구합니다.
* 효율성: 이 방식을 통해 복구 시간을 기존 수십 분에서 2분 미만으로 단축할 수 있으며, 수천 개의 GPU 클러스터에서도 95% 이상의 가동률(Goodput)을 유지할 수 있습니다. 
이 시스템 덕분에 기업은 수개월이 걸리는 LLM 훈련 과정에서 발생할 수 있는 하드웨어 장애 리스크를 최소화하고, 유휴 자원 낭비 없이 안정적으로 모델을 개발할 수 있습니다. 


---

2025년 기준 Amazon SageMaker HyperPod가 EKS와 통합되어 작업을 재실행하는 과정은 쿠버네티스 오퍼레이터(Operator)와 HyperPod 회복성 에이전트 간의 긴밀한 협업으로 이루어집니다. 구체적인 작업 재실행(Auto-Resume) 프로세스는 다음과 같습니다.

#### 1. 장애 감지 및 노드 교체 (Infrastructure Layer)  ####
에이전트 감시: 각 GPU 노드에서 실행되는 HyperPod 모니터링 에이전트가 하드웨어 결함(GPU 오류, 네트워크 단절 등)을 감지합니다.
자동 수리: 결함이 확인되면 HyperPod는 즉시 해당 노드를 클러스터에서 제거(Cordon/Drain)하고, 자동으로 정상적인 새 인스턴스를 프로비저닝하여 EKS 클러스터에 다시 조인시킵니다. 

#### 2. 쿠버네티스 오퍼레이터를 통한 작업 관리 (Orchestration Layer) ####
SageMaker HyperPod Training Operator: EKS에 설치된 이 전용 오퍼레이터가 학습 작업의 상태를 중앙에서 관리합니다.
작업 일시 중지: 특정 노드에 장애가 생겨 Pod가 비정상 종료되면, 오퍼레이터는 전체 학습 작업에 '중단' 신호를 보내 연쇄적인 타임아웃을 방지합니다.
자동 재스케줄링: 새 노드가 준비되면 오퍼레이터는 중단되었던 PyTorchJob 등의 커스텀 리소스를 감지하고, 동일한 설정으로 Pod를 다시 생성(Resubmission)하여 학습을 재개합니다. 

#### 3. 세분화된 복구 (Process-level Recovery) ####
2025년 최신 기술인 체크포인트리스(Checkpointless) 훈련 기능이 활성화된 경우 더욱 빠르게 복구됩니다.
프로세스 단위 재시작: 전체 Pod나 인프라를 다시 생성하는 대신, 실패한 학습 프로세스만 골라 재시작합니다.
P2P 상태 복구: 체크포인트를 파일 시스템에서 읽어오는 대신, 살아있는 다른 노드(Peer)로부터 모델 상태를 직접 전송받아 2분 이내에 학습을 재개할 수 있습니다. 

#### 4. 개발자가 설정해야 할 사항 ####
EKS 환경에서 이 기능이 작동하려면 작업 제출 시 다음 설정이 필요합니다.
HyperPod CLI 사용: 작업을 제출할 때 --auto-resume true 옵션을 포함해야 합니다.
YAML 설정: config.yaml 파일 내에 재시도 횟수(max-retry)와 복구 전략을 정의해야 합니다. 
결과적으로 EKS 환경의 HyperPod는 "고장 난 부품(노드)을 자동으로 갈아 끼우고, 쿠버네티스 관리자가 수동으로 조작하는 것처럼 시스템이 대신 학습 명령을 다시 내려주는 방식"으로 구현되어 있
