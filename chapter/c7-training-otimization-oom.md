* 리소스 설정 최적화: 훈련 작업의 중앙값 메모리 사용량을 Request로, 최대 사용량에 여유분을 더한 값을 Limit으로 설정하세요.
* QoS(Quality of Service) 관리: 파드에 Guaranteed 클래스를 부여하여 자원을 보장받고 축출 우선순위에서 밀려나지 않게 설정하는 것이 중요합니다.

 
### 체크포인트 최적화 ###
분산 훈련 중 체크포인트를 저장할 때 메모리 피크가 발생하지 않도록 비동기 저장 방식이나 오프로딩 기술을 검토해야 한다. 체크포인트 저장 시 메모리 점유율이 급증하는 이유는 단순히 데이터를 옮기는 과정 때문이 아니라, "저장하는 순간의 모델 상태를 copy 하기 위해 시스템 메모리(RAM)를 경유지로 사용"하기 때문이다. 쉽게 말해 GPU에 있던 모델을 파일로 만들기 위해 잠시 CPU RAM에 올려두는 과정에서 메모리가 두 배로 필요하기 때문"에 피크가 발생하게 된다.


#### 1. CPU RAM으로의 데이터 복사 (Host Memory Staging) ####
대부분의 딥러닝 프레임워크(PyTorch 등)에서 체크포인트를 저장할 때, 데이터는 GPU VRAM → CPU RAM → Storage (SSD/NFS) 의 경로를 사용한다.
GPU에 있는 수십 GB의 파라미터와 옵티마이저 상태를 파일로 쓰기 전, 우선 CPU 메모리로 전부 복사하게 되는데 이때 훈련 프로세스가 점유하던 기존 RAM 외에 모델 크기만큼의 추가적인 RAM 공간이 순식간에 필요해지며 Peak(정점)를 찍게 된다.

#### 2. 직렬화(Serialization) 과정의 오버헤드 ####
데이터를 디스크에 저장 가능한 형태(예: .bin, .pt)로 변환하는 과정을 직렬화라고 한다. 이 과정에서 프레임워크는 원본 데이터의 복사본을 만들거나 임시 버퍼를 생성하는데, 
만약 모델 파라미터가 50GB라면, Pickle 같은 직렬화 도구가 작동하는 동안 일시적으로 그 이상의 메모리를 점유할 수 있다.

#### 3. 분산 환경에서의 '동시성' 문제 ####
분산 훈련(DDP, FSDP 등)에서는 여러 노드가 동시에 체크포인트를 저장하려고 시도한다. 모든 노드가 동시에 수백 GB의 데이터를 네트워크 스토리지(Lustre, NFS 등)로 밀어넣으면 쓰기 속도가 느려지게 되고, 
저장 속도가 느려지면, CPU RAM에 복사된 체크포인트 데이터가 해제되지 못하고 메모리에 오래 머물게 된다. 이 상태에서 다음 훈련 스텝(Iteration)이 시작되어 데이터 배치가 다시 RAM으로 들어오면, [메모리 중첩] 현상이 발생하여 OOM Kill이 발생하게 된다. 
