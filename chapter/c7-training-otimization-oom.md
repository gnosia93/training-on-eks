### Pod 리소스 설정 ###
resource request 와 limit 를 같은 값으로 유지하고, 체크 포인트에 대비해 request/limit 에 여유분을 가져간다. 
노드 전체의 메모리가 부족해지면 쿠버네티스 엔진(kubelet)은 먼저 추출할 파드 리스트를 아래와 같은 기준으로 섲렁한다. 
* BestEffort: (Resource 설정 없음) 가장 먼저 죽임.
* Burstable: (Request < Limit) 그다음으로 죽임.
* Guaranteed: (Request == Limit) 노드 시스템에 심각한 문제가 생기기 전까지 절대 건드리지 않음.
 
### 체크포인트 최적화 ###
분산 훈련 중 체크포인트를 저장할 때 메모리 피크가 발생하지 않도록 비동기 저장 방식이나 오프로딩 기술을 검토해야 한다. 체크포인트 저장 시 메모리 점유율이 급증하는 이유는 단순히 데이터를 옮기는 과정 때문이 아니라, "저장하는 순간의 모델 상태를 copy 하기 위해 시스템 메모리(RAM)를 경유지로 사용"하기 때문이다. 쉽게 말해 GPU에 있던 모델을 파일로 만들기 위해 잠시 CPU RAM에 올려두는 과정에서 메모리가 두 배로 필요하기 때문"에 피크가 발생하게 된다.

#### 1. CPU RAM으로의 데이터 복사 (Host Memory Staging) ####
대부분의 딥러닝 프레임워크(PyTorch 등)에서 체크포인트를 저장할 때, 데이터는 GPU VRAM → CPU RAM → Storage (SSD/NFS) 의 경로를 사용한다.
GPU에 있는 수십 GB의 파라미터와 옵티마이저 상태를 파일로 쓰기 전, 우선 CPU 메모리로 전부 복사하게 되는데 이때 훈련 프로세스가 점유하던 기존 RAM 외에 모델 크기만큼의 추가적인 RAM 공간이 순식간에 필요해지며 Peak(정점)를 찍게 된다.

#### 2. 직렬화(Serialization) 과정의 오버헤드 ####
데이터를 디스크에 저장 가능한 형태(예: .bin, .pt)로 변환하는 과정을 직렬화라고 한다. 이 과정에서 프레임워크는 원본 데이터의 복사본을 만들거나 임시 버퍼를 생성하는데, 
만약 모델 파라미터가 50GB라면, Pickle 같은 직렬화 도구가 작동하는 동안 일시적으로 그 이상의 메모리를 점유할 수 있다.

#### 3. 분산 환경에서의 '동시성' 문제 ####
분산 훈련(DDP, FSDP 등)에서는 여러 노드가 동시에 체크포인트를 저장하려고 시도한다. 모든 노드가 동시에 수백 GB의 데이터를 네트워크 스토리지(Lustre, NFS 등)로 밀어넣으면 쓰기 속도가 느려지게 되고, 
저장 속도가 느려지면, CPU RAM에 복사된 체크포인트 데이터가 해제되지 못하고 메모리에 오래 머물게 된다. 이 상태에서 다음 훈련 스텝(Iteration)이 시작되어 데이터 배치가 다시 RAM으로 들어오면, [메모리 중첩] 현상이 발생하여 OOM Kill이 발생하게 된다. 

_LLM처럼 모델 파라미터가 거대한 환경에서 메모리 피크(Peak) 문제를 근본적으로 해결하는 방법은 분산 체크포인팅(Distributed/Sharded Checkpointing) 이 유일하다._
