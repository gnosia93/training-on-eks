Model config LlamaConfig {
  "architectures": [
    "LlamaForCausalLM"
  ],
  "attention_bias": false,
  "attention_dropout": 0.0,
  "bos_token_id": 128000,
  "dtype": "bfloat16",
  "eos_token_id": 128001,
  "head_dim": 128,
  "hidden_act": "silu",
  "hidden_size": 4096,
  "initializer_range": 0.02,
  "intermediate_size": 14336,
  "max_position_embeddings": 8192,
  "mlp_bias": false,
  "model_type": "llama",
  "num_attention_heads": 32,
  "num_hidden_layers": 32,
  "num_key_value_heads": 8,
  "pretraining_tp": 1,
  "rms_norm_eps": 1e-05,
  "rope_scaling": null,
  "rope_theta": 500000.0,
  "tie_word_embeddings": false,
  "transformers_version": "4.57.6",
  "use_cache": true,
  "vocab_size": 128256
}

loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/tokenizer.json
loading file tokenizer.model from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/special_tokens_map.json
loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/tokenizer_config.json
loading file chat_template.jinja from cache at None
loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/tokenizer.json
loading file tokenizer.model from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/special_tokens_map.json
loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/tokenizer_config.json
loading file chat_template.jinja from cache at None
loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/tokenizer.json
loading file tokenizer.model from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/special_tokens_map.json
loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/tokenizer_config.json
loading file chat_template.jinja from cache at None
loading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/tokenizer.json
loading file tokenizer.model from cache at None
loading file added_tokens.json from cache at None
loading file special_tokens_map.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/special_tokens_map.json
loading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--meta-llama--Meta-Llama-3-8B/snapshots/8cde5ca8380496c9a6cc7ef3a8b46a0372a1d920/tokenizer_config.json
loading file chat_template.jinja from cache at None
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.
`torch_dtype` is deprecated! Use `dtype` instead!
Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

`torch_dtype` is deprecated! Use `dtype` instead!
`torch_dtype` is deprecated! Use `dtype` instead!
Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
`torch_dtype` is deprecated! Use `dtype` instead!
Instantiating LlamaForCausalLM model under default dtype torch.bfloat16.
Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

Generate config GenerationConfig {
  "bos_token_id": 128000,
  "eos_token_id": 128001
}

llama-3-8b-node-0-0:232:232 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
llama-3-8b-node-0-0:232:232 [0] NCCL INFO cudaDriverVersion 13000
llama-3-8b-node-0-0:232:232 [0] NCCL INFO NCCL version 2.27.3+cuda12.9
llama-3-8b-node-0-0:232:232 [0] NCCL INFO Comm config Blocking set to 1
llama-3-8b-node-0-0:235:235 [3] NCCL INFO cudaDriverVersion 13000
llama-3-8b-node-0-0:235:235 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
llama-3-8b-node-0-0:235:235 [3] NCCL INFO NCCL version 2.27.3+cuda12.9
llama-3-8b-node-0-0:235:235 [3] NCCL INFO Comm config Blocking set to 1
llama-3-8b-node-0-0:233:233 [1] NCCL INFO cudaDriverVersion 13000
llama-3-8b-node-0-0:234:234 [2] NCCL INFO cudaDriverVersion 13000
llama-3-8b-node-0-0:233:233 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
llama-3-8b-node-0-0:234:234 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
llama-3-8b-node-0-0:233:233 [1] NCCL INFO NCCL version 2.27.3+cuda12.9
llama-3-8b-node-0-0:234:234 [2] NCCL INFO NCCL version 2.27.3+cuda12.9
llama-3-8b-node-0-0:233:233 [1] NCCL INFO Comm config Blocking set to 1
llama-3-8b-node-0-0:234:234 [2] NCCL INFO Comm config Blocking set to 1
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net.so
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net.so
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/Plugin: Loaded net plugin Libfabric (v10)
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net.so
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net.so
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/Plugin: Loaded net plugin Libfabric (v10)
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/Plugin: Loaded net plugin Libfabric (v10)
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/Plugin: Loaded net plugin Libfabric (v10)
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v9 symbol.
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v9 symbol.
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v9 symbol.
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v9 symbol.
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
llama-3-8b-node-0-0:232:679 [0] NCCL INFO Successfully loaded external plugin libnccl-net.so
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
llama-3-8b-node-0-0:234:682 [2] NCCL INFO Successfully loaded external plugin libnccl-net.so
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
llama-3-8b-node-0-0:235:680 [3] NCCL INFO Successfully loaded external plugin libnccl-net.so
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
llama-3-8b-node-0-0:233:681 [1] NCCL INFO Successfully loaded external plugin libnccl-net.so
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.16.2
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI Using Libfabric version 2.1
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.16.2
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.16.2
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI Using Libfabric version 2.1
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI Using Libfabric version 2.1
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.16.2
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI Using Libfabric version 2.1
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI Using CUDA driver version 13000 with runtime 12090
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI Using CUDA driver version 13000 with runtime 12090
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI Using CUDA driver version 13000 with runtime 12090
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI Using CUDA driver version 13000 with runtime 12090
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI Configuring AWS-specific options
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI Configuring AWS-specific options
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI Configuring AWS-specific options
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI Configuring AWS-specific options
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI Setting provider_filter to efa
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI Setting provider_filter to efa
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI Setting provider_filter to efa
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI Setting provider_filter to efa
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI Running on g6e.48xlarge platform, NCCL_TOPO_FILE environment variable is already set to 
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI Running on g6e.48xlarge platform, NCCL_TOPO_FILE environment variable is already set to 
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI Running on g6e.48xlarge platform, NCCL_TOPO_FILE environment variable is already set to 
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI Running on g6e.48xlarge platform, NCCL_TOPO_FILE environment variable is already set to 
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI Internode latency set at 75.0 us
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI Internode latency set at 75.0 us
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI Internode latency set at 75.0 us
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI Internode latency set at 75.0 us
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI Selected provider is efa, fabric is efa (found 4 nics)
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI Selected provider is efa, fabric is efa (found 4 nics)
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI Selected provider is efa, fabric is efa (found 4 nics)
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI Selected provider is efa, fabric is efa (found 4 nics)
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI NIC group 0 device #0 0000:9b:00.0
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI NIC group 1 device #0 0000:9c:00.0
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI NIC group 2 device #0 0000:bc:00.0
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI NIC group 3 device #0 0000:bd:00.0
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI Selected provider is efa, fabric is efa (found 4 nics)
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI NIC group 0 device #0 0000:9b:00.0
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI NIC group 0 device #0 0000:9b:00.0
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI NIC group 1 device #0 0000:9c:00.0
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI NIC group 1 device #0 0000:9c:00.0
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI NIC group 2 device #0 0000:bc:00.0
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI NIC group 2 device #0 0000:bc:00.0
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI NIC group 3 device #0 0000:bd:00.0
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI NIC group 3 device #0 0000:bd:00.0
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI NIC group 0 device #0 0000:9b:00.0
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI NIC group 1 device #0 0000:9c:00.0
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI NIC group 2 device #0 0000:bc:00.0
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI Selected provider is efa, fabric is efa (found 4 nics)
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI NIC group 3 device #0 0000:bd:00.0
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI Selected provider is efa, fabric is efa (found 4 nics)
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI Creating one domain per process
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI Selected provider is efa, fabric is efa (found 4 nics)
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI GUID of rdmap155s0: 3b6e201200002d00
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI Creating one domain per process
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI Creating one domain per process
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI GUID of rdmap155s0: 3b6e201200002d00
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI GUID of rdmap155s0: 3b6e201200002d00
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI GUID for dev[0]: 00000000000000000a000b8d00000000
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI GUID of rdmap156s0: a7b5b9c000002d01
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI Creating one domain per process
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI GUID for dev[0]: 00000000000000000a000b8d00000000
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI GUID for dev[1]: 00000000000000000a000b8d00000001
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI GUID for dev[0]: 00000000000000000a000b8d00000000
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI GUID of rdmap188s0: 95f9fbea0000f001
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI GUID of rdmap155s0: 3b6e201200002d00
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI GUID of rdmap156s0: a7b5b9c000002d01
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI GUID of rdmap156s0: a7b5b9c000002d01
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI GUID for dev[2]: 00000000000000000a000b8d00000002
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI GUID of rdmap189s0: 2959b5a20000f000
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI GUID for dev[1]: 00000000000000000a000b8d00000001
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI GUID for dev[1]: 00000000000000000a000b8d00000001
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI GUID for dev[0]: 00000000000000000a000b8d00000000
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI GUID for dev[3]: 00000000000000000a000b8d00000003
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI GUID of rdmap188s0: 95f9fbea0000f001
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI GUID of rdmap188s0: 95f9fbea0000f001
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI GUID for dev[2]: 00000000000000000a000b8d00000002
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI GUID for dev[2]: 00000000000000000a000b8d00000002
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI GUID of rdmap156s0: a7b5b9c000002d01
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI GUID of rdmap189s0: 2959b5a20000f000
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI GUID of rdmap189s0: 2959b5a20000f000
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI GUID for dev[1]: 00000000000000000a000b8d00000001
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI GUID for dev[3]: 00000000000000000a000b8d00000003
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI GUID for dev[3]: 00000000000000000a000b8d00000003
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI GUID of rdmap188s0: 95f9fbea0000f001
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI GUID for dev[2]: 00000000000000000a000b8d00000002
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI GUID of rdmap189s0: 2959b5a20000f000
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI GUID for dev[3]: 00000000000000000a000b8d00000003
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI Could not disable CUDA API usage for HMEM, disabling GDR
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI Need to force simple protocol: byte delivery ordering not supported
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI Need to force simple protocol: byte delivery ordering not supported
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI Need to force simple protocol: byte delivery ordering not supported
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI Setting FI_OPT_EFA_SENDRECV_IN_ORDER_ALIGNED_128_BYTES not supported.
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI Need to force simple protocol: byte delivery ordering not supported
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI Support for global registrations: false
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI Support for global registrations: false
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI Support for global registrations: false
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI Support for global registrations: false
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI Support for DMA-BUF registrations: false
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI Need to force simple protocol: GDR not supported
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI Adding FI_EFA_FORK_SAFE=1 to environment
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI Adding NCCL_PROTO=simple to environment
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI Adding NCCL_TUNER_PLUGIN=libnccl-net.so to environment
llama-3-8b-node-0-0:235:680 [3] NCCL INFO Initialized NET plugin Libfabric
llama-3-8b-node-0-0:235:680 [3] NCCL INFO Assigned NET plugin Libfabric to comm
llama-3-8b-node-0-0:235:680 [3] NCCL INFO Using network Libfabric
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI Need to force simple protocol: GDR not supported
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI Adding FI_EFA_FORK_SAFE=1 to environment
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI Adding NCCL_PROTO=simple to environment
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI Adding NCCL_TUNER_PLUGIN=libnccl-net.so to environment
llama-3-8b-node-0-0:234:682 [2] NCCL INFO Initialized NET plugin Libfabric
llama-3-8b-node-0-0:234:682 [2] NCCL INFO Assigned NET plugin Libfabric to comm
llama-3-8b-node-0-0:234:682 [2] NCCL INFO Using network Libfabric
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI Need to force simple protocol: GDR not supported
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI Adding FI_EFA_FORK_SAFE=1 to environment
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI Adding NCCL_PROTO=simple to environment
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI Adding NCCL_TUNER_PLUGIN=libnccl-net.so to environment
llama-3-8b-node-0-0:233:681 [1] NCCL INFO Initialized NET plugin Libfabric
llama-3-8b-node-0-0:233:681 [1] NCCL INFO Assigned NET plugin Libfabric to comm
llama-3-8b-node-0-0:233:681 [1] NCCL INFO Using network Libfabric
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI Need to force simple protocol: GDR not supported
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI Adding FI_EFA_FORK_SAFE=1 to environment
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI Adding NCCL_PROTO=simple to environment
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI Adding NCCL_TUNER_PLUGIN=libnccl-net.so to environment
llama-3-8b-node-0-0:232:679 [0] NCCL INFO Initialized NET plugin Libfabric
llama-3-8b-node-0-0:232:679 [0] NCCL INFO Assigned NET plugin Libfabric to comm
llama-3-8b-node-0-0:232:679 [0] NCCL INFO Using network Libfabric
llama-3-8b-node-0-0:233:681 [1] NCCL INFO DMA-BUF is available on GPU device 1
llama-3-8b-node-0-0:234:682 [2] NCCL INFO DMA-BUF is available on GPU device 2
llama-3-8b-node-0-0:233:681 [1] NCCL INFO ncclCommInitRankConfig comm 0x55e27e4dafc0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId a0000 commId 0x44729c4d1bbe1c01 - Init START
llama-3-8b-node-0-0:234:682 [2] NCCL INFO ncclCommInitRankConfig comm 0x561dded8cb10 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId a2000 commId 0x44729c4d1bbe1c01 - Init START
llama-3-8b-node-0-0:235:680 [3] NCCL INFO DMA-BUF is available on GPU device 3
llama-3-8b-node-0-0:235:680 [3] NCCL INFO ncclCommInitRankConfig comm 0x564750bb9610 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId a4000 commId 0x44729c4d1bbe1c01 - Init START
llama-3-8b-node-0-0:232:679 [0] NCCL INFO DMA-BUF is available on GPU device 0
llama-3-8b-node-0-0:232:679 [0] NCCL INFO ncclCommInitRankConfig comm 0x55a2dce56a80 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 9e000 commId 0x44729c4d1bbe1c01 - Init START
llama-3-8b-node-0-0:234:682 [2] NCCL INFO RAS client listening socket at ::1<28028>
llama-3-8b-node-0-0:235:680 [3] NCCL INFO RAS client listening socket at ::1<28028>
llama-3-8b-node-0-0:233:681 [1] NCCL INFO RAS client listening socket at ::1<28028>
llama-3-8b-node-0-0:232:679 [0] NCCL INFO RAS client listening socket at ::1<28028>
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to NVL
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to NVL
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to NVL
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to NVL
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NCCL_TOPO_FILE set by environment to 
llama-3-8b-node-0-0:233:681 [1] NCCL INFO Could not open XML topology file  : No such file or directory
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NCCL_TOPO_FILE set by environment to 
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NCCL_TOPO_FILE set by environment to 
llama-3-8b-node-0-0:235:680 [3] NCCL INFO Could not open XML topology file  : No such file or directory
llama-3-8b-node-0-0:234:682 [2] NCCL INFO Could not open XML topology file  : No such file or directory
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NCCL_TOPO_FILE set by environment to 
llama-3-8b-node-0-0:232:679 [0] NCCL INFO Could not open XML topology file  : No such file or directory
llama-3-8b-node-0-0:233:681 [1] NCCL INFO TOPO/NET : Importing network plugins to topology
llama-3-8b-node-0-0:234:682 [2] NCCL INFO TOPO/NET : Importing network plugins to topology
llama-3-8b-node-0-0:233:681 [1] NCCL INFO Retrieving state for Libfabric
llama-3-8b-node-0-0:234:682 [2] NCCL INFO Retrieving state for Libfabric
llama-3-8b-node-0-0:233:681 [1] NCCL INFO Initialized state 0 for Libfabric
llama-3-8b-node-0-0:234:682 [2] NCCL INFO Initialized state 0 for Libfabric
llama-3-8b-node-0-0:235:680 [3] NCCL INFO TOPO/NET : Importing network plugins to topology
llama-3-8b-node-0-0:235:680 [3] NCCL INFO Retrieving state for Libfabric
llama-3-8b-node-0-0:235:680 [3] NCCL INFO Initialized state 0 for Libfabric
llama-3-8b-node-0-0:232:679 [0] NCCL INFO TOPO/NET : Importing network plugins to topology
llama-3-8b-node-0-0:232:679 [0] NCCL INFO Retrieving state for Libfabric
llama-3-8b-node-0-0:232:679 [0] NCCL INFO Initialized state 0 for Libfabric
llama-3-8b-node-0-0:234:682 [2] NCCL INFO ncclTopoPopulateNics : Filled rdmap155s0 in topo with pciPath=/sys/devices/pci0000:84/0000:84:00.0/0000:85:00.0/0000:86:02.4/0000:9b:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:233:681 [1] NCCL INFO ncclTopoPopulateNics : Filled rdmap155s0 in topo with pciPath=/sys/devices/pci0000:84/0000:84:00.0/0000:85:00.0/0000:86:02.4/0000:9b:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:235:680 [3] NCCL INFO ncclTopoPopulateNics : Filled rdmap155s0 in topo with pciPath=/sys/devices/pci0000:84/0000:84:00.0/0000:85:00.0/0000:86:02.4/0000:9b:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:232:679 [0] NCCL INFO ncclTopoPopulateNics : Filled rdmap155s0 in topo with pciPath=/sys/devices/pci0000:84/0000:84:00.0/0000:85:00.0/0000:86:02.4/0000:9b:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:234:682 [2] NCCL INFO ncclTopoPopulateNics : Filled rdmap156s0 in topo with pciPath=/sys/devices/pci0000:84/0000:84:00.0/0000:85:00.0/0000:86:02.5/0000:9c:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:233:681 [1] NCCL INFO ncclTopoPopulateNics : Filled rdmap156s0 in topo with pciPath=/sys/devices/pci0000:84/0000:84:00.0/0000:85:00.0/0000:86:02.5/0000:9c:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:235:680 [3] NCCL INFO ncclTopoPopulateNics : Filled rdmap156s0 in topo with pciPath=/sys/devices/pci0000:84/0000:84:00.0/0000:85:00.0/0000:86:02.5/0000:9c:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:232:679 [0] NCCL INFO ncclTopoPopulateNics : Filled rdmap156s0 in topo with pciPath=/sys/devices/pci0000:84/0000:84:00.0/0000:85:00.0/0000:86:02.5/0000:9c:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:233:681 [1] NCCL INFO ncclTopoPopulateNics : Filled rdmap188s0 in topo with pciPath=/sys/devices/pci0000:a5/0000:a5:00.0/0000:a6:00.0/0000:a7:02.4/0000:bc:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:234:682 [2] NCCL INFO ncclTopoPopulateNics : Filled rdmap188s0 in topo with pciPath=/sys/devices/pci0000:a5/0000:a5:00.0/0000:a6:00.0/0000:a7:02.4/0000:bc:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:232:679 [0] NCCL INFO ncclTopoPopulateNics : Filled rdmap188s0 in topo with pciPath=/sys/devices/pci0000:a5/0000:a5:00.0/0000:a6:00.0/0000:a7:02.4/0000:bc:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:235:680 [3] NCCL INFO ncclTopoPopulateNics : Filled rdmap188s0 in topo with pciPath=/sys/devices/pci0000:a5/0000:a5:00.0/0000:a6:00.0/0000:a7:02.4/0000:bc:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:234:682 [2] NCCL INFO ncclTopoPopulateNics : Filled rdmap189s0 in topo with pciPath=/sys/devices/pci0000:a5/0000:a5:00.0/0000:a6:00.0/0000:a7:02.5/0000:bd:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:232:679 [0] NCCL INFO ncclTopoPopulateNics : Filled rdmap189s0 in topo with pciPath=/sys/devices/pci0000:a5/0000:a5:00.0/0000:a6:00.0/0000:a7:02.5/0000:bd:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:235:680 [3] NCCL INFO ncclTopoPopulateNics : Filled rdmap189s0 in topo with pciPath=/sys/devices/pci0000:a5/0000:a5:00.0/0000:a6:00.0/0000:a7:02.5/0000:bd:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:233:681 [1] NCCL INFO ncclTopoPopulateNics : Filled rdmap189s0 in topo with pciPath=/sys/devices/pci0000:a5/0000:a5:00.0/0000:a6:00.0/0000:a7:02.5/0000:bd:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:235:680 [3] NCCL INFO === System : maxBw 12.0 totalBw 12.0 ===
llama-3-8b-node-0-0:235:680 [3] NCCL INFO CPU/0-0 (1/2/-1)
llama-3-8b-node-0-0:235:680 [3] NCCL INFO + PCI[24.0] - PCI/0-85000 (1d0f02001d0f0200)
llama-3-8b-node-0-0:235:680 [3] NCCL INFO               + PCI[24.0] - NIC/0-9b000
llama-3-8b-node-0-0:235:680 [3] NCCL INFO               + PCI[24.0] - NIC/0-9c000
llama-3-8b-node-0-0:235:680 [3] NCCL INFO + SYS[16.0] - CPU/0-1
llama-3-8b-node-0-0:235:680 [3] NCCL INFO + PCI[12.0] - GPU/0-9e000 (0)
llama-3-8b-node-0-0:235:680 [3] NCCL INFO + PCI[12.0] - GPU/0-a0000 (1)
llama-3-8b-node-0-0:235:680 [3] NCCL INFO + PCI[12.0] - GPU/0-a2000 (2)
llama-3-8b-node-0-0:235:680 [3] NCCL INFO + PCI[12.0] - GPU/0-a4000 (3)
llama-3-8b-node-0-0:235:680 [3] NCCL INFO CPU/0-1 (1/2/-1)
llama-3-8b-node-0-0:235:680 [3] NCCL INFO + PCI[24.0] - PCI/0-a6000 (1d0f02001d0f0200)
llama-3-8b-node-0-0:235:680 [3] NCCL INFO               + PCI[24.0] - NIC/0-bc000
llama-3-8b-node-0-0:235:680 [3] NCCL INFO               + PCI[24.0] - NIC/0-bd000
llama-3-8b-node-0-0:235:680 [3] NCCL INFO + SYS[16.0] - CPU/0-0
llama-3-8b-node-0-0:235:680 [3] NCCL INFO ==========================================
llama-3-8b-node-0-0:235:680 [3] NCCL INFO GPU/0-9e000 :GPU/0-9e000 (0/5000.0/LOC) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:235:680 [3] NCCL INFO GPU/0-a0000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (0/5000.0/LOC) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:235:680 [3] NCCL INFO GPU/0-a2000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (0/5000.0/LOC) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:235:680 [3] NCCL INFO GPU/0-a4000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (0/5000.0/LOC) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:235:680 [3] NCCL INFO Setting affinity for GPU 3 to 0-47,96-143
llama-3-8b-node-0-0:233:681 [1] NCCL INFO === System : maxBw 12.0 totalBw 12.0 ===
llama-3-8b-node-0-0:233:681 [1] NCCL INFO CPU/0-0 (1/2/-1)
llama-3-8b-node-0-0:233:681 [1] NCCL INFO + PCI[24.0] - PCI/0-85000 (1d0f02001d0f0200)
llama-3-8b-node-0-0:233:681 [1] NCCL INFO               + PCI[24.0] - NIC/0-9b000
llama-3-8b-node-0-0:233:681 [1] NCCL INFO               + PCI[24.0] - NIC/0-9c000
llama-3-8b-node-0-0:233:681 [1] NCCL INFO + SYS[16.0] - CPU/0-1
llama-3-8b-node-0-0:233:681 [1] NCCL INFO + PCI[12.0] - GPU/0-9e000 (0)
llama-3-8b-node-0-0:233:681 [1] NCCL INFO + PCI[12.0] - GPU/0-a0000 (1)
llama-3-8b-node-0-0:233:681 [1] NCCL INFO + PCI[12.0] - GPU/0-a2000 (2)
llama-3-8b-node-0-0:233:681 [1] NCCL INFO + PCI[12.0] - GPU/0-a4000 (3)
llama-3-8b-node-0-0:233:681 [1] NCCL INFO CPU/0-1 (1/2/-1)
llama-3-8b-node-0-0:233:681 [1] NCCL INFO + PCI[24.0] - PCI/0-a6000 (1d0f02001d0f0200)
llama-3-8b-node-0-0:233:681 [1] NCCL INFO               + PCI[24.0] - NIC/0-bc000
llama-3-8b-node-0-0:233:681 [1] NCCL INFO               + PCI[24.0] - NIC/0-bd000
llama-3-8b-node-0-0:233:681 [1] NCCL INFO + SYS[16.0] - CPU/0-0
llama-3-8b-node-0-0:233:681 [1] NCCL INFO ==========================================
llama-3-8b-node-0-0:233:681 [1] NCCL INFO GPU/0-9e000 :GPU/0-9e000 (0/5000.0/LOC) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:233:681 [1] NCCL INFO GPU/0-a0000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (0/5000.0/LOC) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:233:681 [1] NCCL INFO GPU/0-a2000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (0/5000.0/LOC) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:233:681 [1] NCCL INFO GPU/0-a4000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (0/5000.0/LOC) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:233:681 [1] NCCL INFO Setting affinity for GPU 1 to 0-47,96-143
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NVLS multicast support is not available on dev 3 (NVLS_NCHANNELS 0)
llama-3-8b-node-0-0:234:682 [2] NCCL INFO === System : maxBw 12.0 totalBw 12.0 ===
llama-3-8b-node-0-0:234:682 [2] NCCL INFO CPU/0-0 (1/2/-1)
llama-3-8b-node-0-0:234:682 [2] NCCL INFO + PCI[24.0] - PCI/0-85000 (1d0f02001d0f0200)
llama-3-8b-node-0-0:234:682 [2] NCCL INFO               + PCI[24.0] - NIC/0-9b000
llama-3-8b-node-0-0:234:682 [2] NCCL INFO               + PCI[24.0] - NIC/0-9c000
llama-3-8b-node-0-0:234:682 [2] NCCL INFO + SYS[16.0] - CPU/0-1
llama-3-8b-node-0-0:234:682 [2] NCCL INFO + PCI[12.0] - GPU/0-9e000 (0)
llama-3-8b-node-0-0:234:682 [2] NCCL INFO + PCI[12.0] - GPU/0-a0000 (1)
llama-3-8b-node-0-0:234:682 [2] NCCL INFO + PCI[12.0] - GPU/0-a2000 (2)
llama-3-8b-node-0-0:234:682 [2] NCCL INFO + PCI[12.0] - GPU/0-a4000 (3)
llama-3-8b-node-0-0:234:682 [2] NCCL INFO CPU/0-1 (1/2/-1)
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NVLS multicast support is not available on dev 1 (NVLS_NCHANNELS 0)
llama-3-8b-node-0-0:234:682 [2] NCCL INFO + PCI[24.0] - PCI/0-a6000 (1d0f02001d0f0200)
llama-3-8b-node-0-0:234:682 [2] NCCL INFO               + PCI[24.0] - NIC/0-bc000
llama-3-8b-node-0-0:234:682 [2] NCCL INFO               + PCI[24.0] - NIC/0-bd000
llama-3-8b-node-0-0:234:682 [2] NCCL INFO + SYS[16.0] - CPU/0-0
llama-3-8b-node-0-0:234:682 [2] NCCL INFO ==========================================
llama-3-8b-node-0-0:234:682 [2] NCCL INFO GPU/0-9e000 :GPU/0-9e000 (0/5000.0/LOC) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:234:682 [2] NCCL INFO GPU/0-a0000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (0/5000.0/LOC) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:234:682 [2] NCCL INFO GPU/0-a2000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (0/5000.0/LOC) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:234:682 [2] NCCL INFO GPU/0-a4000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (0/5000.0/LOC) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:234:682 [2] NCCL INFO Setting affinity for GPU 2 to 0-47,96-143
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NVLS multicast support is not available on dev 2 (NVLS_NCHANNELS 0)
llama-3-8b-node-0-0:235:680 [3] NCCL INFO Pattern 4, crossNic 0, nChannels 1, bw 12.000000/12.000000, type PHB/PIX, sameChannels 1
llama-3-8b-node-0-0:235:680 [3] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
llama-3-8b-node-0-0:235:680 [3] NCCL INFO Pattern 1, crossNic 0, nChannels 1, bw 12.000000/12.000000, type PHB/PIX, sameChannels 1
llama-3-8b-node-0-0:233:681 [1] NCCL INFO Pattern 4, crossNic 0, nChannels 1, bw 12.000000/12.000000, type PHB/PIX, sameChannels 1
llama-3-8b-node-0-0:235:680 [3] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
llama-3-8b-node-0-0:233:681 [1] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
llama-3-8b-node-0-0:233:681 [1] NCCL INFO Pattern 1, crossNic 0, nChannels 1, bw 12.000000/12.000000, type PHB/PIX, sameChannels 1
llama-3-8b-node-0-0:233:681 [1] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
llama-3-8b-node-0-0:234:682 [2] NCCL INFO Pattern 4, crossNic 0, nChannels 1, bw 12.000000/12.000000, type PHB/PIX, sameChannels 1
llama-3-8b-node-0-0:234:682 [2] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
llama-3-8b-node-0-0:232:679 [0] NCCL INFO === System : maxBw 12.0 totalBw 12.0 ===
llama-3-8b-node-0-0:232:679 [0] NCCL INFO CPU/0-0 (1/2/-1)
llama-3-8b-node-0-0:232:679 [0] NCCL INFO + PCI[24.0] - PCI/0-85000 (1d0f02001d0f0200)
llama-3-8b-node-0-0:232:679 [0] NCCL INFO               + PCI[24.0] - NIC/0-9b000
llama-3-8b-node-0-0:232:679 [0] NCCL INFO               + PCI[24.0] - NIC/0-9c000
llama-3-8b-node-0-0:232:679 [0] NCCL INFO + SYS[16.0] - CPU/0-1
llama-3-8b-node-0-0:234:682 [2] NCCL INFO Pattern 1, crossNic 0, nChannels 1, bw 12.000000/12.000000, type PHB/PIX, sameChannels 1
llama-3-8b-node-0-0:232:679 [0] NCCL INFO + PCI[12.0] - GPU/0-9e000 (0)
llama-3-8b-node-0-0:232:679 [0] NCCL INFO + PCI[12.0] - GPU/0-a0000 (1)
llama-3-8b-node-0-0:232:679 [0] NCCL INFO + PCI[12.0] - GPU/0-a2000 (2)
llama-3-8b-node-0-0:234:682 [2] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
llama-3-8b-node-0-0:232:679 [0] NCCL INFO + PCI[12.0] - GPU/0-a4000 (3)
llama-3-8b-node-0-0:232:679 [0] NCCL INFO CPU/0-1 (1/2/-1)
llama-3-8b-node-0-0:232:679 [0] NCCL INFO + PCI[24.0] - PCI/0-a6000 (1d0f02001d0f0200)
llama-3-8b-node-0-0:232:679 [0] NCCL INFO               + PCI[24.0] - NIC/0-bc000
llama-3-8b-node-0-0:232:679 [0] NCCL INFO               + PCI[24.0] - NIC/0-bd000
llama-3-8b-node-0-0:232:679 [0] NCCL INFO + SYS[16.0] - CPU/0-0
llama-3-8b-node-0-0:232:679 [0] NCCL INFO ==========================================
llama-3-8b-node-0-0:232:679 [0] NCCL INFO GPU/0-9e000 :GPU/0-9e000 (0/5000.0/LOC) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:232:679 [0] NCCL INFO GPU/0-a0000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (0/5000.0/LOC) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:232:679 [0] NCCL INFO GPU/0-a2000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (0/5000.0/LOC) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:232:679 [0] NCCL INFO GPU/0-a4000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (0/5000.0/LOC) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:232:679 [0] NCCL INFO Setting affinity for GPU 0 to 0-47,96-143
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NVLS multicast support is not available on dev 0 (NVLS_NCHANNELS 0)
llama-3-8b-node-0-0:232:679 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 1, bw 12.000000/12.000000, type PHB/PIX, sameChannels 1
llama-3-8b-node-0-0:232:679 [0] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
llama-3-8b-node-0-0:232:679 [0] NCCL INFO Pattern 1, crossNic 0, nChannels 1, bw 12.000000/12.000000, type PHB/PIX, sameChannels 1
llama-3-8b-node-0-0:232:679 [0] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
llama-3-8b-node-0-0:235:680 [3] NCCL INFO comm 0x564750bb9610 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
llama-3-8b-node-0-0:234:682 [2] NCCL INFO comm 0x561dded8cb10 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0
llama-3-8b-node-0-0:233:681 [1] NCCL INFO comm 0x55e27e4dafc0 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0
llama-3-8b-node-0-0:232:679 [0] NCCL INFO comm 0x55a2dce56a80 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0
llama-3-8b-node-0-0:233:681 [1] NCCL INFO Tree 0 : 0 -> 1 -> 2/-1/-1
llama-3-8b-node-0-0:232:679 [0] NCCL INFO Tree 0 : -1 -> 0 -> 1/-1/-1
llama-3-8b-node-0-0:233:681 [1] NCCL INFO Tree 1 : 0 -> 1 -> 2/-1/-1
llama-3-8b-node-0-0:234:682 [2] NCCL INFO Ring 00 : 1 -> 2 -> 3
llama-3-8b-node-0-0:232:679 [0] NCCL INFO Tree 1 : -1 -> 0 -> 1/-1/-1
llama-3-8b-node-0-0:235:680 [3] NCCL INFO Ring 00 : 2 -> 3 -> 0
llama-3-8b-node-0-0:234:682 [2] NCCL INFO Ring 01 : 1 -> 2 -> 3
llama-3-8b-node-0-0:235:680 [3] NCCL INFO Ring 01 : 2 -> 3 -> 0
llama-3-8b-node-0-0:234:682 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
llama-3-8b-node-0-0:233:681 [1] NCCL INFO Ring 00 : 0 -> 1 -> 2
llama-3-8b-node-0-0:235:680 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
llama-3-8b-node-0-0:232:679 [0] NCCL INFO Channel 00/02 : 0 1 2 3
llama-3-8b-node-0-0:233:681 [1] NCCL INFO Ring 01 : 0 -> 1 -> 2
llama-3-8b-node-0-0:234:682 [2] NCCL INFO P2P Chunksize set to 131072
llama-3-8b-node-0-0:235:680 [3] NCCL INFO P2P Chunksize set to 131072
llama-3-8b-node-0-0:233:681 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
llama-3-8b-node-0-0:232:679 [0] NCCL INFO Channel 01/02 : 0 1 2 3
llama-3-8b-node-0-0:233:681 [1] NCCL INFO P2P Chunksize set to 131072
llama-3-8b-node-0-0:232:679 [0] NCCL INFO Ring 00 : 3 -> 0 -> 1
llama-3-8b-node-0-0:232:679 [0] NCCL INFO Ring 01 : 3 -> 0 -> 1
llama-3-8b-node-0-0:232:679 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
llama-3-8b-node-0-0:232:679 [0] NCCL INFO P2P Chunksize set to 131072
llama-3-8b-node-0-0:232:679 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
llama-3-8b-node-0-0:234:682 [2] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
llama-3-8b-node-0-0:235:680 [3] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
llama-3-8b-node-0-0:233:681 [1] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
llama-3-8b-node-0-0:232:679 [0] NCCL INFO Check P2P Type isAllDirectP2p 0 directMode 0
llama-3-8b-node-0-0:232:695 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 109
llama-3-8b-node-0-0:235:692 [3] NCCL INFO [Proxy Service] Device 3 CPU core 43
llama-3-8b-node-0-0:235:696 [3] NCCL INFO [Proxy Service UDS] Device 3 CPU core 132
llama-3-8b-node-0-0:233:698 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 32
llama-3-8b-node-0-0:232:691 [0] NCCL INFO [Proxy Service] Device 0 CPU core 43
llama-3-8b-node-0-0:234:697 [2] NCCL INFO [Proxy Service UDS] Device 2 CPU core 132
llama-3-8b-node-0-0:233:693 [1] NCCL INFO [Proxy Service] Device 1 CPU core 43
llama-3-8b-node-0-0:234:694 [2] NCCL INFO [Proxy Service] Device 2 CPU core 43
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NCCL_PROTO set by environment to simple
llama-3-8b-node-0-0:233:681 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
llama-3-8b-node-0-0:233:681 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NCCL_PROTO set by environment to simple
llama-3-8b-node-0-0:232:679 [0] NCCL INFO Enabled NCCL Func/Proto/Algo Matrix:
     Function |       LL     LL128    Simple   |          Tree           Ring  CollNetDirect   CollNetChain           NVLS       NVLSTree            PAT  
    Broadcast |        0         0         1   |             1              1              1              1              1              1              1  
       Reduce |        0         0         1   |             1              1              1              1              1              1              1  
    AllGather |        0         0         1   |             1              1              1              1              1              1              1  
ReduceScatter |        0         0         1   |             1              1              1              1              1              1              1  
    AllReduce |        0         0         1   |             1              1              1              1              1              1              1  

llama-3-8b-node-0-0:232:679 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
llama-3-8b-node-0-0:232:679 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NCCL_PROTO set by environment to simple
llama-3-8b-node-0-0:234:682 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
llama-3-8b-node-0-0:234:682 [2] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NCCL_PROTO set by environment to simple
llama-3-8b-node-0-0:235:680 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
llama-3-8b-node-0-0:235:680 [3] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
llama-3-8b-node-0-0:232:679 [0] NCCL INFO CC Off, workFifoBytes 1048576
llama-3-8b-node-0-0:232:679 [0] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net.so
llama-3-8b-node-0-0:232:679 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v4 symbol.
llama-3-8b-node-0-0:232:679 [0] NCCL INFO TUNER/Plugin: Using tuner plugin nccl_ofi_tuner
llama-3-8b-node-0-0:232:679 [0] NCCL INFO NET/OFI NCCL_OFI_TUNER is not available for platform : g6e.48xlarge, Fall back to NCCL's tuner
llama-3-8b-node-0-0:232:679 [0] NCCL INFO ncclCommInitRankConfig comm 0x55a2dce56a80 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 9e000 commId 0x44729c4d1bbe1c01 - Init COMPLETE
llama-3-8b-node-0-0:232:679 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 4 total 2.10 (kernels 0.27, alloc 1.80, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
llama-3-8b-node-0-0:234:682 [2] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net.so
llama-3-8b-node-0-0:234:682 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v4 symbol.
llama-3-8b-node-0-0:234:682 [2] NCCL INFO TUNER/Plugin: Using tuner plugin nccl_ofi_tuner
llama-3-8b-node-0-0:233:681 [1] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net.so
llama-3-8b-node-0-0:234:682 [2] NCCL INFO NET/OFI NCCL_OFI_TUNER is not available for platform : g6e.48xlarge, Fall back to NCCL's tuner
llama-3-8b-node-0-0:234:682 [2] NCCL INFO ncclCommInitRankConfig comm 0x561dded8cb10 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId a2000 commId 0x44729c4d1bbe1c01 - Init COMPLETE
llama-3-8b-node-0-0:235:680 [3] NCCL INFO TUNER/Plugin: Plugin name set by env to libnccl-net.so
llama-3-8b-node-0-0:233:681 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v4 symbol.
llama-3-8b-node-0-0:233:681 [1] NCCL INFO TUNER/Plugin: Using tuner plugin nccl_ofi_tuner
llama-3-8b-node-0-0:234:682 [2] NCCL INFO Init timings - ncclCommInitRankConfig: rank 2 nranks 4 total 2.08 (kernels 0.25, alloc 1.80, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.00, connections 0.01, rest 0.00)
llama-3-8b-node-0-0:235:680 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v4 symbol.
llama-3-8b-node-0-0:235:680 [3] NCCL INFO TUNER/Plugin: Using tuner plugin nccl_ofi_tuner
llama-3-8b-node-0-0:233:681 [1] NCCL INFO NET/OFI NCCL_OFI_TUNER is not available for platform : g6e.48xlarge, Fall back to NCCL's tuner
llama-3-8b-node-0-0:233:681 [1] NCCL INFO ncclCommInitRankConfig comm 0x55e27e4dafc0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId a0000 commId 0x44729c4d1bbe1c01 - Init COMPLETE
llama-3-8b-node-0-0:235:680 [3] NCCL INFO NET/OFI NCCL_OFI_TUNER is not available for platform : g6e.48xlarge, Fall back to NCCL's tuner
llama-3-8b-node-0-0:235:680 [3] NCCL INFO ncclCommInitRankConfig comm 0x564750bb9610 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId a4000 commId 0x44729c4d1bbe1c01 - Init COMPLETE
llama-3-8b-node-0-0:233:681 [1] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 4 total 2.08 (kernels 0.25, alloc 1.80, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.00, connections 0.01, rest 0.00)
llama-3-8b-node-0-0:235:680 [3] NCCL INFO Init timings - ncclCommInitRankConfig: rank 3 nranks 4 total 2.09 (kernels 0.26, alloc 1.80, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.00, connections 0.01, rest 0.00)
llama-3-8b-node-0-0:232:699 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct
llama-3-8b-node-0-0:233:701 [1] NCCL INFO Channel 00 : 1[1] -> 2[2] via SHM/direct/direct
llama-3-8b-node-0-0:235:702 [3] NCCL INFO Channel 00 : 3[3] -> 0[0] via SHM/direct/direct
llama-3-8b-node-0-0:234:700 [2] NCCL INFO Channel 00 : 2[2] -> 3[3] via SHM/direct/direct
llama-3-8b-node-0-0:232:699 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct
llama-3-8b-node-0-0:234:700 [2] NCCL INFO Channel 01 : 2[2] -> 3[3] via SHM/direct/direct
llama-3-8b-node-0-0:235:702 [3] NCCL INFO Channel 01 : 3[3] -> 0[0] via SHM/direct/direct
llama-3-8b-node-0-0:233:701 [1] NCCL INFO Channel 01 : 1[1] -> 2[2] via SHM/direct/direct
llama-3-8b-node-0-0:233:701 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
llama-3-8b-node-0-0:232:699 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
llama-3-8b-node-0-0:235:702 [3] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
llama-3-8b-node-0-0:234:700 [2] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 582441.99 examples/s]
Generating train split: 100%|██████████| 36718/36718 [00:00<00:00, 1229317.63 examples/s]
Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 1029209.88 examples/s]
Map: 100%|██████████| 36718/36718 [00:01<00:00, 20985.11 examples/s]
PyTorch: setting up devices
Map: 100%|██████████| 36718/36718 [00:01<00:00, 20232.84 examples/s]
PyTorch: setting up devices
Map: 100%|██████████| 36718/36718 [00:01<00:00, 19410.40 examples/s]
PyTorch: setting up devices
Map:  82%|████████▏ | 30000/36718 [00:01<00:00, 9557.60 examples/s] max_steps is given, it will override any value given in num_train_epochs
Using auto half precision backend
훈련 소요시간 기록 시작: 1769411439.12s
Map: 100%|██████████| 36718/36718 [00:02<00:00, 16306.11 examples/s]
PyTorch: setting up devices
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 4. Using DeepSpeed's value.
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[rank3]:W0126 07:10:42.882000 235 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank3]:W0126 07:10:42.882000 235 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
Adam Optimizer #0 is created with AVX2 arithmetic capability.
Config: alpha=0.000020, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
Stage 3 initialize beginning
MA 0.0 GB         Max_MA 0.98 GB         CA 0.0 GB         Max_CA 1 GB 
CPU Virtual Memory:  used = 82.67 GB, percent = 5.5%
DeepSpeedZeRoOffload initialize [begin]
MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
CPU Virtual Memory:  used = 82.66 GB, percent = 5.5%
llama-3-8b-node-0-0:235:1734 [3] NCCL INFO Channel 00 : 3[3] -> 2[2] via SHM/direct/direct
Parameter Offload - Persistent parameters statistics: param_count = 65, numel = 266240
llama-3-8b-node-0-0:235:1734 [3] NCCL INFO Channel 01 : 3[3] -> 2[2] via SHM/direct/direct
llama-3-8b-node-0-0:234:1735 [2] NCCL INFO Channel 00 : 2[2] -> 1[1] via SHM/direct/direct
llama-3-8b-node-0-0:233:1736 [1] NCCL INFO Channel 00 : 1[1] -> 0[0] via SHM/direct/direct
llama-3-8b-node-0-0:234:1735 [2] NCCL INFO Channel 01 : 2[2] -> 1[1] via SHM/direct/direct
llama-3-8b-node-0-0:233:1736 [1] NCCL INFO Channel 01 : 1[1] -> 0[0] via SHM/direct/direct
llama-3-8b-node-0-0:235:1734 [3] NCCL INFO Connected all trees
DeepSpeedZeRoOffload initialize [end]
MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
CPU Virtual Memory:  used = 82.65 GB, percent = 5.5%
Before creating fp16 partitions
MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
CPU Virtual Memory:  used = 82.64 GB, percent = 5.5%
/usr/local/lib/python3.12/site-packages/torch/distributed/distributed_c10d.py:4807: UserWarning: No device id is provided via `init_process_group` or `barrier `. Using the current device set by the user. 
  warnings.warn(  # warn only once
llama-3-8b-node-0-0:232:1737 [0] NCCL INFO Connected all trees
llama-3-8b-node-0-0:233:1736 [1] NCCL INFO Connected all trees
llama-3-8b-node-0-0:234:1735 [2] NCCL INFO Connected all trees
After creating fp16 partitions: 19
MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
CPU Virtual Memory:  used = 100.6 GB, percent = 6.7%
Before creating fp32 partitions
MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
CPU Virtual Memory:  used = 101.91 GB, percent = 6.8%
After creating fp32 partitions
MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
CPU Virtual Memory:  used = 130.7 GB, percent = 8.8%
Before initializing optimizer states
MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
CPU Virtual Memory:  used = 130.69 GB, percent = 8.8%
After initializing optimizer states
MA 0.0 GB         Max_MA 0.0 GB         CA 0.0 GB         Max_CA 0 GB 
CPU Virtual Memory:  used = 169.23 GB, percent = 11.3%
After initializing ZeRO optimizer
MA 0.09 GB         Max_MA 2.05 GB         CA 2.05 GB         Max_CA 2 GB 
CPU Virtual Memory:  used = 183.86 GB, percent = 12.3%
***** Running training *****
  Num examples = 36,718
  Num Epochs = 1
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 4
  Total optimization steps = 50
  Number of trainable parameters = 8,030,261,248
  0%|          | 0/50 [00:00<?, ?it/s]`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.
llama-3-8b-node-0-0:232:1783 [0] NCCL INFO Comm config Blocking set to 1
llama-3-8b-node-0-0:233:1788 [1] NCCL INFO Comm config Blocking set to 1
llama-3-8b-node-0-0:235:1794 [3] NCCL INFO Comm config Blocking set to 1
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO Assigned NET plugin Libfabric to comm
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO Using network Libfabric
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO DMA-BUF is available on GPU device 0
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO ncclCommInitRankConfig comm 0x7f58f40687f0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 9e000 commId 0xad1935d57cfceef1 - Init START
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO Assigned NET plugin Libfabric to comm
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO Using network Libfabric
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO DMA-BUF is available on GPU device 3
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO Assigned NET plugin Libfabric to comm
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO Using network Libfabric
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO DMA-BUF is available on GPU device 1
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO ncclCommInitRankConfig comm 0x7fb6c0065b70 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId a4000 commId 0xad1935d57cfceef1 - Init START
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO ncclCommInitRankConfig comm 0x7f62c406aa10 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId a0000 commId 0xad1935d57cfceef1 - Init START
llama-3-8b-node-0-0:234:1793 [2] NCCL INFO Comm config Blocking set to 1
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO Assigned NET plugin Libfabric to comm
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO Using network Libfabric
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO DMA-BUF is available on GPU device 2
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO ncclCommInitRankConfig comm 0x7fcd2894dd40 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId a2000 commId 0xad1935d57cfceef1 - Init START
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO NCCL_TOPO_FILE set by environment to 
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO NCCL_TOPO_FILE set by environment to 
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO NCCL_TOPO_FILE set by environment to 
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO NCCL_TOPO_FILE set by environment to 
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO Could not open XML topology file  : No such file or directory
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO Could not open XML topology file  : No such file or directory
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO Could not open XML topology file  : No such file or directory
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO Could not open XML topology file  : No such file or directory
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO TOPO/NET : Importing network plugins to topology
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO Retrieving state for Libfabric
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO TOPO/NET : Importing network plugins to topology
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO Retrieving state for Libfabric
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO TOPO/NET : Importing network plugins to topology
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO TOPO/NET : Importing network plugins to topology
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO Retrieving state for Libfabric
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO Retrieving state for Libfabric
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO ncclTopoPopulateNics : Filled rdmap155s0 in topo with pciPath=/sys/devices/pci0000:84/0000:84:00.0/0000:85:00.0/0000:86:02.4/0000:9b:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO ncclTopoPopulateNics : Filled rdmap155s0 in topo with pciPath=/sys/devices/pci0000:84/0000:84:00.0/0000:85:00.0/0000:86:02.4/0000:9b:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO ncclTopoPopulateNics : Filled rdmap155s0 in topo with pciPath=/sys/devices/pci0000:84/0000:84:00.0/0000:85:00.0/0000:86:02.4/0000:9b:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO ncclTopoPopulateNics : Filled rdmap155s0 in topo with pciPath=/sys/devices/pci0000:84/0000:84:00.0/0000:85:00.0/0000:86:02.4/0000:9b:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO ncclTopoPopulateNics : Filled rdmap156s0 in topo with pciPath=/sys/devices/pci0000:84/0000:84:00.0/0000:85:00.0/0000:86:02.5/0000:9c:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO ncclTopoPopulateNics : Filled rdmap156s0 in topo with pciPath=/sys/devices/pci0000:84/0000:84:00.0/0000:85:00.0/0000:86:02.5/0000:9c:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO ncclTopoPopulateNics : Filled rdmap156s0 in topo with pciPath=/sys/devices/pci0000:84/0000:84:00.0/0000:85:00.0/0000:86:02.5/0000:9c:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO ncclTopoPopulateNics : Filled rdmap156s0 in topo with pciPath=/sys/devices/pci0000:84/0000:84:00.0/0000:85:00.0/0000:86:02.5/0000:9c:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO ncclTopoPopulateNics : Filled rdmap188s0 in topo with pciPath=/sys/devices/pci0000:a5/0000:a5:00.0/0000:a6:00.0/0000:a7:02.4/0000:bc:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO ncclTopoPopulateNics : Filled rdmap188s0 in topo with pciPath=/sys/devices/pci0000:a5/0000:a5:00.0/0000:a6:00.0/0000:a7:02.4/0000:bc:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO ncclTopoPopulateNics : Filled rdmap188s0 in topo with pciPath=/sys/devices/pci0000:a5/0000:a5:00.0/0000:a6:00.0/0000:a7:02.4/0000:bc:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO ncclTopoPopulateNics : Filled rdmap188s0 in topo with pciPath=/sys/devices/pci0000:a5/0000:a5:00.0/0000:a6:00.0/0000:a7:02.4/0000:bc:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO ncclTopoPopulateNics : Filled rdmap189s0 in topo with pciPath=/sys/devices/pci0000:a5/0000:a5:00.0/0000:a6:00.0/0000:a7:02.5/0000:bd:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO ncclTopoPopulateNics : Filled rdmap189s0 in topo with pciPath=/sys/devices/pci0000:a5/0000:a5:00.0/0000:a6:00.0/0000:a7:02.5/0000:bd:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO ncclTopoPopulateNics : Filled rdmap189s0 in topo with pciPath=/sys/devices/pci0000:a5/0000:a5:00.0/0000:a6:00.0/0000:a7:02.5/0000:bd:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO ncclTopoPopulateNics : Filled rdmap189s0 in topo with pciPath=/sys/devices/pci0000:a5/0000:a5:00.0/0000:a6:00.0/0000:a7:02.5/0000:bd:00.0 keep=1 coll=(null)
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO === System : maxBw 12.0 totalBw 12.0 ===
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO CPU/0-0 (1/2/-1)
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO + PCI[24.0] - PCI/0-85000 (1d0f02001d0f0200)
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO               + PCI[24.0] - NIC/0-9b000
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO               + PCI[24.0] - NIC/0-9c000
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO + SYS[16.0] - CPU/0-1
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO + PCI[12.0] - GPU/0-9e000 (0)
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO + PCI[12.0] - GPU/0-a0000 (1)
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO + PCI[12.0] - GPU/0-a2000 (2)
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO + PCI[12.0] - GPU/0-a4000 (3)
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO CPU/0-1 (1/2/-1)
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO + PCI[24.0] - PCI/0-a6000 (1d0f02001d0f0200)
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO               + PCI[24.0] - NIC/0-bc000
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO               + PCI[24.0] - NIC/0-bd000
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO + SYS[16.0] - CPU/0-0
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO ==========================================
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO GPU/0-9e000 :GPU/0-9e000 (0/5000.0/LOC) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO GPU/0-a0000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (0/5000.0/LOC) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO GPU/0-a2000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (0/5000.0/LOC) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO GPU/0-a4000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (0/5000.0/LOC) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO Setting affinity for GPU 3 to 0-47,96-143
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO NVLS multicast support is not available on dev 3 (NVLS_NCHANNELS 0)
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO === System : maxBw 12.0 totalBw 12.0 ===
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO CPU/0-0 (1/2/-1)
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO + PCI[24.0] - PCI/0-85000 (1d0f02001d0f0200)
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO               + PCI[24.0] - NIC/0-9b000
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO               + PCI[24.0] - NIC/0-9c000
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO + SYS[16.0] - CPU/0-1
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO + PCI[12.0] - GPU/0-9e000 (0)
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO + PCI[12.0] - GPU/0-a0000 (1)
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO + PCI[12.0] - GPU/0-a2000 (2)
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO + PCI[12.0] - GPU/0-a4000 (3)
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO CPU/0-1 (1/2/-1)
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO + PCI[24.0] - PCI/0-a6000 (1d0f02001d0f0200)
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO               + PCI[24.0] - NIC/0-bc000
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO               + PCI[24.0] - NIC/0-bd000
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO + SYS[16.0] - CPU/0-0
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO ==========================================
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO GPU/0-9e000 :GPU/0-9e000 (0/5000.0/LOC) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO GPU/0-a0000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (0/5000.0/LOC) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO GPU/0-a2000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (0/5000.0/LOC) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO GPU/0-a4000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (0/5000.0/LOC) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO Setting affinity for GPU 1 to 0-47,96-143
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO NVLS multicast support is not available on dev 1 (NVLS_NCHANNELS 0)
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO Pattern 4, crossNic 0, nChannels 1, bw 12.000000/12.000000, type PHB/PIX, sameChannels 1
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO Pattern 1, crossNic 0, nChannels 1, bw 12.000000/12.000000, type PHB/PIX, sameChannels 1
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO Pattern 4, crossNic 0, nChannels 1, bw 12.000000/12.000000, type PHB/PIX, sameChannels 1
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO === System : maxBw 12.0 totalBw 12.0 ===
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO CPU/0-0 (1/2/-1)
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO + PCI[24.0] - PCI/0-85000 (1d0f02001d0f0200)
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO               + PCI[24.0] - NIC/0-9b000
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO               + PCI[24.0] - NIC/0-9c000
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO + SYS[16.0] - CPU/0-1
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO + PCI[12.0] - GPU/0-9e000 (0)
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO + PCI[12.0] - GPU/0-a0000 (1)
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO + PCI[12.0] - GPU/0-a2000 (2)
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO + PCI[12.0] - GPU/0-a4000 (3)
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO CPU/0-1 (1/2/-1)
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO + PCI[24.0] - PCI/0-a6000 (1d0f02001d0f0200)
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO               + PCI[24.0] - NIC/0-bc000
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO               + PCI[24.0] - NIC/0-bd000
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO + SYS[16.0] - CPU/0-0
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO ==========================================
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO Pattern 1, crossNic 0, nChannels 1, bw 12.000000/12.000000, type PHB/PIX, sameChannels 1
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO GPU/0-9e000 :GPU/0-9e000 (0/5000.0/LOC) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO GPU/0-a0000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (0/5000.0/LOC) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO GPU/0-a2000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (0/5000.0/LOC) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO GPU/0-a4000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (0/5000.0/LOC) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO Setting affinity for GPU 2 to 0-47,96-143
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO NVLS multicast support is not available on dev 2 (NVLS_NCHANNELS 0)
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO === System : maxBw 12.0 totalBw 12.0 ===
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO CPU/0-0 (1/2/-1)
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO + PCI[24.0] - PCI/0-85000 (1d0f02001d0f0200)
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO               + PCI[24.0] - NIC/0-9b000
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO               + PCI[24.0] - NIC/0-9c000
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO + SYS[16.0] - CPU/0-1
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO + PCI[12.0] - GPU/0-9e000 (0)
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO + PCI[12.0] - GPU/0-a0000 (1)
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO + PCI[12.0] - GPU/0-a2000 (2)
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO + PCI[12.0] - GPU/0-a4000 (3)
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO CPU/0-1 (1/2/-1)
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO + PCI[24.0] - PCI/0-a6000 (1d0f02001d0f0200)
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO               + PCI[24.0] - NIC/0-bc000
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO               + PCI[24.0] - NIC/0-bd000
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO + SYS[16.0] - CPU/0-0
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO ==========================================
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO GPU/0-9e000 :GPU/0-9e000 (0/5000.0/LOC) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO GPU/0-a0000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (0/5000.0/LOC) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO GPU/0-a2000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (0/5000.0/LOC) GPU/0-a4000 (2/12.0/PHB) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO GPU/0-a4000 :GPU/0-9e000 (2/12.0/PHB) GPU/0-a0000 (2/12.0/PHB) GPU/0-a2000 (2/12.0/PHB) GPU/0-a4000 (0/5000.0/LOC) CPU/0-0 (1/12.0/PHB) CPU/0-1 (2/12.0/SYS) 
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO Setting affinity for GPU 0 to 0-47,96-143
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO NVLS multicast support is not available on dev 0 (NVLS_NCHANNELS 0)
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO Pattern 4, crossNic 0, nChannels 1, bw 12.000000/12.000000, type PHB/PIX, sameChannels 1
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO Pattern 1, crossNic 0, nChannels 1, bw 12.000000/12.000000, type PHB/PIX, sameChannels 1
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO Pattern 4, crossNic 0, nChannels 1, bw 12.000000/12.000000, type PHB/PIX, sameChannels 1
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO Pattern 1, crossNic 0, nChannels 1, bw 12.000000/12.000000, type PHB/PIX, sameChannels 1
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO  0 : GPU/0 GPU/1 GPU/2 GPU/3
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO comm 0x7fcd2894dd40 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO comm 0x7fb6c0065b70 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO comm 0x7f62c406aa10 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO comm 0x7f58f40687f0 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO Ring 00 : 2 -> 3 -> 0
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO Ring 00 : 1 -> 2 -> 3
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO Tree 0 : 0 -> 1 -> 2/-1/-1
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO Ring 01 : 2 -> 3 -> 0
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO Tree 0 : -1 -> 0 -> 1/-1/-1
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO Tree 1 : 0 -> 1 -> 2/-1/-1
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO Ring 01 : 1 -> 2 -> 3
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO Tree 1 : -1 -> 0 -> 1/-1/-1
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO P2P Chunksize set to 131072
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO P2P Chunksize set to 131072
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO Ring 00 : 0 -> 1 -> 2
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO Channel 00/02 : 0 1 2 3
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO Ring 01 : 0 -> 1 -> 2
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO Channel 01/02 : 0 1 2 3
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO P2P Chunksize set to 131072
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO Ring 00 : 3 -> 0 -> 1
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO Ring 01 : 3 -> 0 -> 1
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO P2P Chunksize set to 131072
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO Check P2P Type isAllDirectP2p 0 directMode 0
llama-3-8b-node-0-0:235:1820 [3] NCCL INFO [Proxy Service] Device 3 CPU core 27
llama-3-8b-node-0-0:234:1825 [2] NCCL INFO [Proxy Service UDS] Device 2 CPU core 39
llama-3-8b-node-0-0:235:1824 [3] NCCL INFO [Proxy Service UDS] Device 3 CPU core 20
llama-3-8b-node-0-0:233:1827 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 128
llama-3-8b-node-0-0:232:1826 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 33
llama-3-8b-node-0-0:234:1821 [2] NCCL INFO [Proxy Service] Device 2 CPU core 27
llama-3-8b-node-0-0:232:1822 [0] NCCL INFO [Proxy Service] Device 0 CPU core 27
llama-3-8b-node-0-0:233:1823 [1] NCCL INFO [Proxy Service] Device 1 CPU core 27
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO NCCL_PROTO set by environment to simple
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO NCCL_PROTO set by environment to simple
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO NCCL_PROTO set by environment to simple
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO NCCL_PROTO set by environment to simple
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO Enabled NCCL Func/Proto/Algo Matrix:
     Function |       LL     LL128    Simple   |          Tree           Ring  CollNetDirect   CollNetChain           NVLS       NVLSTree            PAT  
    Broadcast |        0         0         1   |             1              1              1              1              1              1              1  
       Reduce |        0         0         1   |             1              1              1              1              1              1              1  
    AllGather |        0         0         1   |             1              1              1              1              1              1              1  
ReduceScatter |        0         0         1   |             1              1              1              1              1              1              1  
    AllReduce |        0         0         1   |             1              1              1              1              1              1              1  

llama-3-8b-node-0-0:232:1816 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO 2 coll channels, 2 collnet channels, 0 nvls channels, 2 p2p channels, 2 p2p channels per peer
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO CC Off, workFifoBytes 1048576
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO NET/OFI NCCL_OFI_TUNER is not available for platform : g6e.48xlarge, Fall back to NCCL's tuner
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO NET/OFI NCCL_OFI_TUNER is not available for platform : g6e.48xlarge, Fall back to NCCL's tuner
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO NET/OFI NCCL_OFI_TUNER is not available for platform : g6e.48xlarge, Fall back to NCCL's tuner
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO ncclCommInitRankConfig comm 0x7fb6c0065b70 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId a4000 commId 0xad1935d57cfceef1 - Init COMPLETE
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO ncclCommInitRankConfig comm 0x7f62c406aa10 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId a0000 commId 0xad1935d57cfceef1 - Init COMPLETE
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO NET/OFI NCCL_OFI_TUNER is not available for platform : g6e.48xlarge, Fall back to NCCL's tuner
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO ncclCommInitRankConfig comm 0x7fcd2894dd40 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId a2000 commId 0xad1935d57cfceef1 - Init COMPLETE
llama-3-8b-node-0-0:235:1818 [3] NCCL INFO Init timings - ncclCommInitRankConfig: rank 3 nranks 4 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.02, allgathers 0.00, topo 0.01, graphs 0.00, connections 0.01, rest 0.00)
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO ncclCommInitRankConfig comm 0x7f58f40687f0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 9e000 commId 0xad1935d57cfceef1 - Init COMPLETE
llama-3-8b-node-0-0:233:1817 [1] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 4 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.02, allgathers 0.00, topo 0.01, graphs 0.00, connections 0.00, rest 0.00)
llama-3-8b-node-0-0:234:1819 [2] NCCL INFO Init timings - ncclCommInitRankConfig: rank 2 nranks 4 total 0.03 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.01, graphs 0.00, connections 0.00, rest 0.00)
llama-3-8b-node-0-0:232:1816 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 4 total 0.04 (kernels 0.00, alloc 0.00, bootstrap 0.02, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.01, rest 0.00)
llama-3-8b-node-0-0:232:1830 [0] NCCL INFO Channel 00 : 0[0] -> 1[1] via SHM/direct/direct
llama-3-8b-node-0-0:235:1831 [3] NCCL INFO Channel 00 : 3[3] -> 0[0] via SHM/direct/direct
llama-3-8b-node-0-0:233:1828 [1] NCCL INFO Channel 00 : 1[1] -> 2[2] via SHM/direct/direct
llama-3-8b-node-0-0:234:1829 [2] NCCL INFO Channel 00 : 2[2] -> 3[3] via SHM/direct/direct
llama-3-8b-node-0-0:232:1830 [0] NCCL INFO Channel 01 : 0[0] -> 1[1] via SHM/direct/direct
llama-3-8b-node-0-0:235:1831 [3] NCCL INFO Channel 01 : 3[3] -> 0[0] via SHM/direct/direct
llama-3-8b-node-0-0:234:1829 [2] NCCL INFO Channel 01 : 2[2] -> 3[3] via SHM/direct/direct
llama-3-8b-node-0-0:233:1828 [1] NCCL INFO Channel 01 : 1[1] -> 2[2] via SHM/direct/direct
llama-3-8b-node-0-0:234:1829 [2] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
llama-3-8b-node-0-0:232:1830 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
llama-3-8b-node-0-0:233:1828 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
llama-3-8b-node-0-0:235:1831 [3] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
llama-3-8b-node-0-0:235:1877 [3] NCCL INFO Channel 00 : 3[3] -> 2[2] via SHM/direct/direct
llama-3-8b-node-0-0:235:1877 [3] NCCL INFO Channel 01 : 3[3] -> 2[2] via SHM/direct/direct
llama-3-8b-node-0-0:234:1876 [2] NCCL INFO Channel 00 : 2[2] -> 1[1] via SHM/direct/direct
llama-3-8b-node-0-0:234:1876 [2] NCCL INFO Channel 01 : 2[2] -> 1[1] via SHM/direct/direct
llama-3-8b-node-0-0:233:1879 [1] NCCL INFO Channel 00 : 1[1] -> 0[0] via SHM/direct/direct
llama-3-8b-node-0-0:233:1879 [1] NCCL INFO Channel 01 : 1[1] -> 0[0] via SHM/direct/direct
llama-3-8b-node-0-0:235:1877 [3] NCCL INFO Connected all trees
llama-3-8b-node-0-0:232:1878 [0] NCCL INFO Connected all trees
llama-3-8b-node-0-0:234:1876 [2] NCCL INFO Connected all trees
llama-3-8b-node-0-0:233:1879 [1] NCCL INFO Connected all trees

[Step 5] GPU 3 Memory: Allocated: 0.11GB, Reserved: 7.16GB, Peak: 3.29GB
[Step 5] GPU 2 Memory: Allocated: 0.11GB, Reserved: 6.43GB, Peak: 3.29GB


[Step 5] GPU 0 Memory: Allocated: 0.11GB, Reserved: 8.10GB, Peak: 3.29GB

[Step 5] Memory: 0.11GB | Net Sent: 30.4MB, Recv: 30.4MB | Throughput: 0.2MB/s
[Step 5] Memory: 0.11GB | Net Sent: 30.4MB, Recv: 30.4MB | Throughput: 0.2MB/s


[Step 5] Memory: 0.11GB | Net Sent: 30.4MB, Recv: 30.4MB | Throughput: 0.2MB/s

[Step 5] GPU 1 Memory: Allocated: 0.11GB, Reserved: 6.43GB, Peak: 3.29GB

[Step 5] Memory: 0.11GB | Net Sent: 30.4MB, Recv: 30.4MB | Throughput: 0.2MB/s
{'loss': 10.0959, 'grad_norm': 6.115786075592041, 'learning_rate': 1.8400000000000003e-05, 'epoch': 0.0, 'time': '07:15:41'}

[Step 10] GPU 0 Memory: Allocated: 0.11GB, Reserved: 8.10GB, Peak: 3.29GB

[Step 10] GPU 3 Memory: Allocated: 0.11GB, Reserved: 7.16GB, Peak: 3.29GB

[Step 10] Memory: 0.11GB | Net Sent: 0.6MB, Recv: 0.6MB | Throughput: 0.0MB/s

[Step 10] Memory: 0.11GB | Net Sent: 0.6MB, Recv: 0.6MB | Throughput: 0.0MB/s

[Step 10] GPU 2 Memory: Allocated: 0.11GB, Reserved: 6.43GB, Peak: 3.29GB

[Step 10] GPU 1 Memory: Allocated: 0.11GB, Reserved: 6.43GB, Peak: 3.29GB

[Step 10] Memory: 0.11GB | Net Sent: 0.6MB, Recv: 0.6MB | Throughput: 0.0MB/s

[Step 10] Memory: 0.11GB | Net Sent: 0.6MB, Recv: 0.6MB | Throughput: 0.0MB/s
{'loss': 8.7966, 'grad_norm': 5.96052360534668, 'learning_rate': 1.64e-05, 'epoch': 0.0, 'time': '07:19:21'}

[Step 15] GPU 0 Memory: Allocated: 0.11GB, Reserved: 8.10GB, Peak: 3.29GB

[Step 15] GPU 3 Memory: Allocated: 0.11GB, Reserved: 7.16GB, Peak: 3.29GB

[Step 15] GPU 1 Memory: Allocated: 0.11GB, Reserved: 6.43GB, Peak: 3.29GB

[Step 15] Memory: 0.11GB | Net Sent: 0.6MB, Recv: 0.6MB | Throughput: 0.0MB/s

[Step 15] Memory: 0.11GB | Net Sent: 0.6MB, Recv: 0.6MB | Throughput: 0.0MB/s

[Step 15] Memory: 0.11GB | Net Sent: 0.6MB, Recv: 0.6MB | Throughput: 0.0MB/s

[Step 15] GPU 2 Memory: Allocated: 0.11GB, Reserved: 6.43GB, Peak: 3.29GB

[Step 15] Memory: 0.11GB | Net Sent: 0.6MB, Recv: 0.6MB | Throughput: 0.0MB/s
{'loss': 8.424, 'grad_norm': 4.528095245361328, 'learning_rate': 1.4400000000000001e-05, 'epoch': 0.01, 'time': '07:23:04'}

[Step 20] GPU 3 Memory: Allocated: 0.11GB, Reserved: 7.16GB, Peak: 3.29GB

[Step 20] GPU 1 Memory: Allocated: 0.11GB, Reserved: 6.43GB, Peak: 3.29GB

[Step 20] GPU 0 Memory: Allocated: 0.11GB, Reserved: 8.10GB, Peak: 3.29GB

[Step 20] Memory: 0.11GB | Net Sent: 0.6MB, Recv: 0.6MB | Throughput: 0.0MB/s

[Step 20] Memory: 0.11GB | Net Sent: 0.6MB, Recv: 0.6MB | Throughput: 0.0MB/s

[Step 20] GPU 2 Memory: Allocated: 0.11GB, Reserved: 6.43GB, Peak: 3.29GB

[Step 20] Memory: 0.11GB | Net Sent: 0.6MB, Recv: 0.6MB | Throughput: 0.0MB/s

[Step 20] Memory: 0.11GB | Net Sent: 0.6MB, Recv: 0.6MB | Throughput: 0.0MB/s
{'loss': 8.1776, 'grad_norm': 4.685335636138916, 'learning_rate': 1.2400000000000002e-05, 'epoch': 0.01, 'time': '07:26:48'}

[Step 25] GPU 1 Memory: Allocated: 0.11GB, Reserved: 6.43GB, Peak: 3.29GB

[Step 25] GPU 0 Memory: Allocated: 0.11GB, Reserved: 8.10GB, Peak: 3.29GB

[Step 25] Memory: 0.11GB | Net Sent: 0.6MB, Recv: 0.6MB | Throughput: 0.0MB/s

[Step 25] GPU 3 Memory: Allocated: 0.11GB, Reserved: 7.16GB, Peak: 3.29GB

[Step 25] Memory: 0.11GB | Net Sent: 0.6MB, Recv: 0.6MB | Throughput: 0.0MB/s

[Step 25] GPU 2 Memory: Allocated: 0.11GB, Reserved: 6.43GB, Peak: 3.29GB

[Step 25] Memory: 0.11GB | Net Sent: 0.6MB, Recv: 0.6MB | Throughput: 0.0MB/s

[Step 25] Memory: 0.11GB | Net Sent: 0.6MB, Recv: 0.6MB | Throughput: 0.0MB/s
{'loss': 7.8922, 'grad_norm': 3.0508933067321777, 'learning_rate': 1.04e-05, 'epoch': 0.01, 'time': '07:30:32'}

[Step 30] GPU 1 Memory: Allocated: 0.11GB, Reserved: 6.43GB, Peak: 3.29GB

[Step 30] GPU 0 Memory: Allocated: 0.11GB, Reserved: 8.10GB, Peak: 3.29GB
[Step 30] Memory: 0.11GB | Net Sent: 0.6MB, Recv: 0.6MB | Throughput: 0.0MB/s


[Step 30] GPU 3 Memory: Allocated: 0.11GB, Reserved: 7.16GB, Peak: 3.29GB

[Step 30] Memory: 0.11GB | Net Sent: 0.6MB, Recv: 0.6MB | Throughput: 0.0MB/s
