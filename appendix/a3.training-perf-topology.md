## 토폴로지별 훈련성능 ##

* Llama-3-8B 
* DeepSpeed Stage 3                         

### deepspeed 설정 ###
```
{
    "fp16": { "enabled": false },
    "bf16": { "enabled": true },
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": { "device": "cpu", "pin_memory": true },
        "offload_param": { "device": "cpu", "pin_memory": true },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e8,
        "reduce_bucket_size": 5e7,
        "stage3_prefetch_bucket_size": 5e7,
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e8,
        "stage3_max_reuse_distance": 1e8,
        "stage3_gather_16bit_weights_on_model_save": true
    },
    "gradient_clipping": "auto",
    "steps_per_print": 10,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "gradient_accumulation_steps": "auto",
    "wall_clock_breakdown": false
}
```
### trainer 설정 ###
```
per_device_train_batch_size=1,                 
gradient_accumulation_steps=4,                 
learning_rate=2e-5,
max_steps=50,                                     
num_train_epochs=1,
bf16=True,                                     
logging_steps=5,
deepspeed="llama-3-8b-stage3.json", 
save_strategy="no",                            
gradient_checkpointing=True,                   
```
```
***** Running training *****
  Num examples = 36,718
  Num Epochs = 1
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 4
  Total optimization steps = 50
  Number of trainable parameters = 8,030,261,248
```

### Pod Spec ###
```
  containers:
    env:
    - name: NCCL_P2P_LEVEL
      value: NVL
    image: public.ecr.aws/deep-learning-containers/pytorch-training:2.8.0-gpu-py312-cu129-ubuntu22.04-ec2-v1.0
    resources:
      limits:
        nvidia.com/gpu: "4"
        vpc.amazonaws.com/efa: "4"
    securityContext:
      capabilities:
        add:
        - IPC_LOCK
    volumeMounts:
    - mountPath: /dev/shm
      name: dshm
  hostIPC: true
  nodeSelector:
    node.kubernetes.io/instance-type: p4d.24xlarge
  shareProcessNamespace: true
  tolerations:
  - effect: NoSchedule
    key: nvidia.com/gpu
    operator: Exists
  - effect: NoSchedule
    key: vpc.amazonaws.com/efa
    operator: Exists
  volumes:
  - emptyDir:
      medium: Memory
      sizeLimit: 64Gi
    name: dshm
```

## 훈련성능 측정결과 ##

### p4d.24xlarge ###

* 1 Pod [4 GPU / 4 EFA] - 0:16:21 ( via P2P/CUMEM/read, P2P Type isAllDirectP2p 1 directMode 0)
* 4 Pod [1 GPU / 1 EFA] - 0:39:32 ( via NET/Libfabric/0/GDRDMA )        
* 4 Pod [1 GPU / 1 ENI] - ???     ( via  )        


## P2P 의 이해 ## 

* Write 모드 (Push): 송신 GPU가 데이터를 상대방 메모리에 직접 밀어 넣습니다. NVLink의 대역폭을 거의 100% 활용하며, 이론상 수백 GB/s의 속도를 냅니다. 즉시 전송이 시작되므로 지연 시간이 극도로 낮습니다
* Read 모드 (Pull): 수신 GPU가 상대방 메모리를 읽어옵니다. 가상화 계층의 간섭과 데이터 확인 절차(Handshake) 때문에 대역폭이 30%~50% 이상 급감할 수 있습니다. "데이터가 준비되었는가?" 확인 -> "읽기 요청" -> "데이터 수신"이라는 복잡한 단계를 거치므로 지연 시간이 훨씬 깁니다. 8B 모델처럼 수많은 파라미터를 빈번하게 교환해야 하는 환경에서는 이 미세한 지연이 누적되어 큰 차이를 만듭니다.

#### 니트로 EC2의 태생적 한계 ####

* Nitro 하이퍼바이저가 보안과 격리를 위해 GPU 간의 직접 쓰기(Direct Write) 권한을 제한
* 하드웨어는 NVLink라는 초고속 도로를 갖고 있지만, 규제(가상화 설정) 때문에 속도 제한이 걸린 read 모드로만 달려야 하는 상황.

## 멀티 노드 동기화 ##

8개의 GPU가 NVLink로 묶여 있어도, 노드 밖의 파라미터를 기다려야 하는 순간 NVLink의 초고속 성능은 "무용지물"이 된다.

#### 1. 동기화의 함정: "가장 느린 놈이 전체 속도를 정한다" ####
DeepSpeed ZeRO-3 같은 데이터 병렬 처리에서 16개 GPU(2개 노드)가 학습할 때, 한 걸음(Step)을 마치려면 16개 GPU의 파라미터가 모두 일치해야 한다.
* 노드 내부(1~8번 GPU): NVLink로 눈 깜짝할 새 대화를 끝낸다.
* 노드 외부(9~16번 GPU): EFA라는 상대적으로 좁은 통로를 통해 데이터를 주고 받는다.
* 결과: 1 ~ 8번 GPU는 자기들끼리 대화를 마쳤어도, 9 ~ 16번 GPU의 데이터가 EFA를 타고 올 때까지 아무것도 못 하고 멍하니 기다려야 함. (Barrier Wait)

#### 2. NVLink가 억울한 이유 ####
NVLink가 아무리 600GB/s로 데이터를 쏠 수 있어도, 데이터를 받는 상대방 노드로 가는 길이 50GB/s(EFA)뿐이라면, NVLink는 자신의 성능의 10%도 쓰지 못하고 EFA 속도에 맞춰서 데이터를 조금씩 흘려보낼 수밖에 없다.

#### 3. 왜 멀티 노드에서 효율이 급락하는가? ####
단일 노드에서는 모든 통신이 NVLink 전용 도로에서 일어나지만, 멀티 노드가 되는 순간 모든 통신 프로세스가 'EFA라는 톨게이트'를 통과해야 하는 구조로 바뀜.
* 노드 1대일 때 속도: NVLink 속도 (매우 빠름)
* 노드 2대 이상일 때 속도: EFA 속도 + 네트워크 지연 시간(Latency) + 동기화 오버헤드 (갑자기 느려짐)

#### 4. 그래서 전문가들이 내리는 결론 ####
"노드를 늘리는 순간 NVLink는 무용지물"이 되는 현상을 막기 위해, 고도로 설계된 학습 환경(예: NVIDIA SuperPOD)에서는 인피니밴드(InfiniBand)를 GPU 개수만큼 꽂아서 인터노드 대역폭을 NVLink 급으로 올리려고 수십억 원을 투자함.
하지만 일반적인 EFA 환경에서는 대역폭 차이가 극명하므로, 앞서 논의한 것처럼 "노드 간(EFA)에는 통신을 거의 안 하게 만들고, 노드 내(NVLink)에서만 통신이 폭발하게 만드는" TP/PP 전략이 무조건 강제되는 것.

## NVIDIA의 메가트론(Megatron-LM) ## 
NVIDIA의 메가트론(Megatron-LM)은 바로 그 '복잡하고 고통스러운 병렬화 설계'를 구조적으로 해결하기 위해 탄생한 프레임워크이다.
단순히 자동화라고 하기엔 개발자가 초기에 지정해줘야 할 파라미터가 있지만, "토폴로지에 최적화된 병렬 연산 그래프를 생성하고 통신을 스케줄링"한다는 점에서 좋은 솔루션이다.

#### 1. 토폴로지 맞춤형 설계 (Intra-node vs Inter-node) ####
* 메가트론은 실행 시 사용자가 입력한 TP(Tensor Parallel)와 PP(Pipeline Parallel) 크기를 바탕으로 GPU들을 그룹핑
* TP 그룹 (노드 내부): "너희 8개는 NVLink로 묶여 있으니, 모델의 레이어 하나를 조각조각 나눠서 같이 계산해!" (가장 빈번한 통신을 NVLink 구간에 가둠)
* PP/DP 그룹 (노드 외부): "너희는 멀리 떨어져 있고 길이 좁으니(EFA/InfiniBand), 계산이 다 끝난 결과값만 가끔 주고받아!" (인터노드 통신 최소화)

#### 2. 메가트론이 "자동화"해주는 핵심 기능 ####
개발자가 "Llama 3 모델을 TP=8, PP=2로 돌려줘"라고 실행 인자만 주면, 메가트론 내부 코드가 다음을 자동으로 처리.
* 커스텀 레이어 구현: 일반적인 PyTorch 레이어를 TP용 병렬 레이어(Column/Row Parallel Linear)로 알아서 교체하여 실행.
* 커뮤니케이션 그룹 설정: 어떤 GPU끼리 NVLink로 대화할지, 어떤 GPU끼리 네트워크망(EFA)을 쓸지 통신 그룹(Process Group)을 자동으로 분리.
* 마이크로 배치(Micro-batch) 스케줄링: 파이프라인 병렬화(PP) 시 GPU가 노는 시간(Bubble)을 줄이기 위해 데이터를 잘게 쪼개어 밀어넣는 복잡한 스케줄링을 알아서 수행.

#### 3. 왜 DeepSpeed보다 메가트론이 이 분야의 강자인가? ####
* DeepSpeed ZeRO: 모델이 어떻게 생겼든 상관없어, 가중치만 잘게 쪼개서 분산 처리.
* Megatron-LM: 모델 구조와 하드웨어 토폴로지를 보고, 어디를 분산할지를 결정 (설계 단계부터 성능 최적화)

#### 그러나 사용자는 TP를 4로 할지 8로 할지, PP를 몇 단계로 나눌지는 직접 실험하며 최적의 'Sweet Spot'을 찾아야 한다. ####


## 레퍼런스 ##

* [Amazon EC2에서 ML 워크로드를 위한 EFA 및 NCCL 시작하기](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/efa-start-nccl.html)
