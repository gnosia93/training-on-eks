## 토폴로지별 훈련성능 ##

### 모델 정보 ###
* Hugging Face Lllama-3-8b 
* DeepSpeed Stage 3
* No Checkpointing
* No Other Specific Tunning for Performance 
 
### 인스턴스 정보 ###
```
INSTANCE_TYPE=g6e.12xlarge              # 훈련 인스턴스 타입   
GPU=NVIDIA L40S Tensor Core GPUs 48 GB
NODE_NUM=4                              # 4대 
GPU_PER_NODE=4                          # g6e.12xlarge 타입은 GPU 가 4장이다.
EFA_PER_NODE=1                          # 100Gbp 사용

NO Placement Group
REGION Level Instance
```

### k8s 설정 ###
```
image: public.ecr.aws/deep-learning-containers/pytorch-training:2.8.0-gpu-py312-cu129-ubuntu22.04-ec2-v1.0
dshm : "64Gi"                                   # shared memory 공간을 기본값(64Mi) 에서 64Gi 로 설정 
torchrun --nproc_per_node=gpu                   # 4 pods * 4 gpu (파드당 4개의 파이썬 프로세스)
nvidia.com/gpu: 4
vpc.amazonaws.com/efa: 1                        # eni 테스트시 주석처리               
```

### deepspeed 설정 ###
```
{
    "fp16": { "enabled": false },
    "bf16": { "enabled": true },
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": { "device": "cpu", "pin_memory": true },
        "offload_param": { "device": "cpu", "pin_memory": true },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": false
    },
    "gradient_clipping": "auto",
    "steps_per_print": 10,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "gradient_accumulation_steps": "auto",
    "wall_clock_breakdown": false
}
```
### trainer 설정 ###
```
per_device_train_batch_size=4,
gradient_accumulation_steps=4,                 # 실제 배치 사이즈 = 4 * 4 * GPU 개수
learning_rate=2e-5,
max_steps=50,                                  # 딱 50번의 스텝만 하고 종료 / 이경우 에포크는 무시됨   
num_train_epochs=1,
bf16=True,                                     # A100/H100/B200 GPU 권장
logging_steps=5,
deepspeed="llama-3-8b-stage3.json", 
save_strategy="epoch",
save_total_limit=2,     
gradient_checkpointing=False,                  # 메모리 절약을 위한 재계산
```

### 토폴로지 ###
* 노드 내(Intra Node) : PCIe / Shared Memory 통신
* 노드 간(Inter Node) : ENI or EFA
```
***** Running training *****
  Num examples = 36,718
  Num Epochs = 1
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 256
  Gradient Accumulation steps = 4
  Total optimization steps = 50

llama-3-8b-node-0-0:194:1158 [2] NCCL INFO Channel 00 : 2[2] -> 3[3] via SHM/direct/direct
llama-3-8b-node-0-0:194:1158 [2] NCCL INFO Channel 01 : 2[2] -> 3[3] via SHM/direct/direct

llama-3-8b-node-0-0:195:1161 [3] NCCL INFO [Proxy Progress] Device 3 CPU core 20
llama-3-8b-node-0-0:195:1159 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[0] [send] via NET/Socket/0
llama-3-8b-node-0-0:195:1159 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[0] [send] via NET/Socket/0

llama-3-8b-node-0-0:195:1120 [3] NCCL INFO Channel 00/0 : 3[3] -> 4[0] [send] via NET/Libfabric/0
llama-3-8b-node-0-0:195:1120 [3] NCCL INFO Channel 01/0 : 3[3] -> 4[0] [send] via NET/Libfabric/0       
``` 

### 측정 결과 ###

* EFA - 2441.04s (약 41분)
* ENI - 5408.85s (약 90분)
