## 토폴로지별 훈련성능 ##

* Lllama-3-8B 
* DeepSpeed Stage 3                         

### deepspeed 설정 ###
```
{
    "fp16": { "enabled": false },
    "bf16": { "enabled": true },
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": { "device": "cpu", "pin_memory": true },
        "offload_param": { "device": "cpu", "pin_memory": true },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": false
    },
    "gradient_clipping": "auto",
    "steps_per_print": 10,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "gradient_accumulation_steps": "auto",
    "wall_clock_breakdown": false
}
```
### trainer 설정 ###
```
per_device_train_batch_size=4,
gradient_accumulation_steps=4,                 # 실제 배치 사이즈 = 4 * 4 * GPU 개수
learning_rate=2e-5,
max_steps=50,                                  # 딱 50번의 스텝만 하고 종료 / 이경우 에포크는 무시됨   
num_train_epochs=1,
bf16=True,                                     # A100/H100/B200 GPU 권장
logging_steps=5,
deepspeed="llama-3-8b-stage3.json", 
save_strategy="epoch",
save_total_limit=2,     
gradient_checkpointing=False,                  # 메모리 절약을 위한 재계산
```
```
***** Running training *****
  Num examples = 36,718
  Num Epochs = 1
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 4
  Total optimization steps = 50
  Number of trainable parameters = 8,030,261,248
```

### Pod Spec ###
```
apiVersion: v1
kind: Pod
metadata:
  annotations:
    jobset.sigs.k8s.io/jobset-name: llama-3-8b
  labels:
    job-name: llama-3-8b-node-0
    jobset.sigs.k8s.io/replicatedjob-name: node
  name: llama-3-8b-node-0-0-z59th
  namespace: default
spec:
  containers:
    env:
    - name: NCCL_P2P_LEVEL
      value: NVL
    - name: NCCL_P2P_DIRECT_READ
      value: "0"
    - name: NCCL_P2P_DIRECT_WRITE
      value: "1"
    - name: NCCL_CUMEM_HOST_ENABLE
      value: "0"
    - name: PET_NNODES
      value: "1"
    - name: PET_NPROC_PER_NODE
      value: "4"
    - name: PET_NODE_RANK
      valueFrom:
        fieldRef:
          apiVersion: v1
          fieldPath: metadata.annotations['batch.kubernetes.io/job-completion-index']
    - name: PET_MASTER_ADDR
      value: llama-3-8b-node-0-0.llama-3-8b
    - name: PET_MASTER_PORT
      value: "29500"
    image: public.ecr.aws/deep-learning-containers/pytorch-training:2.8.0-gpu-py312-cu129-ubuntu22.04-ec2-v1.0
    resources:
      limits:
        nvidia.com/gpu: "4"
        vpc.amazonaws.com/efa: "4"
      requests:
        nvidia.com/gpu: "4"
        vpc.amazonaws.com/efa: "4"
    securityContext:
      capabilities:
        add:
        - IPC_LOCK
    volumeMounts:
    - mountPath: /dev/shm
      name: dshm
  hostIPC: true
  hostname: llama-3-8b-node-0-0
  nodeName: ip-10-0-5-88.ap-northeast-2.compute.internal
  nodeSelector:
    node.kubernetes.io/instance-type: p4d.24xlarge
  shareProcessNamespace: true
  tolerations:
  - effect: NoSchedule
    key: nvidia.com/gpu
    operator: Exists
  - effect: NoSchedule
    key: vpc.amazonaws.com/efa
    operator: Exists
  volumes:
  - emptyDir:
      medium: Memory
      sizeLimit: 64Gi
    name: dshm
```













## 훈련성능 측정결과 ##

### p4d.24xlarge ###

* Nitro / 1 Pod / 4 GPU / 4 EFA - 0:30:00 ( via P2P/CUMEM/read )
* Nitro / 1 Pod / 4 GPU / 4 EFA - 0:16:41 ( via P2P/CUMEM/read && NCCL_CUMEM_HOST_ENABLE=0 )
* Nitro / 4 Pod / 1 GPU / 1 EFA - 0:28:15 ( via NET/Libfabric/0/GDRDMA )        
* Nitro / 4 Pod / 1 GPU / 1 ENI - ???     ( via  )        
* Metal ?  



#### 전송 메커니즘의 근본적 차이 ####

* Write 모드 (Push): 송신 GPU가 데이터를 상대방 메모리에 직접 밀어 넣습니다. NVLink의 대역폭을 거의 100% 활용하며, 이론상 수백 GB/s의 속도를 냅니다. 즉시 전송이 시작되므로 지연 시간이 극도로 낮습니다
* Read 모드 (Pull): 수신 GPU가 상대방 메모리를 읽어옵니다. 가상화 계층의 간섭과 데이터 확인 절차(Handshake) 때문에 대역폭이 30%~50% 이상 급감할 수 있습니다. "데이터가 준비되었는가?" 확인 -> "읽기 요청" -> "데이터 수신"이라는 복잡한 단계를 거치므로 지연 시간이 훨씬 깁니다. 8B 모델처럼 수많은 파라미터를 빈번하게 교환해야 하는 환경에서는 이 미세한 지연이 누적되어 큰 차이를 만듭니다.

#### 니트로 EC2의 태생적 한계 ####
* 하이퍼바이저의 간섭: Nitro 하이퍼바이저가 보안과 격리를 위해 GPU 간의 직접 쓰기(Direct Write) 권한을 제한합니다.
* 결과: 하드웨어는 NVLink라는 초고속 도로를 갖고 있지만, 규제(가상화 설정) 때문에 속도 제한이 걸린 read 모드로만 달려야 하는 상황입니다. NVIDIA NCCL 기술 문서에서도 가상화에 따른 이러한 성능 저하를 명시하고 있습니다.


#### 제어 신호와 데이터 경로의 분리 ####
NVLink는 데이터 전송(Data Path)만을 담당하는 초고속 통로입니다. 하지만 GPU가 서로를 인식하고, 통신 세션을 열고, 메모리 주소를 매핑하는 제어 신호(Control Path)는 여전히 PCIe 버스를 통해 이루어집니다.
* PCIe 단계에서 ACS가 활성화되어 장치 간 격리가 엄격하면, GPU들이 서로의 메모리 주소 공간을 직접 매핑(Peer-to-Peer Mapping)하는 것을 차단할 수 있습니다.
* 이 매핑이 실패하면 NCCL은 NVLink가 물리적으로 연결되어 있어도 이를 사용하지 못하고 TCP나 일반 PCIe 전송으로 우회(Fallback)하게 됩니다.



#### 팁 ?? ####
* 순수 성능이 제일 중요하다면 → 베어메탈(p4d.metal)로 가서 write 모드를 씁니다.
* 가성비와 운영이 중요하다면 → 니트로 EC2에서 파드를 쪼개 GDRDMA(Write 방식)를 활용해 성능 손실을 메웁니다

## 레퍼런스 ##

* [Amazon EC2에서 ML 워크로드를 위한 EFA 및 NCCL 시작하기](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/efa-start-nccl.html)
