## 토폴로지별 훈련성능 ##

* Llama-3-8B 
* DeepSpeed Stage 3                         

### deepspeed 설정 ###
```
{
    "fp16": { "enabled": false },
    "bf16": { "enabled": true },
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": { "device": "cpu", "pin_memory": true },
        "offload_param": { "device": "cpu", "pin_memory": true },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e8,
        "reduce_bucket_size": 5e7,
        "stage3_prefetch_bucket_size": 5e7,
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e8,
        "stage3_max_reuse_distance": 1e8,
        "stage3_gather_16bit_weights_on_model_save": true
    },
    "gradient_clipping": "auto",
    "steps_per_print": 10,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "gradient_accumulation_steps": "auto",
    "wall_clock_breakdown": false
}
```
### trainer 설정 ###
```
per_device_train_batch_size=1,                 
gradient_accumulation_steps=4,                 
learning_rate=2e-5,
max_steps=50,                                     
num_train_epochs=1,
bf16=True,                                     
logging_steps=5,
deepspeed="llama-3-8b-stage3.json", 
save_strategy="no",                            
gradient_checkpointing=True,                   
```
```
***** Running training *****
  Num examples = 36,718
  Num Epochs = 1
  Instantaneous batch size per device = 1
  Total train batch size (w. parallel, distributed & accumulation) = 16
  Gradient Accumulation steps = 4
  Total optimization steps = 50
  Number of trainable parameters = 8,030,261,248
```

### Pod Spec ###
```
spec:
  affinity:
    podAffinity:
      # EFA 는 동일 AZ 안에서만 동작하므로, 하나의 AZ 만 사용하도록 강제한다. 
      requiredDuringSchedulingIgnoredDuringExecution:
        - labelSelector:
            matchLabels:
              jobset.sigs.k8s.io/jobset-name: "llama-3-8b"
        topologyKey: "topology.kubernetes.io/zone"
  containers:
    env:
    - name: NCCL_P2P_LEVEL
      value: NVL
    image: public.ecr.aws/deep-learning-containers/pytorch-training:2.8.0-gpu-py312-cu129-ubuntu22.04-ec2-v1.0
    resources:
      limits:
        nvidia.com/gpu: "4"
        vpc.amazonaws.com/efa: "4"
    securityContext:
      capabilities:
        add:
        - IPC_LOCK
      privileged: true
    volumeMounts:
    - mountPath: /dev/shm
      name: dshm
  hostIPC: true
  nodeSelector:
    node.kubernetes.io/instance-type: p4d.24xlarge
  shareProcessNamespace: true
  tolerations:
  - effect: NoSchedule
    key: nvidia.com/gpu
    operator: Exists
  - effect: NoSchedule
    key: vpc.amazonaws.com/efa
    operator: Exists
  volumes:
  - emptyDir:
      medium: Memory
      sizeLimit: 64Gi
    name: dshm
```

## 훈련성능 측정결과 ##

### 멀티 GPU ###

#### g6e.48xlarge 1대 (LS40S) ####
* 1 Pod [4 GPU / 4 EFA] - 0:35:48 ( via SHM/direct/direct )
* 4 Pod [1 GPU / 1 EFA] -  ( )        
* 4 Pod [1 GPU / 1 ENI] - ( )       

#### p4d.24xlarge 1대 (A100) ####
* 1 Pod [4 GPU / 4 EFA] - 0:16:21 ( via P2P/CUMEM/read, P2P Type isAllDirectP2p 1 directMode 0)
* 4 Pod [1 GPU / 1 EFA] - 0:39:32 ( via NET/Libfabric/0/GDRDMA )        
* 4 Pod [1 GPU / 1 ENI] - 1:20:29 ( via NET/Socket/0 )        

#### p5.48xlarge 1대 (H100) ####
A100 과 H100의 성능을 단순 비교하면 안된다. H100의 첫 실행시 쿠다 커널을 custom 하게 빌드하는 프로세스가 훨씬더 정교하고 오랫동안 동작하기 때문에 Warming UP 시간이 A100 보다 훨씬 오래 걸린다. 하드웨어 자체도 H100 훨씬더 복잡하고 다양한 기능을 제공하기 때문에 이에 최적화된 커널을 굽기 위해서 시간이 더 걸리다고 생각하면 쉽다.
이 방식은 파이토치 2.X 에 탑재된 torch.compile() 기능으로 내부적으로 OpenAI 가 개발한 트리톤 엔진을 쓰고 있으며, H100이라는 최신 하드웨어의 성능을 극한으로 뽑아내기 위해, 실행 직후에 트리톤이 출동해서 H100 전용 커널을 새로 굽느라 시간이 오래 걸리는 것이다. 실제로 훨씬 더 많은 python 프로세스들이 동작하여 해당 하드웨에 최적화된 커널을 구워낸다. 여기서 주의할 점은 이렇게 실행시 구워낸 커널은 ~/.cache/torch/inductor 디렉토리에 .py (커널 래퍼), .so (컴파일된 바이너리), .cubin (GPU 기계어) 형태의 파일들로 저장되는데, 컨테이너 환경에서는 반드시 호스트의 볼륨에 저장해서 재 실행시 다시 컴파일 하지 않도록 해야 한다.  

* 1 Pod [4 GPU / 4 EFA] - 0:20:06 ( via P2P/CUMEM, P2P Type isAllDirectP2p 1 directMode 0)
* 4 Pod [1 GPU / 1 EFA] - 1:06:09 ( via NET/Libfabric/0/GDRDMA )        
* 4 Pod [1 GPU / 1 ENI] - ?? ( via NET/Socket/0 )  


#### p5e.48xlarge (H200) ####

## A100 vs H100 NCCL 전송 방식의 차이 ##
NCCL 전송 방식이 다르게 나타나는 것은 두 아키텍처의 메모리 컨트롤러 설계와 NVLink 세대의 차이 때문이다.

#### 1. A100: P2P/CUMEM/READ (주로 Read/Pull 방식) ####
A100(Ampere) 아키텍처에서는 NCCL이 데이터를 받는 쪽에서 읽어오는(Read) 방식이 더 효율적이라고 판단하는 경우가 많다.
* 이유: A100 시스템(예: p4d.24xlarge)의 PCIe 스위치와 3세대 NVLink 구조에서는 데이터를 밀어넣는(Write) 것보다, 수신자가 송신자의 메모리 주소에서 데이터를 당겨올 때 지연 시간(Latency)이 더 안정적인 특성을 보인다.
* 특징: 하드웨어 토폴로지상 읽기 요청이 여러 GPU 간의 병목(Congestion)을 피하기에 더 적합한 구조였기 때문에 NCCL이 자동으로 READ를 선택한 것이다.

#### 2. H100: P2P/CUMEM (주로 Write/Push 방식) ####
반면 H100(Hopper) 아키텍처에서는 READ 접미사가 없는 기본 쓰기(Write/Push) 방식으로 동작한다.
* 이유: H100은 4세대 NVLink와 NVLink Switch System을 사용하는데, 이 구조는 Remote Direct Memory Access(RDMA) Write 성능이 극대화되어 설계되었다.
* 핵심 기술: H100에는 Tensor Memory Accelerator(TMA)라는 장치가 추가되어, 데이터를 다른 GPU의 메모리로 즉시 밀어넣는(Push) 속도가 훨씬 빠르다. 따라서 NCCL 엔진이 초기 테스트(BusBW 측정) 시 Write 방식이 압도적으로 유리하다고 판단하여 P2P/CUMEM으로 동작하는 것이다.

#### 3. 성능상 어떤 게 더 좋은가요? ####
결론부터 말씀드리면, H100의 P2P/CUMEM 방식이 훨씬 강력하다.
* 대역폭 차이: A100의 READ 방식은 최대 600GB/s(양방향 합계) 수준이지만, H100의 Write 방식은 900GB/s까지 지원한다.
* 효율성: H100의 방식은 하드웨어가 직접 데이터를 밀어주기 때문에 수신 쪽 GPU의 연산 부하를 줄여주는 효과가 있다.

#### 4. 요약 ###
* A100: 구조적 특성상 "당겨오기(Read)"가 효율적이라 P2P/CUMEM/READ 선택.
* H100: 신기술(TMA, 4세대 NVLink) 덕분에 "밀어넣기(Write)"가 훨씬 빠르므로 P2P/CUMEM 선택.
이러한 차이는 하드웨어가 세대교체 되면서 최적의 통신 프로토콜이 바뀐 결과로, H100이 READ를 안 쓴다고 해서 기능이 빠진 것이 아니라, 더 빠른 길을 찾았기 때문이다. NVIDIA Hopper 아키텍처 화이트페이퍼에서 관련 상세 기술을 확인할 수 있다.  

## A100 vs H100 성능 ##
#### 1. JIT(Just-In-Time) 컴파일과 커널 최적화 ####
H100은 Transformer Engine과 새로운 최적화 기술들을 탑재하고 있습니다. 이를 제대로 활용하기 위해 프레임워크(PyTorch, 텐서플로우 등)가 실행 시점에 하드웨어에 최적화된 쿠다 커널을 빌드하거나 선택하는 과정이 A100보다 복잡합니다. 이 과정에서 발생하는 오버헤드가 초기 실행 속도(Warming Up)를 늦추는 주된 원인이 됩니다.

#### 2. FP8 데이터 타입과 런타임 튜닝 ####
H100의 핵심 성능 향상은 FP8(8비트 부동소수점) 연산에서 나옵니다.
* A100: 주로 FP16/TF32를 사용하며 최적화 경로가 이미 매우 성숙해 있습니다.
* H100: FP8을 사용할 때 정밀도를 유지하면서 속도를 높이기 위해 실시간으로 스케일링 인자를 계산하고 커널을 조정하는 정교한 튜닝이 필요합니다. 이 설정이 제대로 되지 않으면 하드웨어 잠재력을 다 쓰지 못합니다.

#### 3. 컴파일러 및 라이브러리 의존성 ####
H100의 성능을 극대화하려면 CUDA 12.x 버전 이상과 최신 cuDNN/NCCL 라이브러리가 필수적입니다. 단순히 기존 A100 환경의 코드를 그대로 가져와 실행하면, H100의 새로운 기능(Thread Block Clusters 등)을 활용하지 못하고 오히려 구형 커널과의 호환성 작업 때문에 성능이 저하될 수 있습니다.

말씀하신 대로 H100은 '길들이기(Tuning)'가 필요한 GPU입니다.
* Warming Up: 초기 커널 빌드 및 캐싱 시간을 충분히 고려해야 정확한 벤치마크가 가능합니다.
* Optimization: 사용자의 모델 아키텍처에 맞춰 하이퍼파라미터와 데이터 타입을 조정하는 프로세스가 수반되어야 A100 대비 압도적인 성능 차이를 체감할 수 있습니다.
따라서 단순히 "H100이 A100보다 몇 배 빠르다"는 식의 비교보다는, 특정 워크로드에서 최적화가 완료된 후의 안정 상태(Steady State) 성능을 비교하는 것이 훨씬

## nvlink configuration ## 

#### A100 레인수 ####
```
root@llama-3-8b-node-0-0:/# nvidia-smi nvlink -s
GPU 0: NVIDIA A100-SXM4-40GB (UUID: GPU-ceebf8a0-4c3c-d938-6860-7f6afaa20c9c)
         Link 0: 25 GB/s
         Link 1: 25 GB/s
         Link 2: 25 GB/s
         Link 3: 25 GB/s
         Link 4: 25 GB/s
         Link 5: 25 GB/s
         Link 6: 25 GB/s
         Link 7: 25 GB/s
         Link 8: 25 GB/s
         Link 9: 25 GB/s
         Link 10: 25 GB/s
         Link 11: 25 GB/s
GPU 1: NVIDIA A100-SXM4-40GB (UUID: GPU-97e5b638-31e5-82b3-8436-907e63ecca20)
         Link 0: 25 GB/s
         Link 1: 25 GB/s
 
```

#### H100 레인수 ####
```
root@llama-3-8b-node-0-0:/# nvidia-smi nvlink -s
GPU 0: NVIDIA H100 80GB HBM3 (UUID: GPU-d3faac9e-38e5-133f-c270-655c5d464512)
         Link 0: 26.562 GB/s
         Link 1: 26.562 GB/s
         Link 2: 26.562 GB/s
         Link 3: 26.562 GB/s
         Link 4: 26.562 GB/s
         Link 5: 26.562 GB/s
         Link 6: 26.562 GB/s
         Link 7: 26.562 GB/s
         Link 8: 26.562 GB/s
         Link 9: 26.562 GB/s
         Link 10: 26.562 GB/s
         Link 11: 26.562 GB/s
         Link 12: 26.562 GB/s
         Link 13: 26.562 GB/s
         Link 14: 26.562 GB/s
         Link 15: 26.562 GB/s
         Link 16: 26.562 GB/s
         Link 17: 26.562 GB/s
GPU 1: NVIDIA H100 80GB HBM3 (UUID: GPU-8fc9eb48-a6ef-8a85-d7d8-a2e01532aff7)
         Link 0: 26.562 GB/s
         Link 1: 26.562 GB/s
```

## topo.xml ##
```
# ls /opt/amazon/ofi-nccl/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
/opt/amazon/ofi-nccl/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
root@llama-3-8b-node-0-0:/# cat /opt/amazon/ofi-nccl/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
<!--
Copyright (c) 2019-2020, Amazon.com, Inc. or its affiliates. All rights reserved.
See LICENSE.txt for license information

Static pre-configured topology for `p4d.24xlarge` platform type.
This has 2 groups of PCIe hierarchy under each socket. Each group has
2 GPUs and 1 NIC behind a PCIe switch.

This topology is ingested into NCCL using `NCCL_TOPO_FILE` environment
variable when the underlying system is detected as `p4d.24xarge`.

Note: The PCI IDs of GPUs and NICs needs to match the device IDs on the
virtual machine.
-->
<system version="1">
  <cpu numaid="0" affinity="000000ff,ffff0000,00ffffff" arch="x86_64" vendor="GenuineIntel" familyid="6" modelid="85">
    <pci busid="ffff:ff:01.0" class="0x060400" vendor="0xffff" device="0xffff" subsystem_vendor="0xffff" subsystem_device="0xffff" link_speed="8 GT/s" link_width="16">  <!-- Switch 0 begins -->
        <pci busid="0000:10:1c.0" class="0x030200" vendor="0x10de" device="0x20b0" subsystem_vendor="0x10de" subsystem_device="0x134f" link_speed="8 GT/s" link_width="16"/> <!-- GPU 0 -->
        <pci busid="0000:10:1d.0" class="0x030200" vendor="0x10de" device="0x20b0" subsystem_vendor="0x10de" subsystem_device="0x134f" link_speed="8 GT/s" link_width="16"/> <!-- GPU 1 -->
        <pci busid="0000:10:1b.0" class="0x020000" vendor="0x1d0f" device="0xefa0" subsystem_vendor="0x1d0f" subsystem_device="0xefa0" link_speed="8 GT/s" link_width="16"/> <!-- NIC 0 -->
    </pci> <!-- Switch 0 ends -->
    <pci busid="ffff:ff:02.0" class="0x060400" vendor="0xffff" device="0xffff" subsystem_vendor="0xffff" subsystem_device="0xffff" link_speed="8 GT/s" link_width="16">  <!-- Switch 1 begins -->
        <pci busid="0000:20:1c.0" class="0x030200" vendor="0x10de" device="0x20b0" subsystem_vendor="0x10de" subsystem_device="0x134f" link_speed="8 GT/s" link_width="16"/> <!-- GPU 2 -->
        <pci busid="0000:20:1d.0" class="0x030200" vendor="0x10de" device="0x20b0" subsystem_vendor="0x10de" subsystem_device="0x134f" link_speed="8 GT/s" link_width="16"/> <!-- GPU 3 -->
        <pci busid="0000:20:1b.0" class="0x020000" vendor="0x1d0f" device="0xefa0" subsystem_vendor="0x1d0f" subsystem_device="0xefa0" link_speed="8 GT/s" link_width="16"/> <!-- NIC 1 -->
    </pci> <!-- Switch 1 ends -->
  </cpu>
  <cpu numaid="1" affinity="ffffff00,0000ffff,ff000000" arch="x86_64" vendor="GenuineIntel" familyid="6" modelid="85">
    <pci busid="ffff:ff:03.0" class="0x060400" vendor="0xffff" device="0xffff" subsystem_vendor="0xffff" subsystem_device="0xffff" link_speed="8 GT/s" link_width="16">  <!-- Switch 2 begins -->
        <pci busid="0000:90:1c.0" class="0x030200" vendor="0x10de" device="0x20b0" subsystem_vendor="0x10de" subsystem_device="0x134f" link_speed="8 GT/s" link_width="16"/> <!-- GPU 4 -->
        <pci busid="0000:90:1d.0" class="0x030200" vendor="0x10de" device="0x20b0" subsystem_vendor="0x10de" subsystem_device="0x134f" link_speed="8 GT/s" link_width="16"/> <!-- GPU 5 -->
        <pci busid="0000:90:1b.0" class="0x020000" vendor="0x1d0f" device="0xefa0" subsystem_vendor="0x1d0f" subsystem_device="0xefa0" link_speed="8 GT/s" link_width="16"/> <!-- NIC 2 -->
    </pci> <!-- Switch 2 ends -->
    <pci busid="ffff:ff:04.0" class="0x060400" vendor="0xffff" device="0xffff" subsystem_vendor="0xffff" subsystem_device="0xffff" link_speed="8 GT/s" link_width="16">  <!-- Switch 3 begins -->
        <pci busid="0000:a0:1c.0" class="0x030200" vendor="0x10de" device="0x20b0" subsystem_vendor="0x10de" subsystem_device="0x134f" link_speed="8 GT/s" link_width="16"/> <!-- GPU 6 -->
        <pci busid="0000:a0:1d.0" class="0x030200" vendor="0x10de" device="0x20b0" subsystem_vendor="0x10de" subsystem_device="0x134f" link_speed="8 GT/s" link_width="16"/> <!-- GPU 7 -->
        <pci busid="0000:a0:1b.0" class="0x020000" vendor="0x1d0f" device="0xefa0" subsystem_vendor="0x1d0f" subsystem_device="0xefa0" link_speed="8 GT/s" link_width="16"/> <!-- NIC 3 -->
    </pci> <!-- Switch 3 ends -->
  </cpu>
</system>
```

## 멀티 노드 동기화 ##
8개의 GPU가 NVLink로 묶여 있어도, 노드 밖의 파라미터를 기다려야 하는 순간 NVLink의 초고속 성능은 "무용지물"이 된다.

#### 1. 동기화의 함정: "가장 느린 놈이 전체 속도를 정한다" ####
DeepSpeed ZeRO-3 같은 데이터 병렬 처리에서 16개 GPU(2개 노드)가 학습할 때, 한 걸음(Step)을 마치려면 16개 GPU의 파라미터가 모두 일치해야 한다.
* 노드 내부(1~8번 GPU): NVLink로 눈 깜짝할 새 대화를 끝낸다.
* 노드 외부(9~16번 GPU): EFA라는 상대적으로 좁은 통로를 통해 데이터를 주고 받는다.
* 결과: 1 ~ 8번 GPU는 자기들끼리 대화를 마쳤어도, 9 ~ 16번 GPU의 데이터가 EFA를 타고 올 때까지 아무것도 못 하고 멍하니 기다려야 함. (Barrier Wait)

#### 2. NVLink가 억울한 이유 ####
NVLink가 아무리 600GB/s로 데이터를 쏠 수 있어도, 데이터를 받는 상대방 노드로 가는 길이 50GB/s(EFA)뿐이라면, NVLink는 자신의 성능의 10%도 쓰지 못하고 EFA 속도에 맞춰서 데이터를 조금씩 흘려보낼 수밖에 없다.

#### 3. 왜 멀티 노드에서 효율이 급락하는가? ####
단일 노드에서는 모든 통신이 NVLink 전용 도로에서 일어나지만, 멀티 노드가 되는 순간 모든 통신 프로세스가 'EFA라는 톨게이트'를 통과해야 하는 구조로 바뀜.
* 노드 1대일 때 속도: NVLink 속도 (매우 빠름)
* 노드 2대 이상일 때 속도: EFA 속도 + 네트워크 지연 시간(Latency) + 동기화 오버헤드 (갑자기 느려짐)

#### 4. 그래서 전문가들이 내리는 결론 ####
"노드를 늘리는 순간 NVLink는 무용지물"이 되는 현상을 막기 위해, 고도로 설계된 학습 환경(예: NVIDIA SuperPOD)에서는 인피니밴드(InfiniBand)를 GPU 개수만큼 꽂아서 인터노드 대역폭을 NVLink 급으로 올리려고 수십억 원을 투자함.
하지만 일반적인 EFA 환경에서는 대역폭 차이가 극명하므로, 앞서 논의한 것처럼 "노드 간(EFA)에는 통신을 거의 안 하게 만들고, 노드 내(NVLink)에서만 통신이 폭발하게 만드는" TP/PP 전략이 무조건 강제되는 것.

## NVIDIA의 메가트론(Megatron-LM) ## 
NVIDIA의 메가트론(Megatron-LM)은 바로 그 '복잡하고 고통스러운 병렬화 설계'를 구조적으로 해결하기 위해 탄생한 프레임워크이다.
단순히 자동화라고 하기엔 개발자가 초기에 지정해줘야 할 파라미터가 있지만, "토폴로지에 최적화된 병렬 연산 그래프를 생성하고 통신을 스케줄링"한다는 점에서 좋은 솔루션이다.

#### 1. 토폴로지 맞춤형 설계 (Intra-node vs Inter-node) ####
* 메가트론은 실행 시 사용자가 입력한 TP(Tensor Parallel)와 PP(Pipeline Parallel) 크기를 바탕으로 GPU들을 그룹핑
* TP 그룹 (노드 내부): "너희 8개는 NVLink로 묶여 있으니, 모델의 레이어 하나를 조각조각 나눠서 같이 계산해!" (가장 빈번한 통신을 NVLink 구간에 가둠)
* PP/DP 그룹 (노드 외부): "너희는 멀리 떨어져 있고 길이 좁으니(EFA/InfiniBand), 계산이 다 끝난 결과값만 가끔 주고받아!" (인터노드 통신 최소화)

#### 2. 메가트론이 "자동화"해주는 핵심 기능 ####
개발자가 "Llama 3 모델을 TP=8, PP=2로 돌려줘"라고 실행 인자만 주면, 메가트론 내부 코드가 다음을 자동으로 처리.
* 커스텀 레이어 구현: 일반적인 PyTorch 레이어를 TP용 병렬 레이어(Column/Row Parallel Linear)로 알아서 교체하여 실행.
* 커뮤니케이션 그룹 설정: 어떤 GPU끼리 NVLink로 대화할지, 어떤 GPU끼리 네트워크망(EFA)을 쓸지 통신 그룹(Process Group)을 자동으로 분리.
* 마이크로 배치(Micro-batch) 스케줄링: 파이프라인 병렬화(PP) 시 GPU가 노는 시간(Bubble)을 줄이기 위해 데이터를 잘게 쪼개어 밀어넣는 복잡한 스케줄링을 알아서 수행.

#### 3. 왜 DeepSpeed보다 메가트론이 이 분야의 강자인가? ####
* DeepSpeed ZeRO: 모델이 어떻게 생겼든 상관없어, 가중치만 잘게 쪼개서 분산 처리.
* Megatron-LM: 모델 구조와 하드웨어 토폴로지를 보고, 어디를 분산할지를 결정 (설계 단계부터 성능 최적화)

#### 그러나 사용자는 TP를 4로 할지 8로 할지, PP를 몇 단계로 나눌지는 직접 실험하며 최적의 'Sweet Spot'을 찾아야 한다. ####


## 레퍼런스 ##
* https://arxiv.org/pdf/2507.04786
* [Amazon EC2에서 ML 워크로드를 위한 EFA 및 NCCL 시작하기](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/efa-start-nccl.html)
