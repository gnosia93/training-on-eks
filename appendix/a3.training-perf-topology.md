## 토폴로지별 훈련성능 ##

### 모델 정보 ###
* Hugging Face Lllama-3-8b 
* DeepSpeed Stage 3
 
### deepspeed 설정 ###
```
{
    "fp16": { "enabled": false },
    "bf16": { "enabled": true },
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": { "device": "cpu", "pin_memory": true },
        "offload_param": { "device": "cpu", "pin_memory": true },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": false
    },
    "gradient_clipping": "auto",
    "steps_per_print": 10,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "gradient_accumulation_steps": "auto",
    "wall_clock_breakdown": false
}
```
### trainer 설정 ###
```
per_device_train_batch_size=4,
gradient_accumulation_steps=4,                 # 실제 배치 사이즈 = 4 * 4 * GPU 개수
learning_rate=2e-5,
max_steps=50,                                  # 딱 50번의 스텝만 하고 종료 / 이경우 에포크는 무시됨   
num_train_epochs=1,
bf16=True,                                     # A100/H100/B200 GPU 권장
logging_steps=5,
deepspeed="llama-3-8b-stage3.json", 
save_strategy="epoch",
save_total_limit=2,     
gradient_checkpointing=False,                  # 메모리 절약을 위한 재계산
```

### k8s 설정 ###
```
image: public.ecr.aws/deep-learning-containers/pytorch-training:2.8.0-gpu-py312-cu129-ubuntu22.04-ec2-v1.0
dshm : "64Gi"                                   # shared memory 공간을 기본값(64Mi) 에서 64Gi 로 설정 
```

## 훈련성능 측정결과 ##











* EFA - 2441.04s (약 41분)
* ENI - 5408.85s (약 90분)
