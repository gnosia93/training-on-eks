## 토폴로지별 훈련성능 ##

* Llama-3-8B 
* DeepSpeed Stage 3                         

### deepspeed 설정 ###
```
{
    "fp16": { "enabled": false },
    "bf16": { "enabled": true },
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": { "device": "cpu", "pin_memory": true },
        "offload_param": { "device": "cpu", "pin_memory": true },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": false
    },
    "gradient_clipping": "auto",
    "steps_per_print": 10,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "gradient_accumulation_steps": "auto",
    "wall_clock_breakdown": false
}
```
### trainer 설정 ###
```
per_device_train_batch_size=4,
gradient_accumulation_steps=4,                 # 실제 배치 사이즈 = 4 * 4 * GPU 개수
learning_rate=2e-5,
max_steps=50,                                  # 딱 50번의 스텝만 하고 종료 / 이경우 에포크는 무시됨   
num_train_epochs=1,
bf16=True,                                     # A100/H100/B200 GPU 권장
logging_steps=5,
deepspeed="llama-3-8b-stage3.json", 
save_strategy="epoch",
save_total_limit=2,     
gradient_checkpointing=False,                  # 메모리 절약을 위한 재계산
```
```
***** Running training *****
  Num examples = 36,718
  Num Epochs = 1
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 4
  Total optimization steps = 50
  Number of trainable parameters = 8,030,261,248
```

### Pod Spec ###
```
  annotations:
    jobset.sigs.k8s.io/jobset-name: llama-3-8b
  labels:
    job-name: llama-3-8b-node-0
  name: llama-3-8b-node-0-0-z59th
  namespace: default
spec:
  containers:
    env:
    - name: NCCL_P2P_LEVEL
      value: NVL
    image: public.ecr.aws/deep-learning-containers/pytorch-training:2.8.0-gpu-py312-cu129-ubuntu22.04-ec2-v1.0
    resources:
      limits:
        nvidia.com/gpu: "4"
        vpc.amazonaws.com/efa: "4"
    securityContext:
      capabilities:
        add:
        - IPC_LOCK
    volumeMounts:
    - mountPath: /dev/shm
      name: dshm
  hostIPC: true
  hostname: llama-3-8b-node-0-0
  nodeSelector:
    node.kubernetes.io/instance-type: p4d.24xlarge
  shareProcessNamespace: true
  tolerations:
  - effect: NoSchedule
    key: nvidia.com/gpu
    operator: Exists
  - effect: NoSchedule
    key: vpc.amazonaws.com/efa
    operator: Exists
  volumes:
  - emptyDir:
      medium: Memory
      sizeLimit: 64Gi
    name: dshm
```

## 훈련성능 측정결과 ##

### p4d.24xlarge ###

* Nitro / 1 Pod / 4 GPU / 4 EFA - 0:27:38 ( via P2P/CUMEM/read, Check P2P Type isAllDirectP2p 1 directMode 0)
* Nitro / 4 Pod / 1 GPU / 1 EFA - 0:39:32 ( via NET/Libfabric/0/GDRDMA )        
* Nitro / 4 Pod / 1 GPU / 1 ENI - ???     ( via  )        


---
* nvidia fabric manager //
---


#### 전송 메커니즘의 근본적 차이 ####

* Write 모드 (Push): 송신 GPU가 데이터를 상대방 메모리에 직접 밀어 넣습니다. NVLink의 대역폭을 거의 100% 활용하며, 이론상 수백 GB/s의 속도를 냅니다. 즉시 전송이 시작되므로 지연 시간이 극도로 낮습니다
* Read 모드 (Pull): 수신 GPU가 상대방 메모리를 읽어옵니다. 가상화 계층의 간섭과 데이터 확인 절차(Handshake) 때문에 대역폭이 30%~50% 이상 급감할 수 있습니다. "데이터가 준비되었는가?" 확인 -> "읽기 요청" -> "데이터 수신"이라는 복잡한 단계를 거치므로 지연 시간이 훨씬 깁니다. 8B 모델처럼 수많은 파라미터를 빈번하게 교환해야 하는 환경에서는 이 미세한 지연이 누적되어 큰 차이를 만듭니다.

#### 니트로 EC2의 태생적 한계 ####
* 하이퍼바이저의 간섭: Nitro 하이퍼바이저가 보안과 격리를 위해 GPU 간의 직접 쓰기(Direct Write) 권한을 제한합니다.
* 결과: 하드웨어는 NVLink라는 초고속 도로를 갖고 있지만, 규제(가상화 설정) 때문에 속도 제한이 걸린 read 모드로만 달려야 하는 상황입니다. NVIDIA NCCL 기술 문서에서도 가상화에 따른 이러한 성능 저하를 명시하고 있습니다.


---
## 멀티 GPU ##
하나의 파드(Pod) 안에 8개의 GPU를 모두 할당받았더라도 가상화 보안 제약에 따른 P2P(Peer-to-Peer) 차단은 발생합니다.
단일 노드, 단일 파드 환경에서도 NVLink가 제 성능을 못 내는 구체적인 이유는 다음과 같습니다.

#### 1. 가상화 레이어의 간섭 (AWS Nitro System 등) ###
2025년 현재 AWS P5 인스턴스 등은 Nitro Hypervisor를 사용합니다.
* 보안 격리: 파드 내부에서 돌아가는 프로세스가 물리 하드웨어에 직접 쓰기(Direct Write)를 시도할 때, 가상화 레이어는 이를 보안 위협으로 간주하거나 주소 변환(IOMMU) 오버헤드를 발생시킵니다.
* Push 방식의 한계: GPU 0이 GPU 1의 메모리 주소에 직접 데이터를 밀어넣는(Push) 방식은 가상 메모리 주소 체계에서 복잡한 검증을 거쳐야 하므로, 성능 저하를 피하기 위해 운영체제가 이를 차단하고 더 안전하지만 느린 Copy(P2P-Read 또는 Shared Memory) 방식으로 우회시킵니다.

#### 2. NCCL의 "P2P: Disabled" 상태 ####
실제로 파드 안에서 NCCL 로그를 확인해 보면(export NCCL_DEBUG=INFO), 다음과 같은 메시지를 자주 볼 수 있습니다.
NCCL INFO P2P is disabled 또는 NCCL INFO Transport: SHM/Socket
* 의미: 하드웨어는 NVLink로 연결되어 있지만, NCCL이 가상화 환경의 제약으로 인해 P2P 다이렉트 통신을 사용할 수 없다고 판단한 것입니다.
* 결과: NVLink라는 900GB/s 도로를 놔두고, 시스템 메모리(Shared Memory)를 거쳐서 데이터를 복사합니다. 이때 속도는 NVLink의 이론적 수치에 한참 못 미치게 됩니다.

#### 3. K8s/컨테이너 환경의 추가 오버헤드 ####
쿠버네티스(K8s) 환경에서는 GPU 장치를 컨테이너에 매핑하는 과정에서 IPC(Inter-Process Communication) 제약이 추가됩니다.
파드 설정에서 shareProcessNamespace: true나 특권 모드(privileged: true) 등을 적절히 설정하지 않으면, 같은 파드 내의 프로세스끼리도 GPU 메모리를 공유하는 데 제약이 생겨 NVLink 활용도가 더 떨어집니다.

#### 4. 해결을 위한 실무적 접근 (2025년 기준) ####
단일 파드 내에서도 NVLink 효율을 높이려면 다음을 확인해야 합니다.
* NCCL 환경 변수 강제: P2P를 강제로 활성화해 봅니다. (안정성이 떨어질 수 있으나 속도는 올라갑니다.)
    * export NCCL_P2P_DISABLE=0
    * export NCCL_IB_DISABLE=1 (단일 노드인 경우 인터노드용 기능을 끄고 내부망 집중)
* SHM 크기 확장: K8s 파드 설정에서 /dev/shm (공유 메모리) 크기를 충분히 크게(예: 64Gi 이상) 잡아야 NCCL이 NVLink 대용으로 쓰는 복사 경로가 막히지 않습니다.
* Fabric Manager 확인: AWS P 시리즈는 nvidia-fabricmanager가 호스트에서 제대로 돌고 있어야 NVLink 스위칭이 활성화됩니다. 파드 내부에서도 이를 인식하는지 체크가 필요합니다.

*  결론
질문자님의 말씀대로 "한 파드 안인데 왜 NVLink가 느리지?"라는 의문은 지극히 타당합니다. 가상화된 클라우드 환경에서는 물리적인 연결(NVLink)보다 논리적인 보안 정책(P2P 차단)이 통신 속도를 지배하기 때문입니다.
결국 이 제약을 뚫기 위해서는 단순히 데이터만 보내는 방식이 아니라, NCCL 통신을 최소화하도록 연산 자체를 묶는 TP(Tensor Parallel) 전략이 단일 노드 내에서도 여전히 유효하고 필요한 이유가 됩니다.


---
## 이론적 backgound ##

8개의 GPU가 NVLink로 묶여 있어도, 노드 밖의 파라미터를 기다려야 하는 순간 NVLink의 초고속 성능은 "무용지물"이 됩니다.

#### 1. 동기화의 함정: "가장 느린 놈이 전체 속도를 정한다" ####
DeepSpeed ZeRO-3 같은 데이터 병렬 처리에서 16개 GPU(2개 노드)가 학습할 때, 한 걸음(Step)을 마치려면 16개 GPU의 파라미터가 모두 일치해야 합니다.
* 노드 내부(1~8번 GPU): NVLink로 눈 깜짝할 새 대화를 끝냅니다.
* 노드 외부(9~16번 GPU): EFA라는 좁은 통로를 통해 데이터를 주고받습니다.
* 결과: 18번 GPU는 자기들끼리 대화를 마쳤어도, 916번 GPU의 데이터가 EFA를 타고 올 때까지 아무것도 못 하고 멍하니 기다려야 합니다. (이것을 Barrier Wait라고 합니다.)

#### 2. NVLink가 억울한 이유 ####
NVLink가 아무리 600GB/s로 데이터를 쏠 수 있어도, 데이터를 받는 상대방 노드로 가는 길이 50GB/s(EFA)뿐이라면, NVLink는 자신의 성능의 10%도 쓰지 못하고 EFA 속도에 맞춰서 데이터를 조금씩 흘려보낼 수밖에 없습니다.

#### 3. 왜 멀티 노드에서 효율이 급락하는가? ####
단일 노드에서는 모든 통신이 NVLink 전용 도로에서 일어나지만, 멀티 노드가 되는 순간 모든 통신 프로세스가 'EFA라는 톨게이트'를 통과해야 하는 구조로 바뀝니다.
* 노드 1대일 때 속도: NVLink 속도 (매우 빠름)
* 노드 2대 이상일 때 속도: EFA 속도 + 네트워크 지연 시간(Latency) + 동기화 오버헤드 (갑자기 느려짐)

#### 4. 그래서 전문가들이 내리는 결론 ####
질문자님이 말씀하신 대로 "노드를 늘리는 순간 NVLink는 무용지물"이 되는 현상을 막기 위해, 고도로 설계된 학습 환경(예: NVIDIA SuperPOD)에서는 인피니밴드(InfiniBand)를 GPU 개수만큼 꽂아서 인터노드 대역폭을 NVLink 급으로 올리려고 수십억 원을 투자합니다.
하지만 일반적인 EFA 환경에서는 대역폭 차이가 극명하므로, 앞서 논의한 것처럼 "노드 간(EFA)에는 통신을 거의 안 하게 만들고, 노드 내(NVLink)에서만 통신이 폭발하게 만드는" TP/PP 전략이 무조건 강제되는 것입니다.
지금 겪고 계신 "프로세스가 오랫동안 머물러 있는 현상"은 바로 이 톨게이트(EFA)에 데이터가 너무 많이 몰려서 발생한 정체 현상의 명확한 증거입니다.

---

NVIDIA의 메가트론(Megatron-LM)은 바로 그 '복잡하고 고통스러운 병렬화 설계'를 구조적으로 해결하기 위해 탄생한 프레임워크입니다.
단순히 자동화라고 하기엔 개발자가 초기에 지정해줘야 할 파라미터가 있지만, "토폴로지에 최적화된 병렬 연산 그래프를 생성하고 통신을 스케줄링"한다는 점에서 질문하신 취지에 딱 맞는 솔루션입니다.
메가트론이 이 문제를 어떻게 해결하는지 핵심을 정리해 드립니다.

#### 1. 토폴로지 맞춤형 설계 (Intra-node vs Inter-node) ####
* 메가트론은 실행 시 사용자가 입력한 TP(Tensor Parallel)와 PP(Pipeline Parallel) 크기를 바탕으로 GPU들을 그룹핑합니다.
* TP 그룹 (노드 내부): "너희 8개는 NVLink로 묶여 있으니, 모델의 레이어 하나를 조각조각 나눠서 같이 계산해!" (가장 빈번한 통신을 NVLink 구간에 가둠)
* PP/DP 그룹 (노드 외부): "너희는 멀리 떨어져 있고 길이 좁으니(EFA/InfiniBand), 계산이 다 끝난 결과값만 가끔 주고받아!" (인터노드 통신 최소화)

#### 2. 메가트론이 "자동화"해주는 핵심 기능 ####
개발자가 "Llama 3 모델을 TP=8, PP=2로 돌려줘"라고 실행 인자만 주면, 메가트론 내부 코드가 다음을 자동으로 처리합니다.
* 커스텀 레이어 구현: 일반적인 PyTorch 레이어를 TP용 병렬 레이어(Column/Row Parallel Linear)로 알아서 교체하여 실행합니다.
* 커뮤니케이션 그룹 설정: 어떤 GPU끼리 NVLink로 대화할지, 어떤 GPU끼리 네트워크망(EFA)을 쓸지 통신 그룹(Process Group)을 자동으로 분리합니다.
* 마이크로 배치(Micro-batch) 스케줄링: 파이프라인 병렬화(PP) 시 GPU가 노는 시간(Bubble)을 줄이기 위해 데이터를 잘게 쪼개어 밀어넣는 복잡한 스케줄링을 알아서 수행합니다.

#### 3. 왜 DeepSpeed보다 메가트론이 이 분야의 강자인가? ####
* DeepSpeed ZeRO: "모델이 어떻게 생겼든 상관없어, 가중치만 잘게 쪼개서 뿌릴게" (인프라가 나쁘면 통신 지옥 발생)
* Megatron-LM: "모델 구조와 하드웨어 토폴로지를 보고, 어디를 찢어야 가장 빠를지 결정할게" (설계 단계부터 성능 최적화)

#### 4. 하지만 한계도 있습니다 ####
* "완전 자동"은 아닙니다. 2025년 현재에도 다음 결정은 개발자의 몫입니다.
* 최적의 조합 찾기: 내 서버의 EFA 성능과 NVLink 대역폭을 고려했을 때, TP를 4로 할지 8로 할지, PP를 몇 단계로 나눌지는 직접 실험하며 최적의 'Sweet Spot'을 찾아야 합니다


[nccl.log]
```
llama-3-8b-node-0-0:212:212 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
llama-3-8b-node-0-0:212:212 [0] NCCL INFO Bootstrap: Using eth0:10.0.5.188<0>
llama-3-8b-node-0-0:212:212 [0] NCCL INFO cudaDriverVersion 13000
llama-3-8b-node-0-0:212:212 [0] NCCL INFO NCCL_CUMEM_HOST_ENABLE set by environment to 0.
llama-3-8b-node-0-0:212:212 [0] NCCL INFO NCCL version 2.27.3+cuda12.9
llama-3-8b-node-0-0:212:212 [0] NCCL INFO Comm config Blocking set to 1
llama-3-8b-node-0-0:214:214 [2] NCCL INFO cudaDriverVersion 13000
llama-3-8b-node-0-0:214:214 [2] NCCL INFO NCCL_CUMEM_HOST_ENABLE set by environment to 0.
llama-3-8b-node-0-0:214:214 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
llama-3-8b-node-0-0:214:214 [2] NCCL INFO Bootstrap: Using eth0:10.0.5.188<0>
llama-3-8b-node-0-0:214:214 [2] NCCL INFO NCCL version 2.27.3+cuda12.9
llama-3-8b-node-0-0:214:214 [2] NCCL INFO Comm config Blocking set to 1
llama-3-8b-node-0-0:213:213 [1] NCCL INFO cudaDriverVersion 13000
llama-3-8b-node-0-0:213:213 [1] NCCL INFO NCCL_CUMEM_HOST_ENABLE set by environment to 0.
llama-3-8b-node-0-0:213:213 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
llama-3-8b-node-0-0:213:213 [1] NCCL INFO Bootstrap: Using eth0:10.0.5.188<0>
llama-3-8b-node-0-0:213:213 [1] NCCL INFO NCCL version 2.27.3+cuda12.9
llama-3-8b-node-0-0:215:215 [3] NCCL INFO cudaDriverVersion 13000
llama-3-8b-node-0-0:215:215 [3] NCCL INFO NCCL_CUMEM_HOST_ENABLE set by environment to 0.
llama-3-8b-node-0-0:215:215 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
llama-3-8b-node-0-0:213:213 [1] NCCL INFO Comm config Blocking set to 1
llama-3-8b-node-0-0:215:215 [3] NCCL INFO Bootstrap: Using eth0:10.0.5.188<0>
llama-3-8b-node-0-0:215:215 [3] NCCL INFO NCCL version 2.27.3+cuda12.9
llama-3-8b-node-0-0:215:215 [3] NCCL INFO Comm config Blocking set to 1
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net.so
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net.so
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NET/Plugin: Loaded net plugin Libfabric (v10)
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NET/Plugin: Loaded net plugin Libfabric (v10)
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v9 symbol.
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v9 symbol.
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
llama-3-8b-node-0-0:214:901 [2] NCCL INFO Successfully loaded external plugin libnccl-net.so
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Successfully loaded external plugin libnccl-net.so
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.16.2
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.16.2
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NET/OFI Using Libfabric version 2.1
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NET/OFI Using Libfabric version 2.1
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NET/OFI Using CUDA driver version 13000 with runtime 12090
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NET/OFI Using CUDA driver version 13000 with runtime 12090
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NET/OFI Configuring AWS-specific options
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NET/OFI Configuring AWS-specific options
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NET/OFI Setting provider_filter to efa
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NET/OFI Setting provider_filter to efa
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NET/OFI Running on p4d.24xlarge platform, topology file /opt/amazon/ofi-nccl/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NET/OFI Running on p4d.24xlarge platform, topology file /opt/amazon/ofi-nccl/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NET/OFI Internode latency set at 75.0 us
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NET/OFI Internode latency set at 75.0 us
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NET/OFI Using transport protocol SENDRECV (platform set)
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NET/OFI Using transport protocol SENDRECV (platform set)

[2025-12-31 16:29:37] llama-3-8b-node-0-0:214:901 [2] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):209 NCCL WARN NET/OFI Failed to initialize sendrecv protocol

[2025-12-31 16:29:37] llama-3-8b-node-0-0:212:900 [0] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):209 NCCL WARN NET/OFI Failed to initialize sendrecv protocol

[2025-12-31 16:29:37] llama-3-8b-node-0-0:212:900 [0] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):352 NCCL WARN NET/OFI aws-ofi-nccl initialization failed

[2025-12-31 16:29:37] llama-3-8b-node-0-0:214:901 [2] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):352 NCCL WARN NET/OFI aws-ofi-nccl initialization failed

[2025-12-31 16:29:37] llama-3-8b-node-0-0:212:900 [0] ncclResult_t nccl_net_ofi_init_v6(ncclDebugLogger_t):166 NCCL WARN NET/OFI Initializing plugin failed

[2025-12-31 16:29:37] llama-3-8b-node-0-0:214:901 [2] ncclResult_t nccl_net_ofi_init_v6(ncclDebugLogger_t):166 NCCL WARN NET/OFI Initializing plugin failed
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NET/IB : No device found.
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NET/IB : No device found.
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NET/IB : Using [RO]; OOB eth0:10.0.5.188<0>
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NET/IB : Using [RO]; OOB eth0:10.0.5.188<0>
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NET/Socket : Using [0]eth0:10.0.5.188<0>
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NET/Socket : Using [0]eth0:10.0.5.188<0>
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Initialized NET plugin Socket
llama-3-8b-node-0-0:214:901 [2] NCCL INFO Initialized NET plugin Socket
llama-3-8b-node-0-0:214:901 [2] NCCL INFO Assigned NET plugin Socket to comm
llama-3-8b-node-0-0:214:901 [2] NCCL INFO Using network Socket
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Assigned NET plugin Socket to comm
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Using network Socket
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net.so
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NET/Plugin: Loaded net plugin Libfabric (v10)
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v9 symbol.
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
llama-3-8b-node-0-0:213:902 [1] NCCL INFO Successfully loaded external plugin libnccl-net.so
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.16.2
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NET/OFI Using Libfabric version 2.1
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NET/OFI Using CUDA driver version 13000 with runtime 12090
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NET/OFI Configuring AWS-specific options
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NET/OFI Setting provider_filter to efa
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NET/OFI Running on p4d.24xlarge platform, topology file /opt/amazon/ofi-nccl/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NET/OFI Internode latency set at 75.0 us
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NET/OFI Using transport protocol SENDRECV (platform set)
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NET/Plugin: Plugin name set by env to libnccl-net.so
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NET/Plugin: Loaded net plugin Libfabric (v10)
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v10 symbol.
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v9 symbol.
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v8 symbol.
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v7 symbol.
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NET/Plugin: Failed to find ncclCollNetPlugin_v6 symbol.
llama-3-8b-node-0-0:215:903 [3] NCCL INFO Successfully loaded external plugin libnccl-net.so
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NET/OFI Initializing aws-ofi-nccl 1.16.2
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NET/OFI Using Libfabric version 2.1
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NET/OFI Using CUDA driver version 13000 with runtime 12090
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NET/OFI Configuring AWS-specific options
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NET/OFI Setting provider_filter to efa
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NET/OFI Running on p4d.24xlarge platform, topology file /opt/amazon/ofi-nccl/share/aws-ofi-nccl/xml/p4d-24xl-topo.xml
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NET/OFI Internode latency set at 75.0 us
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NET/OFI Using transport protocol SENDRECV (platform set)

[2025-12-31 16:29:37] llama-3-8b-node-0-0:213:902 [1] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):209 NCCL WARN NET/OFI Failed to initialize sendrecv protocol

[2025-12-31 16:29:37] llama-3-8b-node-0-0:213:902 [1] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):352 NCCL WARN NET/OFI aws-ofi-nccl initialization failed

[2025-12-31 16:29:37] llama-3-8b-node-0-0:213:902 [1] ncclResult_t nccl_net_ofi_init_v6(ncclDebugLogger_t):166 NCCL WARN NET/OFI Initializing plugin failed
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NET/IB : No device found.
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NET/IB : Using [RO]; OOB eth0:10.0.5.188<0>
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NET/Socket : Using [0]eth0:10.0.5.188<0>
llama-3-8b-node-0-0:213:902 [1] NCCL INFO Initialized NET plugin Socket
llama-3-8b-node-0-0:213:902 [1] NCCL INFO Assigned NET plugin Socket to comm
llama-3-8b-node-0-0:213:902 [1] NCCL INFO Using network Socket

[2025-12-31 16:29:37] llama-3-8b-node-0-0:215:903 [3] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):209 NCCL WARN NET/OFI Failed to initialize sendrecv protocol

[2025-12-31 16:29:37] llama-3-8b-node-0-0:215:903 [3] int nccl_net_ofi_create_plugin(nccl_net_ofi_plugin_t**):352 NCCL WARN NET/OFI aws-ofi-nccl initialization failed

[2025-12-31 16:29:37] llama-3-8b-node-0-0:215:903 [3] ncclResult_t nccl_net_ofi_init_v6(ncclDebugLogger_t):166 NCCL WARN NET/OFI Initializing plugin failed
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NET/IB : No device found.
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NET/IB : Using [RO]; OOB eth0:10.0.5.188<0>
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NCCL_SOCKET_IFNAME set by environment to ^docker0,lo
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NET/Socket : Using [0]eth0:10.0.5.188<0>
llama-3-8b-node-0-0:215:903 [3] NCCL INFO Initialized NET plugin Socket
llama-3-8b-node-0-0:215:903 [3] NCCL INFO Assigned NET plugin Socket to comm
llama-3-8b-node-0-0:215:903 [3] NCCL INFO Using network Socket
llama-3-8b-node-0-0:214:901 [2] NCCL INFO ncclCommInitRankConfig comm 0x55ef6e3de3b0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 201c0 commId 0xd6eff7eb36368c2f - Init START
llama-3-8b-node-0-0:212:900 [0] NCCL INFO ncclCommInitRankConfig comm 0x55d36557b5a0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 101c0 commId 0xd6eff7eb36368c2f - Init START
llama-3-8b-node-0-0:213:902 [1] NCCL INFO ncclCommInitRankConfig comm 0x5578f3f1dcc0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 101d0 commId 0xd6eff7eb36368c2f - Init START
llama-3-8b-node-0-0:213:902 [1] NCCL INFO RAS client listening socket at ::1<28028>
llama-3-8b-node-0-0:215:903 [3] NCCL INFO ncclCommInitRankConfig comm 0x55f4faa4a6f0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 201d0 commId 0xd6eff7eb36368c2f - Init START
llama-3-8b-node-0-0:214:901 [2] NCCL INFO RAS client listening socket at ::1<28028>
llama-3-8b-node-0-0:215:903 [3] NCCL INFO RAS client listening socket at ::1<28028>
llama-3-8b-node-0-0:212:900 [0] NCCL INFO RAS client listening socket at ::1<28028>
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Bootstrap timings total 0.023183 (create 0.000037, send 0.000071, recv 0.018699, ring 0.000054, delay 0.000000)
llama-3-8b-node-0-0:214:901 [2] NCCL INFO Bootstrap timings total 0.023598 (create 0.000059, send 0.000175, recv 0.022655, ring 0.000046, delay 0.000000)
llama-3-8b-node-0-0:215:903 [3] NCCL INFO Bootstrap timings total 0.000888 (create 0.000035, send 0.000077, recv 0.000119, ring 0.000042, delay 0.000000)
llama-3-8b-node-0-0:213:902 [1] NCCL INFO Bootstrap timings total 0.004647 (create 0.000041, send 0.000072, recv 0.000215, ring 0.003594, delay 0.000000)
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NCCL_P2P_LEVEL set by environment to NVL
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NCCL_P2P_LEVEL set by environment to NVL
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NCCL_P2P_LEVEL set by environment to NVL
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NCCL_P2P_LEVEL set by environment to NVL
llama-3-8b-node-0-0:213:902 [1] NCCL INFO Setting affinity for GPU 1 to 0-23,48-71
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NCCL_NVLS_ENABLE set by environment to 1.
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NVLS multicast support is available on dev 1 (NVLS_NCHANNELS 16)
llama-3-8b-node-0-0:215:903 [3] NCCL INFO Setting affinity for GPU 3 to 0-23,48-71
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NCCL_NVLS_ENABLE set by environment to 1.
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NVLS multicast support is available on dev 3 (NVLS_NCHANNELS 16)
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Setting affinity for GPU 0 to 0-23,48-71
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NCCL_NVLS_ENABLE set by environment to 1.
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NVLS multicast support is available on dev 0 (NVLS_NCHANNELS 16)
llama-3-8b-node-0-0:214:901 [2] NCCL INFO Setting affinity for GPU 2 to 0-23,48-71
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NCCL_NVLS_ENABLE set by environment to 1.
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NVLS multicast support is available on dev 2 (NVLS_NCHANNELS 16)
llama-3-8b-node-0-0:214:901 [2] NCCL INFO comm 0x55ef6e3de3b0 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0
llama-3-8b-node-0-0:215:903 [3] NCCL INFO comm 0x55f4faa4a6f0 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
llama-3-8b-node-0-0:212:900 [0] NCCL INFO comm 0x55d36557b5a0 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0
llama-3-8b-node-0-0:213:902 [1] NCCL INFO comm 0x5578f3f1dcc0 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 00/24 : 0 1 2 3
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 01/24 : 0 1 2 3
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 02/24 : 0 1 2 3
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 03/24 : 0 1 2 3
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 04/24 : 0 1 2 3
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 05/24 : 0 1 2 3
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 06/24 : 0 1 2 3
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 07/24 : 0 1 2 3
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 08/24 : 0 1 2 3
llama-3-8b-node-0-0:214:901 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 09/24 : 0 1 2 3
llama-3-8b-node-0-0:214:901 [2] NCCL INFO P2P Chunksize set to 524288
llama-3-8b-node-0-0:215:903 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] -1/-1/-1->3->2 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] -1/-1/-1->3->2 [11] -1/-1/-1->3->2 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->2 [17] -1/-1/-1->3->2 [18] -1/-1/-1->3->2 [19] -1/-1/-1->3->2 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] -1/-1/-1->3->2 [23] -1/-1/-1->3->2
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 10/24 : 0 1 2 3
llama-3-8b-node-0-0:213:902 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 11/24 : 0 1 2 3
llama-3-8b-node-0-0:215:903 [3] NCCL INFO P2P Chunksize set to 524288
llama-3-8b-node-0-0:213:902 [1] NCCL INFO P2P Chunksize set to 524288
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 12/24 : 0 1 2 3
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 13/24 : 0 1 2 3
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 14/24 : 0 1 2 3
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 15/24 : 0 1 2 3
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 16/24 : 0 1 2 3
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 17/24 : 0 1 2 3
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 18/24 : 0 1 2 3
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 19/24 : 0 1 2 3
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 20/24 : 0 1 2 3
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 21/24 : 0 1 2 3
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 22/24 : 0 1 2 3
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Channel 23/24 : 0 1 2 3
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
llama-3-8b-node-0-0:212:900 [0] NCCL INFO P2P Chunksize set to 524288
llama-3-8b-node-0-0:213:902 [1] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
llama-3-8b-node-0-0:213:909 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 19
llama-3-8b-node-0-0:213:908 [1] NCCL INFO [Proxy Service] Device 1 CPU core 2
llama-3-8b-node-0-0:214:901 [2] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
llama-3-8b-node-0-0:212:900 [0] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Check P2P Type isAllDirectP2p 1 directMode 0
llama-3-8b-node-0-0:215:903 [3] NCCL INFO PROFILER/Plugin: Could not find: libnccl-profiler.so. 
llama-3-8b-node-0-0:214:910 [2] NCCL INFO [Proxy Service] Device 2 CPU core 6
llama-3-8b-node-0-0:214:911 [2] NCCL INFO [Proxy Service UDS] Device 2 CPU core 56
llama-3-8b-node-0-0:215:913 [3] NCCL INFO [Proxy Service] Device 3 CPU core 69
llama-3-8b-node-0-0:212:912 [0] NCCL INFO [Proxy Service] Device 0 CPU core 7
llama-3-8b-node-0-0:215:915 [3] NCCL INFO [Proxy Service UDS] Device 3 CPU core 53
llama-3-8b-node-0-0:212:914 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 68
llama-3-8b-node-0-0:213:902 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
llama-3-8b-node-0-0:213:902 [1] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
llama-3-8b-node-0-0:212:900 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
llama-3-8b-node-0-0:212:900 [0] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
llama-3-8b-node-0-0:215:903 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
llama-3-8b-node-0-0:215:903 [3] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
llama-3-8b-node-0-0:214:901 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
llama-3-8b-node-0-0:214:901 [2] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
llama-3-8b-node-0-0:212:900 [0] NCCL INFO CC Off, workFifoBytes 1048576
llama-3-8b-node-0-0:215:903 [3] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
llama-3-8b-node-0-0:214:901 [2] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
llama-3-8b-node-0-0:213:902 [1] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
llama-3-8b-node-0-0:212:900 [0] NCCL INFO TUNER/Plugin: Could not find: libnccl-tuner.so. Using internal tuner plugin.
llama-3-8b-node-0-0:213:902 [1] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v4 symbol.
llama-3-8b-node-0-0:214:901 [2] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v4 symbol.
llama-3-8b-node-0-0:215:903 [3] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v4 symbol.
llama-3-8b-node-0-0:212:900 [0] NCCL INFO TUNER/Plugin: Failed to find ncclTunerPlugin_v4 symbol.
llama-3-8b-node-0-0:214:901 [2] NCCL INFO TUNER/Plugin: Using tuner plugin nccl_ofi_tuner
llama-3-8b-node-0-0:213:902 [1] NCCL INFO TUNER/Plugin: Using tuner plugin nccl_ofi_tuner
llama-3-8b-node-0-0:212:900 [0] NCCL INFO TUNER/Plugin: Using tuner plugin nccl_ofi_tuner
llama-3-8b-node-0-0:215:903 [3] NCCL INFO TUNER/Plugin: Using tuner plugin nccl_ofi_tuner
llama-3-8b-node-0-0:213:902 [1] NCCL INFO NET/OFI NCCL_OFI_TUNER is not available for platform : p4d.24xlarge, Fall back to NCCL's tuner
llama-3-8b-node-0-0:212:900 [0] NCCL INFO NET/OFI NCCL_OFI_TUNER is not available for platform : p4d.24xlarge, Fall back to NCCL's tuner
llama-3-8b-node-0-0:214:901 [2] NCCL INFO NET/OFI NCCL_OFI_TUNER is not available for platform : p4d.24xlarge, Fall back to NCCL's tuner
llama-3-8b-node-0-0:215:903 [3] NCCL INFO NET/OFI NCCL_OFI_TUNER is not available for platform : p4d.24xlarge, Fall back to NCCL's tuner
llama-3-8b-node-0-0:213:902 [1] NCCL INFO ncclCommInitRankConfig comm 0x5578f3f1dcc0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 101d0 commId 0xd6eff7eb36368c2f - Init COMPLETE
llama-3-8b-node-0-0:212:900 [0] NCCL INFO ncclCommInitRankConfig comm 0x55d36557b5a0 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 101c0 commId 0xd6eff7eb36368c2f - Init COMPLETE
llama-3-8b-node-0-0:215:903 [3] NCCL INFO ncclCommInitRankConfig comm 0x55f4faa4a6f0 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 201d0 commId 0xd6eff7eb36368c2f - Init COMPLETE
llama-3-8b-node-0-0:214:901 [2] NCCL INFO ncclCommInitRankConfig comm 0x55ef6e3de3b0 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 201c0 commId 0xd6eff7eb36368c2f - Init COMPLETE
llama-3-8b-node-0-0:213:902 [1] NCCL INFO Init timings - ncclCommInitRankConfig: rank 1 nranks 4 total 0.34 (kernels 0.16, alloc 0.12, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.02, rest 0.01)
llama-3-8b-node-0-0:212:900 [0] NCCL INFO Init timings - ncclCommInitRankConfig: rank 0 nranks 4 total 0.45 (kernels 0.24, alloc 0.14, bootstrap 0.02, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.02, rest 0.01)
llama-3-8b-node-0-0:214:901 [2] NCCL INFO Init timings - ncclCommInitRankConfig: rank 2 nranks 4 total 0.39 (kernels 0.17, alloc 0.14, bootstrap 0.02, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.02, rest 0.01)
llama-3-8b-node-0-0:215:903 [3] NCCL INFO Init timings - ncclCommInitRankConfig: rank 3 nranks 4 total 0.33 (kernels 0.16, alloc 0.12, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.02, rest 0.01)
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).
max_steps is given, it will override any value given in num_train_epochs
Using auto half precision backend
Gradient accumulation steps mismatch: GradientAccumulationPlugin has 1, DeepSpeed config has 4. Using DeepSpeed's value.
Detected ZeRO Offload and non-DeepSpeed optimizers: This combination should work as long as the custom optimizer has both CPU and GPU implementation (except LAMB)
[rank1]:W1231 16:29:41.987000 213 site-packages/torch/utils/cpp_extension.py:2425] TORCH_CUDA_ARCH_LIST is not set, all archs for visible cards are included for compilation. 
[rank1]:W1231 16:29:41.987000 213 site-packages/torch/utils/cpp_extension.py:2425] If this is not desired, please set os.environ['TORCH_CUDA_ARCH_LIST'] to specific architectures.
Adam Optimizer #0 is created with AVX512 arithmetic capability.
Config: alpha=0.000020, betas=(0.900000, 0.999000), weight_decay=0.010000, adam_w=1
llama-3-8b-node-0-0:214:214 [2] NCCL INFO Comm config Blocking set to 1
llama-3-8b-node-0-0:212:212 [0] NCCL INFO Comm config Blocking set to 1
llama-3-8b-node-0-0:213:213 [1] NCCL INFO Comm config Blocking set to 1
llama-3-8b-node-0-0:215:215 [3] NCCL INFO Comm config Blocking set to 1
llama-3-8b-node-0-0:213:1339 [1] NCCL INFO Assigned NET plugin Socket to comm
llama-3-8b-node-0-0:215:1342 [3] NCCL INFO Assigned NET plugin Socket to comm
llama-3-8b-node-0-0:214:1333 [2] NCCL INFO Assigned NET plugin Socket to comm
llama-3-8b-node-0-0:213:1339 [1] NCCL INFO Using network Socket
llama-3-8b-node-0-0:215:1342 [3] NCCL INFO Using network Socket
llama-3-8b-node-0-0:214:1333 [2] NCCL INFO Using network Socket
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Assigned NET plugin Socket to comm
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Using network Socket
llama-3-8b-node-0-0:214:1333 [2] NCCL INFO ncclCommSplit comm 0x55ef6e92e120 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 201c0 parent 0x55ef6e3de3b0 splitCount 1 color 2003953581 key 2- Init START
llama-3-8b-node-0-0:213:1339 [1] NCCL INFO ncclCommSplit comm 0x5578f44697d0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 101d0 parent 0x5578f3f1dcc0 splitCount 1 color 2003953581 key 1- Init START
llama-3-8b-node-0-0:215:1342 [3] NCCL INFO ncclCommSplit comm 0x55f4fb783b90 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 201d0 parent 0x55f4faa4a6f0 splitCount 1 color 2003953581 key 3- Init START
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO ncclCommSplit comm 0x55d35e294380 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 101c0 parent 0x55d36557b5a0 splitCount 1 color 2003953581 key 0- Init START
llama-3-8b-node-0-0:215:1342 [3] NCCL INFO Setting affinity for GPU 3 to 0-23,48-71
llama-3-8b-node-0-0:214:1333 [2] NCCL INFO Setting affinity for GPU 2 to 0-23,48-71
llama-3-8b-node-0-0:215:1342 [3] NCCL INFO NVLS multicast support is available on dev 3 (NVLS_NCHANNELS 16)
llama-3-8b-node-0-0:214:1333 [2] NCCL INFO NVLS multicast support is available on dev 2 (NVLS_NCHANNELS 16)
llama-3-8b-node-0-0:213:1339 [1] NCCL INFO Setting affinity for GPU 1 to 0-23,48-71
llama-3-8b-node-0-0:213:1339 [1] NCCL INFO NVLS multicast support is available on dev 1 (NVLS_NCHANNELS 16)
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Setting affinity for GPU 0 to 0-23,48-71
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO NVLS multicast support is available on dev 0 (NVLS_NCHANNELS 16)
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO comm 0x55d35e294380 rank 0 nRanks 4 nNodes 1 localRanks 4 localRank 0 MNNVL 0
llama-3-8b-node-0-0:214:1333 [2] NCCL INFO comm 0x55ef6e92e120 rank 2 nRanks 4 nNodes 1 localRanks 4 localRank 2 MNNVL 0
llama-3-8b-node-0-0:213:1339 [1] NCCL INFO comm 0x5578f44697d0 rank 1 nRanks 4 nNodes 1 localRanks 4 localRank 1 MNNVL 0
llama-3-8b-node-0-0:215:1342 [3] NCCL INFO comm 0x55f4fb783b90 rank 3 nRanks 4 nNodes 1 localRanks 4 localRank 3 MNNVL 0
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 00/24 : 0 1 2 3
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 01/24 : 0 1 2 3
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 02/24 : 0 1 2 3
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 03/24 : 0 1 2 3
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 04/24 : 0 1 2 3
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 05/24 : 0 1 2 3
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 06/24 : 0 1 2 3
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 07/24 : 0 1 2 3
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 08/24 : 0 1 2 3
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 09/24 : 0 1 2 3
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 10/24 : 0 1 2 3
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 11/24 : 0 1 2 3
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 12/24 : 0 1 2 3
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 13/24 : 0 1 2 3
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 14/24 : 0 1 2 3
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 15/24 : 0 1 2 3
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 16/24 : 0 1 2 3
llama-3-8b-node-0-0:213:1339 [1] NCCL INFO Trees [0] 2/-1/-1->1->0 [1] 2/-1/-1->1->0 [2] 2/-1/-1->1->0 [3] 2/-1/-1->1->0 [4] 2/-1/-1->1->0 [5] 2/-1/-1->1->0 [6] 2/-1/-1->1->0 [7] 2/-1/-1->1->0 [8] 2/-1/-1->1->0 [9] 2/-1/-1->1->0 [10] 2/-1/-1->1->0 [11] 2/-1/-1->1->0 [12] 2/-1/-1->1->0 [13] 2/-1/-1->1->0 [14] 2/-1/-1->1->0 [15] 2/-1/-1->1->0 [16] 2/-1/-1->1->0 [17] 2/-1/-1->1->0 [18] 2/-1/-1->1->0 [19] 2/-1/-1->1->0 [20] 2/-1/-1->1->0 [21] 2/-1/-1->1->0 [22] 2/-1/-1->1->0 [23] 2/-1/-1->1->0
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 17/24 : 0 1 2 3
llama-3-8b-node-0-0:214:1333 [2] NCCL INFO Trees [0] 3/-1/-1->2->1 [1] 3/-1/-1->2->1 [2] 3/-1/-1->2->1 [3] 3/-1/-1->2->1 [4] 3/-1/-1->2->1 [5] 3/-1/-1->2->1 [6] 3/-1/-1->2->1 [7] 3/-1/-1->2->1 [8] 3/-1/-1->2->1 [9] 3/-1/-1->2->1 [10] 3/-1/-1->2->1 [11] 3/-1/-1->2->1 [12] 3/-1/-1->2->1 [13] 3/-1/-1->2->1 [14] 3/-1/-1->2->1 [15] 3/-1/-1->2->1 [16] 3/-1/-1->2->1 [17] 3/-1/-1->2->1 [18] 3/-1/-1->2->1 [19] 3/-1/-1->2->1 [20] 3/-1/-1->2->1 [21] 3/-1/-1->2->1 [22] 3/-1/-1->2->1 [23] 3/-1/-1->2->1
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 18/24 : 0 1 2 3
llama-3-8b-node-0-0:213:1339 [1] NCCL INFO P2P Chunksize set to 524288
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 19/24 : 0 1 2 3
llama-3-8b-node-0-0:214:1333 [2] NCCL INFO P2P Chunksize set to 524288
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 20/24 : 0 1 2 3
llama-3-8b-node-0-0:215:1342 [3] NCCL INFO Trees [0] -1/-1/-1->3->2 [1] -1/-1/-1->3->2 [2] -1/-1/-1->3->2 [3] -1/-1/-1->3->2 [4] -1/-1/-1->3->2 [5] -1/-1/-1->3->2 [6] -1/-1/-1->3->2 [7] -1/-1/-1->3->2 [8] -1/-1/-1->3->2 [9] -1/-1/-1->3->2 [10] -1/-1/-1->3->2 [11] -1/-1/-1->3->2 [12] -1/-1/-1->3->2 [13] -1/-1/-1->3->2 [14] -1/-1/-1->3->2 [15] -1/-1/-1->3->2 [16] -1/-1/-1->3->2 [17] -1/-1/-1->3->2 [18] -1/-1/-1->3->2 [19] -1/-1/-1->3->2 [20] -1/-1/-1->3->2 [21] -1/-1/-1->3->2 [22] -1/-1/-1->3->2 [23] -1/-1/-1->3->2
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 21/24 : 0 1 2 3
llama-3-8b-node-0-0:215:1342 [3] NCCL INFO P2P Chunksize set to 524288
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 22/24 : 0 1 2 3
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Channel 23/24 : 0 1 2 3
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Trees [0] 1/-1/-1->0->-1 [1] 1/-1/-1->0->-1 [2] 1/-1/-1->0->-1 [3] 1/-1/-1->0->-1 [4] 1/-1/-1->0->-1 [5] 1/-1/-1->0->-1 [6] 1/-1/-1->0->-1 [7] 1/-1/-1->0->-1 [8] 1/-1/-1->0->-1 [9] 1/-1/-1->0->-1 [10] 1/-1/-1->0->-1 [11] 1/-1/-1->0->-1 [12] 1/-1/-1->0->-1 [13] 1/-1/-1->0->-1 [14] 1/-1/-1->0->-1 [15] 1/-1/-1->0->-1 [16] 1/-1/-1->0->-1 [17] 1/-1/-1->0->-1 [18] 1/-1/-1->0->-1 [19] 1/-1/-1->0->-1 [20] 1/-1/-1->0->-1 [21] 1/-1/-1->0->-1 [22] 1/-1/-1->0->-1 [23] 1/-1/-1->0->-1
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO P2P Chunksize set to 524288
llama-3-8b-node-0-0:214:1343 [2] NCCL INFO [Proxy Service] Device 2 CPU core 4
llama-3-8b-node-0-0:214:1344 [2] NCCL INFO [Proxy Service UDS] Device 2 CPU core 53
llama-3-8b-node-0-0:213:1345 [1] NCCL INFO [Proxy Service] Device 1 CPU core 7
llama-3-8b-node-0-0:213:1346 [1] NCCL INFO [Proxy Service UDS] Device 1 CPU core 10
llama-3-8b-node-0-0:215:1347 [3] NCCL INFO [Proxy Service] Device 3 CPU core 11
llama-3-8b-node-0-0:215:1348 [3] NCCL INFO [Proxy Service UDS] Device 3 CPU core 18
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Check P2P Type isAllDirectP2p 1 directMode 0
llama-3-8b-node-0-0:212:1349 [0] NCCL INFO [Proxy Service] Device 0 CPU core 70
llama-3-8b-node-0-0:212:1350 [0] NCCL INFO [Proxy Service UDS] Device 0 CPU core 6
llama-3-8b-node-0-0:213:1339 [1] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
llama-3-8b-node-0-0:213:1339 [1] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
llama-3-8b-node-0-0:214:1333 [2] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
llama-3-8b-node-0-0:214:1333 [2] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
llama-3-8b-node-0-0:215:1342 [3] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
llama-3-8b-node-0-0:215:1342 [3] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO threadThresholds 8/8/64 | 32/8/64 | 512 | 512
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO 24 coll channels, 24 collnet channels, 0 nvls channels, 32 p2p channels, 32 p2p channels per peer
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO CC Off, workFifoBytes 1048576
llama-3-8b-node-0-0:214:1333 [2] NCCL INFO NET/OFI NCCL_OFI_TUNER is not available for platform : p4d.24xlarge, Fall back to NCCL's tuner
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO NET/OFI NCCL_OFI_TUNER is not available for platform : p4d.24xlarge, Fall back to NCCL's tuner
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO ncclCommSplit comm 0x55d35e294380 rank 0 nranks 4 cudaDev 0 nvmlDev 0 busId 101c0 parent 0x55d36557b5a0 splitCount 1 color 2003953581 key 0 - Init COMPLETE
llama-3-8b-node-0-0:214:1333 [2] NCCL INFO ncclCommSplit comm 0x55ef6e92e120 rank 2 nranks 4 cudaDev 2 nvmlDev 2 busId 201c0 parent 0x55ef6e3de3b0 splitCount 1 color 2003953581 key 2 - Init COMPLETE
llama-3-8b-node-0-0:212:1336 [0] NCCL INFO Init timings - ncclCommSplit: rank 0 nranks 4 total 1.06 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.03, rest 1.01)
llama-3-8b-node-0-0:214:1333 [2] NCCL INFO Init timings - ncclCommSplit: rank 2 nranks 4 total 1.32 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.03, rest 1.27)
llama-3-8b-node-0-0:213:1339 [1] NCCL INFO NET/OFI NCCL_OFI_TUNER is not available for platform : p4d.24xlarge, Fall back to NCCL's tuner
llama-3-8b-node-0-0:215:1342 [3] NCCL INFO NET/OFI NCCL_OFI_TUNER is not available for platform : p4d.24xlarge, Fall back to NCCL's tuner
llama-3-8b-node-0-0:213:1339 [1] NCCL INFO ncclCommSplit comm 0x5578f44697d0 rank 1 nranks 4 cudaDev 1 nvmlDev 1 busId 101d0 parent 0x5578f3f1dcc0 splitCount 1 color 2003953581 key 1 - Init COMPLETE
llama-3-8b-node-0-0:215:1342 [3] NCCL INFO ncclCommSplit comm 0x55f4fb783b90 rank 3 nranks 4 cudaDev 3 nvmlDev 3 busId 201d0 parent 0x55f4faa4a6f0 splitCount 1 color 2003953581 key 3 - Init COMPLETE
llama-3-8b-node-0-0:213:1339 [1] NCCL INFO Init timings - ncclCommSplit: rank 1 nranks 4 total 0.22 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.03, rest 0.18)
llama-3-8b-node-0-0:215:1342 [3] NCCL INFO Init timings - ncclCommSplit: rank 3 nranks 4 total 0.06 (kernels 0.00, alloc 0.00, bootstrap 0.00, allgathers 0.00, topo 0.02, graphs 0.00, connections 0.03, rest 0.01)
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 02/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 03/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 04/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 05/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 06/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 07/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 08/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 09/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 10/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 11/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 12/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 13/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 14/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 15/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 16/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 17/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 16/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 17/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 19/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 18/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 19/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 20/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 20/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 21/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 21/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 22/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Channel 23/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Channel 23/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1354 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
llama-3-8b-node-0-0:212:1352 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
llama-3-8b-node-0-0:215:1351 [3] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
llama-3-8b-node-0-0:214:1353 [2] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
Stage 3 initialize beginning
MA 14.96 GB         Max_MA 14.96 GB         CA 15.08 GB         Max_CA 15 GB 
CPU Virtual Memory:  used = 23.55 GB, percent = 2.1%
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 00/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 01/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 02/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 03/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 04/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 05/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 06/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 07/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 08/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 09/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 10/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 11/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 12/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 13/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 14/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 15/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 16/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 17/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 18/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 19/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 20/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 21/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 22/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Channel 23/0 : 3[3] -> 0[0] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 00/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 01/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 02/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 03/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 04/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 05/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 06/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 07/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 08/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 09/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 10/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 11/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 12/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 13/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 14/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 15/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 16/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 17/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 18/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 19/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 20/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 21/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 22/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Channel 23/0 : 1[1] -> 2[2] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 00/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 01/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 02/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 03/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 04/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 05/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 06/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 07/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 08/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 09/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 10/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 11/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 12/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 13/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 14/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 15/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 16/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 17/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 18/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 19/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 20/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 21/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 22/0 : 2[2] -> 3[3] via P2P/CUMEM/read
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Channel 23/0 : 2[2] -> 3[3] via P2P/CUMEM/read
DeepSpeedZeRoOffload initialize [begin]
MA 14.96 GB         Max_MA 14.96 GB         CA 15.08 GB         Max_CA 15 GB 
CPU Virtual Memory:  used = 24.06 GB, percent = 2.1%
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 00/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 01/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 02/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 03/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 04/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 05/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 06/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 07/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 08/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 09/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 10/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 11/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 12/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 13/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 14/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 15/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 16/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 17/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 18/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 19/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 20/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 21/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 22/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Channel 23/0 : 0[0] -> 1[1] via P2P/CUMEM/read
llama-3-8b-node-0-0:212:1358 [0] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
llama-3-8b-node-0-0:215:1355 [3] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
llama-3-8b-node-0-0:213:1356 [1] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
llama-3-8b-node-0-0:214:1357 [2] NCCL INFO Connected all rings, use ring PXN 0 GDR 1
Parameter Offload - Persistent parameters statistics: param_count = 65, numel = 266240
DeepSpeedZeRoOffload initialize [end]
MA 3.74 GB         Max_MA 15.2 GB         CA 15.33 GB         Max_CA 15 GB 
CPU Virtual Memory:  used = 24.01 GB, percent = 2.1%
Before creating fp16 partitions
MA 3.74 GB         Max_MA 3.74 GB         CA 15.33 GB         Max_CA 15 GB 
CPU Virtual Memory:  used = 24.01 GB, percent = 2.1%
After creating fp16 partitions: 3
MA 3.74 GB         Max_MA 3.74 GB         CA 3.74 GB         Max_CA 15 GB 
CPU Virtual Memory:  used = 28.22 GB, percent = 2.5%
Before creating fp32 partitions
MA 3.74 GB         Max_MA 3.74 GB         CA 3.74 GB         Max_CA 4 GB 
CPU Virtual Memory:  used = 28.21 GB, percent = 2.5%
After creating fp32 partitions
MA 3.74 GB         Max_MA 3.74 GB         CA 3.74 GB         Max_CA 4 GB 
CPU Virtual Memory:  used = 59.36 GB, percent = 5.3%
Before initializing optimizer states
MA 3.74 GB         Max_MA 3.74 GB         CA 3.74 GB         Max_CA 4 GB 
CPU Virtual Memory:  used = 59.35 GB, percent = 5.3%
After initializing optimizer states
MA 3.74 GB         Max_MA 3.74 GB         CA 3.74 GB         Max_CA 4 GB 
CPU Virtual Memory:  used = 90.98 GB, percent = 8.1%
After initializing ZeRO optimizer
MA 3.77 GB         Max_MA 5.73 GB         CA 5.73 GB         Max_CA 6 GB 
CPU Virtual Memory:  used = 107.13 GB, percent = 9.5%
```









## 레퍼런스 ##

* [Amazon EC2에서 ML 워크로드를 위한 EFA 및 NCCL 시작하기](https://docs.aws.amazon.com/ko_kr/AWSEC2/latest/UserGuide/efa-start-nccl.html)
