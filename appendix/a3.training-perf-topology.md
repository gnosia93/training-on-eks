## 토폴로지별 훈련성능 ##

### Test Configuration ###
* Hugging Face Lllama-3-8B 
* DeepSpeed Stage 3
* image: public.ecr.aws/deep-learning-containers/pytorch-training:2.8.0-gpu-py312-cu129-ubuntu22.04-ec2-v1.0
* Pod shared memory: "64Gi"                                    
* deepspeed 설정
```
{
    "fp16": { "enabled": false },
    "bf16": { "enabled": true },
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": { "device": "cpu", "pin_memory": true },
        "offload_param": { "device": "cpu", "pin_memory": true },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": false
    },
    "gradient_clipping": "auto",
    "steps_per_print": 10,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "gradient_accumulation_steps": "auto",
    "wall_clock_breakdown": false
}
```
* trainer 설정
```
per_device_train_batch_size=4,
gradient_accumulation_steps=4,                 # 실제 배치 사이즈 = 4 * 4 * GPU 개수
learning_rate=2e-5,
max_steps=50,                                  # 딱 50번의 스텝만 하고 종료 / 이경우 에포크는 무시됨   
num_train_epochs=1,
bf16=True,                                     # A100/H100/B200 GPU 권장
logging_steps=5,
deepspeed="llama-3-8b-stage3.json", 
save_strategy="epoch",
save_total_limit=2,     
gradient_checkpointing=False,                  # 메모리 절약을 위한 재계산
```
```
***** Running training *****
  Num examples = 36,718
  Num Epochs = 1
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 4
  Total optimization steps = 50
  Number of trainable parameters = 8,030,261,248
```

## 훈련성능 측정결과 ##

### p4d.24xlarge ###

* 1 Pod / 4 GPU / 4 EFA - 0:29:47 ( via P2P/CUMEM/read ) 
* 4 Pod / 1 GPU / 1 EFA - 0:28:15 ( via NET/Libfabric/0/GDRDMA )        
* 4 Pod / 1 GPU / 1 ENI - ???     ( via NET/Libfabric/0/GDRDMA )        
  


단일 파드(1 Pod / 4 GPU) 환경에서 P2P 성능이 CUMEM/read로 나오는 것은 GPU 간 직접 복사(Direct P2P)가 아닌 간접 참조 방식으로 작동하고 있다는 증거입니다. 이를 최적화하여 NVLink나 PCIe P2P의 최대 속도를 끌어내기 위한 체크리스트는 다음과 같습니다.

#### 1. 호스트 레벨 설정 (가장 중요) ####
GPU 간 직접 통신을 방해하는 커널 및 드라이버 설정을 수정해야 합니다.
* IOMMU 비활성화: 가상화 환경에서 IOMMU가 켜져 있으면 PCIe 패킷이 CPU를 거쳐야 하므로 P2P가 차단됩니다.
* GRUB 설정에서 intel_iommu=off 또는 amd_iommu=off를 확인하세요.
* ACS(Access Control Services) 비활성화: PCIe 스위치의 보안 기능인 ACS가 켜져 있으면 P2P 패킷이 강제로 루트 컴플렉스를 거치게 됩니다.
이는 서버 제조사나 AWS 인스턴스 타입에 따라 고정되어 있을 수 있으나, 일반적으로 P4d 같은 고성능 인스턴스는 이미 최적화되어 있습니다.

#### 2. 쿠버네티스/컨테이너 설정 최적화 ###
파드 내부에서 호스트의 하드웨어 토폴로지를 그대로 인식할 수 있게 해야 합니다.
* hostIPC: true 설정: 파드 간 또는 프로세스 간 공유 메모리와 세마포어를 공유할 수 있게 하여 NCCL이 P2P 경로를 생성하도록 돕습니다.
* PID Namespace 공유: 같은 파드 내 프로세스들이 서로를 정확히 인식하도록 설정합니다.
* 공유 메모리(shm) 확장: /dev/shm 용량이 부족하면 NCCL이 P2P 버퍼를 생성하지 못하고 Fallback 합니다. 최소 2GB 이상, 권장 8GB 이상 할당하세요.


#### 3. NCCL 환경 변수를 통한 강제 최적화 ####
NCCL이 CUMEM/read 대신 더 빠른 알고리즘을 선택하도록 강제할 수 있습니다.
P2P 강제 활성화:
```
export NCCL_P2P_DISABLE=0
export NCCL_P2P_LEVEL=NVL  # NVLink가 있다면 최우선 사용, 없으면 PXB(PCIe Bridge)
```

프로토콜 지정:
```
export NCCL_PROTO=simple  # GDRDMA 환경에서 가장 높은 대역폭을 보장
```

SHM 사용 금지: NCCL이 자꾸 느린 공유 메모리 방식을 쓰려고 한다면 강제로 끕니다.
```
export NCCL_SHM_DISABLE=1
```

#### 4. 하드웨어 토폴로지 확인 (nvidia-smi topo -m) ####
파드 내부에서 이 명령어를 실행하여 GPU 간 연결 상태를 확인하세요.
* NV#: NVLink 연결 (최상)
* PHB/PXB: PCIe 스위치 연결 (우수)
* SYS: CPU/시스템 버스 경유 (이것이 뜨면 최적화 실패이며, 현재 CUMEM/read가 발생하는 원인일 확률이 높습니다.)


#### 5. 왜 4 Pod 설정이 더 빨랐을까? (결론적 분석) ####
* 1 Pod 설정에서는 4개의 GPU가 하나의 메모리 주소 공간을 공유하며 서로 "읽기 권한"을 확인하는 과정에서 CUMEM/read 오버헤드가 발생한 반면, 4 Pod 설정에서는 각 GPU가 독립된 주소 공간을 갖고 EFA(GDRDMA)라는 검증된 외부 고속도로를 탔기 때문에 오히려 병목이 없었던 것입니다.
* 최적화 팁: 만약 P4d 인스턴스라면 NCCL_P2P_LEVEL=NVL 설정을 통해 GPU 간 통신이 NVLink를 타도록 강제하는 것이 가장 확실한 성능 향상 방법입니다. NVIDIA NCCL 가이드에서 각 레벨에 대한 상세 설명을 확인하실 수 있습니다.

