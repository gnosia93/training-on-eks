## 토폴로지별 훈련성능 ##

### Test Configuration ###
* Hugging Face Lllama-3-8B 
* DeepSpeed Stage 3
* image: public.ecr.aws/deep-learning-containers/pytorch-training:2.8.0-gpu-py312-cu129-ubuntu22.04-ec2-v1.0
* Pod shared memory: "64Gi"                                    
* deepspeed 설정
```
{
    "fp16": { "enabled": false },
    "bf16": { "enabled": true },
    "zero_optimization": {
        "stage": 3,
        "offload_optimizer": { "device": "cpu", "pin_memory": true },
        "offload_param": { "device": "cpu", "pin_memory": true },
        "overlap_comm": true,
        "contiguous_gradients": true,
        "sub_group_size": 1e9,
        "reduce_bucket_size": "auto",
        "stage3_prefetch_bucket_size": "auto",
        "stage3_param_persistence_threshold": "auto",
        "stage3_max_live_parameters": 1e9,
        "stage3_max_reuse_distance": 1e9,
        "stage3_gather_16bit_weights_on_model_save": false
    },
    "gradient_clipping": "auto",
    "steps_per_print": 10,
    "train_batch_size": "auto",
    "train_micro_batch_size_per_gpu": "auto",
    "gradient_accumulation_steps": "auto",
    "wall_clock_breakdown": false
}
```
* trainer 설정
```
per_device_train_batch_size=4,
gradient_accumulation_steps=4,                 # 실제 배치 사이즈 = 4 * 4 * GPU 개수
learning_rate=2e-5,
max_steps=50,                                  # 딱 50번의 스텝만 하고 종료 / 이경우 에포크는 무시됨   
num_train_epochs=1,
bf16=True,                                     # A100/H100/B200 GPU 권장
logging_steps=5,
deepspeed="llama-3-8b-stage3.json", 
save_strategy="epoch",
save_total_limit=2,     
gradient_checkpointing=False,                  # 메모리 절약을 위한 재계산
```
```
***** Running training *****
  Num examples = 36,718
  Num Epochs = 1
  Instantaneous batch size per device = 4
  Total train batch size (w. parallel, distributed & accumulation) = 64
  Gradient Accumulation steps = 4
  Total optimization steps = 50
  Number of trainable parameters = 8,030,261,248
```

## 훈련성능 측정결과 ##

### p4d.24xlarge ###

* 1 Pod / 4 GPU / 4 EFA - 
* 4 Pod / 1 GPU / 1 EFA - 0:28:15 ( via NET/Libfabric/0/GDRDMA )        



